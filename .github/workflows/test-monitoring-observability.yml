# ======================================================================
# Test Monitoring & Observability - Phase 3 Three-Layer Architecture
# ======================================================================
# MONITORING CAPABILITIES for test architecture:
# - Test performance tracking across all layers
# - Test success rate monitoring with alerts  
# - Test execution time analysis and trends
# - Test failure pattern detection and reporting
# - Resource usage monitoring per test layer
# - Quality metrics dashboard generation
# ======================================================================

name: "Test Monitoring & Observability"

on:
  schedule:
    # Run monitoring every hour during business hours (UTC)
    - cron: '0 8-20 * * 1-5'
  workflow_dispatch:
    inputs:
      monitoring_scope:
        description: 'Monitoring scope'
        required: false
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'unit-tests'
          - 'integration-tests'
          - 'e2e-tests'
          - 'performance-analysis'
      alert_threshold:
        description: 'Alert threshold for failure rate (%)'
        required: false
        default: '5'
        type: string
      analysis_period_hours:
        description: 'Analysis period (hours)'
        required: false
        default: '24'
        type: string

env:
  NODE_VERSION: "20"
  CI: true
  MONITORING_ENVIRONMENT: "observability"
  
  # Monitoring thresholds
  UNIT_TEST_MAX_DURATION_MS: "2000"
  INTEGRATION_TEST_MAX_DURATION_MS: "30000"
  E2E_TEST_MAX_DURATION_MS: "300000"
  
  # Alert thresholds
  MIN_UNIT_TEST_SUCCESS_RATE: "94"
  MIN_INTEGRATION_SUCCESS_RATE: "90"
  MIN_E2E_SUCCESS_RATE: "85"

jobs:
  # ======================================================================
  # Test Performance Monitoring
  # ======================================================================
  test-performance-monitoring:
    name: ‚ö° Test Performance Monitoring
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      unit_performance: ${{ steps.unit_metrics.outputs.performance }}
      integration_performance: ${{ steps.integration_metrics.outputs.performance }}
      performance_summary: ${{ steps.summary.outputs.summary }}

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: üì¶ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: ‚ö° Unit Tests Performance Monitoring
        id: unit_metrics
        if: contains(fromJson('["all", "unit-tests", "performance-analysis"]'), github.event.inputs.monitoring_scope || 'all')
        run: |
          echo "‚ö° Monitoring Unit Test Performance (Layer 1)"
          echo "Target: <${{ env.UNIT_TEST_MAX_DURATION_MS }}ms for 806+ tests"
          
          # Run unit tests with performance monitoring
          RUNS=3
          TOTAL_TIME=0
          SUCCESS_COUNT=0
          
          for i in $(seq 1 $RUNS); do
            echo "Run $i/$RUNS:"
            start_time=$(date +%s%3N)
            if npm test > test_run_$i.log 2>&1; then
              end_time=$(date +%s%3N)
              duration=$((end_time - start_time))
              TOTAL_TIME=$((TOTAL_TIME + duration))
              SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
              echo "  Duration: ${duration}ms ‚úÖ"
            else
              echo "  Failed ‚ùå"
            fi
          done
          
          # Calculate average performance
          if [ $SUCCESS_COUNT -gt 0 ]; then
            AVG_TIME=$((TOTAL_TIME / SUCCESS_COUNT))
            SUCCESS_RATE=$((SUCCESS_COUNT * 100 / RUNS))
          else
            AVG_TIME=9999
            SUCCESS_RATE=0
          fi
          
          # Performance evaluation
          if [ $AVG_TIME -le ${{ env.UNIT_TEST_MAX_DURATION_MS }} ]; then
            PERFORMANCE_STATUS="EXCELLENT"
          elif [ $AVG_TIME -le 3000 ]; then
            PERFORMANCE_STATUS="GOOD"
          elif [ $AVG_TIME -le 5000 ]; then
            PERFORMANCE_STATUS="NEEDS_IMPROVEMENT"
          else
            PERFORMANCE_STATUS="CRITICAL"
          fi
          
          PERFORMANCE_DATA="{\"avg_duration\":$AVG_TIME,\"success_rate\":$SUCCESS_RATE,\"status\":\"$PERFORMANCE_STATUS\",\"runs\":$RUNS}"
          echo "performance=$PERFORMANCE_DATA" >> $GITHUB_OUTPUT
          
          echo "üìä Unit Test Performance Summary:"
          echo "  Average Duration: ${AVG_TIME}ms (Target: <${{ env.UNIT_TEST_MAX_DURATION_MS }}ms)"
          echo "  Success Rate: ${SUCCESS_RATE}% (Target: ‚â•${{ env.MIN_UNIT_TEST_SUCCESS_RATE }}%)"
          echo "  Performance Status: $PERFORMANCE_STATUS"
        env:
          NODE_OPTIONS: "--max-old-space-size=6144"
          DATABASE_URL: "file:./data/monitoring-unit-test.db"

      - name: üîó Integration Tests Performance Monitoring
        id: integration_metrics
        if: contains(fromJson('["all", "integration-tests", "performance-analysis"]'), github.event.inputs.monitoring_scope || 'all')
        continue-on-error: true
        run: |
          echo "üîó Monitoring Integration Test Performance (Layer 2)"
          
          # Check if integration tests are available
          INTEGRATION_FILES=$(find tests/integration -name "*.test.js" 2>/dev/null | wc -l)
          
          if [ $INTEGRATION_FILES -lt 5 ]; then
            echo "‚ö†Ô∏è Integration tests not production-ready ($INTEGRATION_FILES files)"
            PERFORMANCE_DATA="{\"status\":\"NOT_READY\",\"file_count\":$INTEGRATION_FILES}"
            echo "performance=$PERFORMANCE_DATA" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "Target: <${{ env.INTEGRATION_TEST_MAX_DURATION_MS }}ms for $INTEGRATION_FILES integration tests"
          
          # Run integration tests with performance monitoring
          start_time=$(date +%s%3N)
          if npm run test:integration > integration_test_run.log 2>&1; then
            end_time=$(date +%s%3N)
            duration=$((end_time - start_time))
            SUCCESS_RATE=100
          else
            duration=99999
            SUCCESS_RATE=0
          fi
          
          # Performance evaluation
          if [ $duration -le ${{ env.INTEGRATION_TEST_MAX_DURATION_MS }} ]; then
            PERFORMANCE_STATUS="EXCELLENT"
          elif [ $duration -le 60000 ]; then
            PERFORMANCE_STATUS="GOOD"
          elif [ $duration -le 120000 ]; then
            PERFORMANCE_STATUS="NEEDS_IMPROVEMENT"
          else
            PERFORMANCE_STATUS="CRITICAL"
          fi
          
          PERFORMANCE_DATA="{\"duration\":$duration,\"success_rate\":$SUCCESS_RATE,\"status\":\"$PERFORMANCE_STATUS\",\"file_count\":$INTEGRATION_FILES}"
          echo "performance=$PERFORMANCE_DATA" >> $GITHUB_OUTPUT
          
          echo "üìä Integration Test Performance Summary:"
          echo "  Duration: ${duration}ms (Target: <${{ env.INTEGRATION_TEST_MAX_DURATION_MS }}ms)"
          echo "  Success Rate: ${SUCCESS_RATE}%"
          echo "  Performance Status: $PERFORMANCE_STATUS"
        env:
          NODE_OPTIONS: "--max-old-space-size=4096"
          DATABASE_URL: "file:./data/monitoring-integration-test.db"

      - name: üìä Generate Performance Summary
        id: summary
        run: |
          UNIT_PERF='${{ steps.unit_metrics.outputs.performance }}'
          INTEGRATION_PERF='${{ steps.integration_metrics.outputs.performance }}'
          
          # Extract key metrics (using default values if not available)
          UNIT_AVG=$(echo "$UNIT_PERF" | jq -r '.avg_duration // 1500')
          UNIT_STATUS=$(echo "$UNIT_PERF" | jq -r '.status // "UNKNOWN"')
          INTEGRATION_STATUS=$(echo "$INTEGRATION_PERF" | jq -r '.status // "NOT_READY"')
          
          SUMMARY="Unit Tests: ${UNIT_AVG}ms ($UNIT_STATUS) | Integration Tests: $INTEGRATION_STATUS"
          echo "summary=$SUMMARY" >> $GITHUB_OUTPUT
          
          echo "üìä Overall Performance Summary: $SUMMARY"

  # ======================================================================
  # Test Success Rate Analysis
  # ======================================================================
  test-success-rate-analysis:
    name: üìà Test Success Rate Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      success_analysis: ${{ steps.analysis.outputs.analysis }}
      alerts_triggered: ${{ steps.alerts.outputs.alerts }}

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: üì¶ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: üìà Comprehensive Success Rate Analysis
        id: analysis
        run: |
          echo "üìà Analyzing Test Success Rates"
          echo "Analysis Period: ${{ github.event.inputs.analysis_period_hours || '24' }} hours"
          
          ALERT_THRESHOLD="${{ github.event.inputs.alert_threshold || '5' }}"
          
          # Multiple test runs for reliability analysis
          TOTAL_RUNS=5
          UNIT_SUCCESS=0
          UNIT_TOTAL=$TOTAL_RUNS
          
          echo "Running $TOTAL_RUNS test cycles for success rate analysis..."
          
          for i in $(seq 1 $TOTAL_RUNS); do
            echo "Test cycle $i/$TOTAL_RUNS"
            
            # Unit test success rate
            if timeout 30 npm test > /dev/null 2>&1; then
              UNIT_SUCCESS=$((UNIT_SUCCESS + 1))
              echo "  Unit tests: ‚úÖ"
            else
              echo "  Unit tests: ‚ùå"
            fi
          done
          
          # Calculate success rates
          UNIT_SUCCESS_RATE=$((UNIT_SUCCESS * 100 / UNIT_TOTAL))
          
          # Integration tests (if available)
          INTEGRATION_FILES=$(find tests/integration -name "*.test.js" 2>/dev/null | wc -l)
          if [ $INTEGRATION_FILES -ge 5 ]; then
            INTEGRATION_SUCCESS=0
            INTEGRATION_TOTAL=$TOTAL_RUNS
            
            for i in $(seq 1 $TOTAL_RUNS); do
              if timeout 60 npm run test:integration > /dev/null 2>&1; then
                INTEGRATION_SUCCESS=$((INTEGRATION_SUCCESS + 1))
              fi
            done
            
            INTEGRATION_SUCCESS_RATE=$((INTEGRATION_SUCCESS * 100 / INTEGRATION_TOTAL))
          else
            INTEGRATION_SUCCESS_RATE="N/A"
          fi
          
          # Generate analysis
          ANALYSIS="{\"unit_success_rate\":$UNIT_SUCCESS_RATE,\"integration_success_rate\":\"$INTEGRATION_SUCCESS_RATE\",\"total_runs\":$TOTAL_RUNS,\"alert_threshold\":$ALERT_THRESHOLD}"
          echo "analysis=$ANALYSIS" >> $GITHUB_OUTPUT
          
          echo "üìä Success Rate Analysis Results:"
          echo "  Unit Tests: ${UNIT_SUCCESS_RATE}% ($UNIT_SUCCESS/$UNIT_TOTAL runs)"
          echo "  Integration Tests: ${INTEGRATION_SUCCESS_RATE}"
          echo "  Alert Threshold: ${ALERT_THRESHOLD}% failure rate"
        env:
          NODE_OPTIONS: "--max-old-space-size=6144"
          DATABASE_URL: "file:./data/monitoring-success-analysis.db"

      - name: üö® Alert Generation
        id: alerts
        run: |
          ANALYSIS='${{ steps.analysis.outputs.analysis }}'
          
          UNIT_SUCCESS_RATE=$(echo "$ANALYSIS" | jq -r '.unit_success_rate')
          ALERT_THRESHOLD=$(echo "$ANALYSIS" | jq -r '.alert_threshold')
          MIN_UNIT_THRESHOLD="${{ env.MIN_UNIT_TEST_SUCCESS_RATE }}"
          
          ALERTS_ARRAY="[]"
          ALERT_COUNT=0
          
          # Unit test success rate alerts
          UNIT_FAILURE_RATE=$((100 - UNIT_SUCCESS_RATE))
          if [ $UNIT_FAILURE_RATE -gt $ALERT_THRESHOLD ]; then
            ALERTS_ARRAY=$(echo "$ALERTS_ARRAY" | jq '. + [{"type":"unit_test_failure_rate","severity":"high","message":"Unit test failure rate '"$UNIT_FAILURE_RATE"'% exceeds threshold '"$ALERT_THRESHOLD"'%"}]')
            ALERT_COUNT=$((ALERT_COUNT + 1))
          fi
          
          if [ $UNIT_SUCCESS_RATE -lt $MIN_UNIT_THRESHOLD ]; then
            ALERTS_ARRAY=$(echo "$ALERTS_ARRAY" | jq '. + [{"type":"unit_test_success_rate","severity":"critical","message":"Unit test success rate '"$UNIT_SUCCESS_RATE"'% below production minimum '"$MIN_UNIT_THRESHOLD"'%"}]')
            ALERT_COUNT=$((ALERT_COUNT + 1))
          fi
          
          echo "alerts=$ALERTS_ARRAY" >> $GITHUB_OUTPUT
          
          if [ $ALERT_COUNT -gt 0 ]; then
            echo "üö® $ALERT_COUNT alerts triggered:"
            echo "$ALERTS_ARRAY" | jq -r '.[] | "  - " + .severity + ": " + .message'
          else
            echo "‚úÖ No alerts triggered - all success rates within thresholds"
          fi

  # ======================================================================
  # Test Failure Pattern Detection
  # ======================================================================
  failure-pattern-detection:
    name: üîç Failure Pattern Detection
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      patterns: ${{ steps.detection.outputs.patterns }}
      recommendations: ${{ steps.recommendations.outputs.recommendations }}

    steps:
      - name: üì• Checkout code
        uses: actions/checkout@v4

      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: üì¶ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: üîç Detect Test Failure Patterns
        id: detection
        run: |
          echo "üîç Analyzing Test Failure Patterns"
          
          # Run tests with detailed output capture
          PATTERNS_DETECTED=""
          
          # Memory-related failures
          if npm test 2>&1 | grep -i "out of memory\|heap\|memory"; then
            PATTERNS_DETECTED="${PATTERNS_DETECTED}memory_issues,"
            echo "‚ö†Ô∏è Memory-related failure pattern detected"
          fi
          
          # Timeout-related failures
          if npm test 2>&1 | grep -i "timeout\|timed out"; then
            PATTERNS_DETECTED="${PATTERNS_DETECTED}timeout_issues,"
            echo "‚ö†Ô∏è Timeout-related failure pattern detected"
          fi
          
          # Database-related failures
          if npm test 2>&1 | grep -i "database\|sqlite\|connection"; then
            PATTERNS_DETECTED="${PATTERNS_DETECTED}database_issues,"
            echo "‚ö†Ô∏è Database-related failure pattern detected"
          fi
          
          # Dependency-related failures
          if npm test 2>&1 | grep -i "cannot find module\|import\|require"; then
            PATTERNS_DETECTED="${PATTERNS_DETECTED}dependency_issues,"
            echo "‚ö†Ô∏è Dependency-related failure pattern detected"
          fi
          
          # Clean up trailing comma
          PATTERNS_DETECTED=$(echo "$PATTERNS_DETECTED" | sed 's/,$//')
          
          if [ -z "$PATTERNS_DETECTED" ]; then
            PATTERNS_DETECTED="none"
            echo "‚úÖ No failure patterns detected"
          fi
          
          echo "patterns=$PATTERNS_DETECTED" >> $GITHUB_OUTPUT
          echo "Patterns detected: $PATTERNS_DETECTED"
        env:
          NODE_OPTIONS: "--max-old-space-size=6144"
          DATABASE_URL: "file:./data/monitoring-pattern-detection.db"

      - name: üí° Generate Recommendations
        id: recommendations
        run: |
          PATTERNS="${{ steps.detection.outputs.patterns }}"
          RECOMMENDATIONS="[]"
          
          if [[ "$PATTERNS" == *"memory_issues"* ]]; then
            RECOMMENDATIONS=$(echo "$RECOMMENDATIONS" | jq '. + ["Increase NODE_OPTIONS max-old-space-size allocation","Review test memory usage and optimize large test suites","Consider test parallelization adjustments"]')
          fi
          
          if [[ "$PATTERNS" == *"timeout_issues"* ]]; then
            RECOMMENDATIONS=$(echo "$RECOMMENDATIONS" | jq '. + ["Review and increase test timeout configurations","Optimize slow test execution","Check for hanging async operations"]')
          fi
          
          if [[ "$PATTERNS" == *"database_issues"* ]]; then
            RECOMMENDATIONS=$(echo "$RECOMMENDATIONS" | jq '. + ["Verify database initialization in test setup","Check database file permissions and paths","Review database connection management"]')
          fi
          
          if [[ "$PATTERNS" == *"dependency_issues"* ]]; then
            RECOMMENDATIONS=$(echo "$RECOMMENDATIONS" | jq '. + ["Run npm ci to ensure clean dependency installation","Check for missing dependencies in package.json","Verify import/require paths in test files"]')
          fi
          
          if [ "$PATTERNS" = "none" ]; then
            RECOMMENDATIONS=$(echo "$RECOMMENDATIONS" | jq '. + ["Continue current testing practices","Monitor for emerging patterns","Regular maintenance of test suite"]')
          fi
          
          echo "recommendations=$RECOMMENDATIONS" >> $GITHUB_OUTPUT
          
          echo "üí° Recommendations generated:"
          echo "$RECOMMENDATIONS" | jq -r '.[] | "  - " + .'

  # ======================================================================
  # Monitoring Dashboard Generation
  # ======================================================================
  monitoring-dashboard:
    name: üìä Monitoring Dashboard
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test-performance-monitoring, test-success-rate-analysis, failure-pattern-detection]
    if: always()

    steps:
      - name: üìä Generate Comprehensive Monitoring Dashboard
        run: |
          echo "# üìä Test Monitoring & Observability Dashboard" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "_Generated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')_" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Performance Monitoring Section
          echo "## ‚ö° Performance Monitoring" >> $GITHUB_STEP_SUMMARY
          
          PERFORMANCE_SUMMARY="${{ needs.test-performance-monitoring.outputs.performance_summary || 'No performance data available' }}"
          echo "**Summary**: $PERFORMANCE_SUMMARY" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Success Rate Analysis Section
          echo "## üìà Success Rate Analysis" >> $GITHUB_STEP_SUMMARY
          
          SUCCESS_ANALYSIS='${{ needs.test-success-rate-analysis.outputs.success_analysis }}'
          if [ "$SUCCESS_ANALYSIS" != "null" ] && [ "$SUCCESS_ANALYSIS" != "" ]; then
            UNIT_SUCCESS_RATE=$(echo "$SUCCESS_ANALYSIS" | jq -r '.unit_success_rate // "N/A"')
            INTEGRATION_SUCCESS_RATE=$(echo "$SUCCESS_ANALYSIS" | jq -r '.integration_success_rate // "N/A"')
            TOTAL_RUNS=$(echo "$SUCCESS_ANALYSIS" | jq -r '.total_runs // "N/A"')
            
            echo "- **Unit Tests Success Rate**: ${UNIT_SUCCESS_RATE}% (over $TOTAL_RUNS runs)" >> $GITHUB_STEP_SUMMARY
            echo "- **Integration Tests Success Rate**: ${INTEGRATION_SUCCESS_RATE}" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Analysis**: Not available" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Alerts Section
          ALERTS='${{ needs.test-success-rate-analysis.outputs.alerts_triggered }}'
          if [ "$ALERTS" != "null" ] && [ "$ALERTS" != "" ] && [ "$ALERTS" != "[]" ]; then
            echo "## üö® Active Alerts" >> $GITHUB_STEP_SUMMARY
            echo "$ALERTS" | jq -r '.[] | "- **" + .severity + "**: " + .message' >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚úÖ No Active Alerts" >> $GITHUB_STEP_SUMMARY
            echo "All monitoring thresholds are within acceptable ranges." >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Failure Patterns Section
          echo "## üîç Failure Pattern Analysis" >> $GITHUB_STEP_SUMMARY
          
          PATTERNS="${{ needs.failure-pattern-detection.outputs.patterns || 'none' }}"
          if [ "$PATTERNS" != "none" ]; then
            echo "**Patterns Detected**: $PATTERNS" >> $GITHUB_STEP_SUMMARY
            
            RECOMMENDATIONS='${{ needs.failure-pattern-detection.outputs.recommendations }}'
            if [ "$RECOMMENDATIONS" != "null" ] && [ "$RECOMMENDATIONS" != "" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Recommendations**:" >> $GITHUB_STEP_SUMMARY
              echo "$RECOMMENDATIONS" | jq -r '.[] | "- " + .' >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "**Status**: No failure patterns detected ‚úÖ" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # System Health Section
          echo "## üè• System Health" >> $GITHUB_STEP_SUMMARY
          echo "- **Node.js**: ${{ env.NODE_VERSION }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Monitoring Environment**: ${{ env.MONITORING_ENVIRONMENT }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Layers**: Layer 1 (Unit), Layer 2 (Integration), Layer 3 (E2E)" >> $GITHUB_STEP_SUMMARY
          echo "- **Performance Thresholds**: Unit <2s, Integration <30s, E2E <5min" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Configuration Section
          echo "## ‚öôÔ∏è Monitoring Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Monitoring Scope**: ${{ github.event.inputs.monitoring_scope || 'all' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Alert Threshold**: ${{ github.event.inputs.alert_threshold || '5' }}% failure rate" >> $GITHUB_STEP_SUMMARY
          echo "- **Analysis Period**: ${{ github.event.inputs.analysis_period_hours || '24' }} hours" >> $GITHUB_STEP_SUMMARY
          echo "- **Minimum Success Rates**: Unit ‚â•${{ env.MIN_UNIT_TEST_SUCCESS_RATE }}%, Integration ‚â•${{ env.MIN_INTEGRATION_SUCCESS_RATE }}%, E2E ‚â•${{ env.MIN_E2E_SUCCESS_RATE }}%" >> $GITHUB_STEP_SUMMARY

      - name: üìä Upload Monitoring Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-dashboard-${{ github.run_number }}
          path: |
            test_run_*.log
            integration_test_run.log
            monitoring-metrics.json
          retention-days: 14