name: "ðŸ›¡ï¸ Quality Gates & Code Standards"

on:
  workflow_call:
    outputs:
      test_exit_code:
        description: "Quality check exit code"
        value: ${{ jobs.summary.outputs.test_exit_code }}
      total_tests:
        description: "Total quality checks performed"
        value: ${{ jobs.summary.outputs.total_tests }}
      passed_tests:
        description: "Passed quality checks"
        value: ${{ jobs.summary.outputs.passed_tests }}
      failed_tests:
        description: "Failed quality checks"
        value: ${{ jobs.summary.outputs.failed_tests }}
  schedule:
    # Run daily at 2 AM UTC to catch any degradation
    - cron: '0 2 * * *'
  workflow_dispatch:

concurrency:
  group: quality-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  security-events: write

env:
  # Consolidated environment variables to reduce duplication
  NODE_VERSION: "20.19.5"
  NODE_OPTIONS: "--max-old-space-size=4096"
  npm_config_build_from_source: "false"
  CI: true
  NGROK_SKIP_DOWNLOAD: '1'
  PYTHON: '/usr/bin/python3'

jobs:
  code-quality:
    name: "ðŸ” Code Quality Analysis"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      eslint_errors: ${{ steps.eslint.outputs.eslint_errors }}
      eslint_warnings: ${{ steps.eslint.outputs.eslint_warnings }}
      html_errors: ${{ steps.htmlhint.outputs.html_errors }}
      html_warnings: ${{ steps.htmlhint.outputs.html_warnings }}
      markdown_issues: ${{ steps.markdown.outputs.markdown_issues }}
      structure_valid: ${{ steps.structure.outputs.structure_valid }}
      formatting_issues: ${{ steps.formatting.outputs.formatting_issues }}

    steps:
      - name: "ðŸ“¥ Checkout Code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "ðŸ”§ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: '20.19.5'
          cache: 'npm'

      - name: "ðŸ“¦ Install Dependencies"
        run: npm ci --prefer-offline --no-audit

      - name: "ðŸ” JavaScript Linting (ESLint)"
        id: eslint
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ” Running ESLint Analysis"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          npm run lint:js 2>&1 | tee eslint-output.log || true

          # Parse error and warning counts from ESLint summary line
          errors=$(grep "problems" eslint-output.log 2>/dev/null | sed -n 's/.*(\([0-9]*\) errors.*/\1/p' | head -1)
          errors=${errors:-0}
          warnings=$(grep "problems" eslint-output.log 2>/dev/null | sed -n 's/.* \([0-9]*\) warnings.*/\1/p' | head -1)
          warnings=${warnings:-0}

          echo "eslint_errors=$errors" >> $GITHUB_OUTPUT
          echo "eslint_warnings=$warnings" >> $GITHUB_OUTPUT

          if [ "$errors" -gt 0 ]; then
            echo "âŒ ESLint found $errors error(s)"
            exit 1
          elif [ "$warnings" -gt 0 ]; then
            echo "âš ï¸ ESLint found $warnings warning(s)"
          else
            echo "âœ… ESLint: No issues found"
          fi

      - name: "ðŸŽ¨ HTML Validation (HTMLHint)"
        id: htmlhint
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸŽ¨ Running HTML Validation"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          # Run HTMLHint and capture exit code
          set +e
          npm run lint:html 2>&1 | tee htmlhint-output.log
          htmlhint_exit_code=$?
          set -e

          # Check for actual HTMLHint errors (not the word "error" in success messages)
          if grep -q "L[0-9]" htmlhint-output.log 2>/dev/null; then
            html_errors=$(grep -c "L[0-9]" htmlhint-output.log || echo "0")
            html_warnings="0"
          else
            html_errors="0"
            html_warnings="0"
          fi

          echo "html_errors=$html_errors" >> $GITHUB_OUTPUT
          echo "html_warnings=$html_warnings" >> $GITHUB_OUTPUT

          if [ $htmlhint_exit_code -ne 0 ]; then
            echo "âŒ HTMLHint found $html_errors error(s)"
            exit 1
          elif [ "$html_warnings" -gt 0 ]; then
            echo "âš ï¸ HTMLHint found $html_warnings warning(s)"
          else
            echo "âœ… HTML: Valid and well-formed"
          fi

      - name: "ðŸ“ Markdown Quality Check"
        id: markdown
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ“ Validating Markdown Files"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          # Use npx to avoid global install
          npx --yes markdownlint-cli '**/*.md' --ignore node_modules 2>&1 | tee markdown-output.log || true

          md_issues=$(grep -c "MD" markdown-output.log 2>/dev/null | head -1 || echo "0")
          echo "markdown_issues=$md_issues" >> $GITHUB_OUTPUT

          if [ "$md_issues" -gt 0 ]; then
            echo "âš ï¸ Markdown: $md_issues style issue(s) found"
          else
            echo "âœ… Markdown: Well-formatted"
          fi

      - name: "ðŸ“ CLAUDE.md Size Check"
        id: claude_md_size
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ“ Checking CLAUDE.md File Size"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          npm run lint:claude-md

      - name: "ðŸ—ï¸ Project Structure Validation"
        id: structure
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ—ï¸ Validating Project Structure"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          # Capture exit code properly
          set +e
          node scripts/verify-structure.js 2>&1 | tee structure-output.log
          structure_exit_code=$?
          set -e

          if [ $structure_exit_code -ne 0 ]; then
            echo "structure_valid=false" >> $GITHUB_OUTPUT
            echo "âŒ Structure: Issues detected"
            exit 1
          else
            echo "structure_valid=true" >> $GITHUB_OUTPUT
            echo "âœ… Structure: Valid and consistent"
          fi

      - name: "ðŸ’… Code Formatting Check"
        id: formatting
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ’… Checking Code Formatting"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          # Check if prettier is configured
          if [ -f ".prettierrc" ] || [ -f ".prettierrc.json" ] || [ -f ".prettierrc.js" ]; then
            npx prettier --check "**/*.{js,html,css,md}" --ignore-path .gitignore 2>&1 | tee format-output.log || true

            if grep -q "not formatted" format-output.log; then
              unformatted=$(grep -c "not formatted" format-output.log || echo "0")
              echo "formatting_issues=$unformatted" >> $GITHUB_OUTPUT
              echo "âš ï¸ Formatting: $unformatted file(s) need formatting"
            else
              echo "formatting_issues=0" >> $GITHUB_OUTPUT
              echo "âœ… Formatting: All files properly formatted"
            fi
          else
            echo "formatting_issues=0" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ Formatting: No prettier config found, skipping"
          fi

  security-scan:
    name: "ðŸ”’ Security Analysis"
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      vuln_critical: ${{ steps.security.outputs.vuln_critical }}
      vuln_high: ${{ steps.security.outputs.vuln_high }}
      vuln_moderate: ${{ steps.security.outputs.vuln_moderate }}
      vuln_low: ${{ steps.security.outputs.vuln_low }}
      pii_errors: ${{ steps.pii_scan.outputs.pii_errors }}
      pii_warnings: ${{ steps.pii_scan.outputs.pii_warnings }}

    steps:
      - name: "ðŸ“¥ Checkout Code"
        uses: actions/checkout@v4

      - name: "ðŸ”§ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: '20.19.5'
          cache: 'npm'

      - name: "ðŸ“¦ Install Dependencies"
        run: npm ci --prefer-offline --no-audit

      - name: "ðŸ”’ Security Vulnerability Scan"
        id: security
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ”’ Running Security Scan"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          # Run npm audit on production dependencies only
          # Dev dependencies (like Vercel CLI) are not deployed to production
          npm audit --omit=dev --audit-level=moderate 2>&1 | tee audit-output.log || true

          # Extract vulnerability counts
          critical=$(grep -oP '(\d+) critical' audit-output.log | grep -oP '\d+' || echo "0")
          high=$(grep -oP '(\d+) high' audit-output.log | grep -oP '\d+' || echo "0")
          moderate=$(grep -oP '(\d+) moderate' audit-output.log | grep -oP '\d+' || echo "0")
          low=$(grep -oP '(\d+) low' audit-output.log | grep -oP '\d+' || echo "0")

          echo "vuln_critical=$critical" >> $GITHUB_OUTPUT
          echo "vuln_high=$high" >> $GITHUB_OUTPUT
          echo "vuln_moderate=$moderate" >> $GITHUB_OUTPUT
          echo "vuln_low=$low" >> $GITHUB_OUTPUT

          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ”’ Security Scan Results (Production Dependencies):"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ”´ Critical: $critical"
          echo "ðŸŸ  High: $high"
          echo "ðŸŸ¡ Moderate: $moderate"
          echo "ðŸŸ¢ Low: $low"

          if [ "$critical" -gt 0 ]; then
            echo "âŒ Security: Critical vulnerabilities found in production dependencies!"
            exit 1
          elif [ "$high" -gt 0 ]; then
            echo "âš ï¸ Security: High vulnerabilities found in production dependencies"
            echo "   Consider updating: npm audit fix"
          elif [ "$moderate" -gt 0 ]; then
            echo "âš ï¸ Security: Moderate vulnerabilities found"
          else
            echo "âœ… Security: No significant vulnerabilities"
          fi

      - name: "ðŸ” Check for Secrets"
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ” Scanning for Exposed Secrets"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          # Basic secret detection patterns
          patterns=(
            "password.*=.*['\"].*['\"]"
            "api[_-]?key.*=.*['\"].*['\"]"
            "secret.*=.*['\"].*['\"]"
            "token.*=.*['\"].*['\"]"
            "private[_-]?key"
          )

          found_secrets=false
          for pattern in "${patterns[@]}"; do
            if grep -r -i "$pattern" \
              --include="*.js" --include="*.ts" --include="*.json" \
              --include="*.yml" --include="*.yaml" --include="*.env*" \
              --exclude-dir=node_modules --exclude-dir=.git --exclude-dir=.tmp . > /dev/null 2>&1; then
              echo "âš ï¸ Potential secret pattern found: $pattern"
              found_secrets=true
            fi
          done

          if [ "$found_secrets" = true ]; then
            echo "âš ï¸ Warning: Potential secrets detected. Please review."
          else
            echo "âœ… Secrets: No hardcoded secrets detected"
          fi

      - name: "ðŸ” PII Exposure Scan"
        id: pii_scan
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ” Scanning for PII Exposure in Code"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          # Run PII exposure scanner
          node scripts/check-pii-exposure.js --json > pii-scan-results.json || true

          # Parse results
          pii_errors=$(jq -r '.errors' pii-scan-results.json || echo "0")
          pii_warnings=$(jq -r '.warnings' pii-scan-results.json || echo "0")
          pii_total=$(jq -r '.violations' pii-scan-results.json || echo "0")

          echo "pii_errors=$pii_errors" >> $GITHUB_OUTPUT
          echo "pii_warnings=$pii_warnings" >> $GITHUB_OUTPUT

          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ” PII Exposure Scan Results:"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ðŸ”´ Errors: $pii_errors"
          echo "âš ï¸  Warnings: $pii_warnings"
          echo "ðŸ“Š Total violations: $pii_total"

          if [ "$pii_total" -gt 0 ]; then
            echo ""
            echo "Details:"
            jq -r '.details[] | "  \(.file):\(.line) - \(.message)"' pii-scan-results.json
            echo ""
          fi

          if [ "$pii_errors" -gt 0 ]; then
            echo "âŒ PII Exposure: Blocking violations detected in code!"
            echo "   Please use maskEmail() or sanitization utilities"
            exit 1
          elif [ "$pii_warnings" -gt 0 ]; then
            echo "âš ï¸  PII Exposure: Warnings detected - please review"
          else
            echo "âœ… PII Exposure: No hardcoded PII detected in logging"
          fi

  summary:
    name: "ðŸ“Š Quality Report"
    runs-on: ubuntu-latest
    needs: [code-quality, security-scan]
    if: always()
    outputs:
      test_exit_code: ${{ steps.aggregate.outputs.test_exit_code }}
      parsing_method: ${{ steps.aggregate.outputs.parsing_method }}
      total_tests: ${{ steps.aggregate.outputs.total_tests }}
      passed_tests: ${{ steps.aggregate.outputs.passing_tests }}
      failed_tests: ${{ steps.aggregate.outputs.failing_tests }}
      skipped_tests: ${{ steps.aggregate.outputs.skipped_tests }}
      duration_ms: ${{ steps.aggregate.outputs.duration }}

    steps:
      - name: "ðŸ“Š Aggregate Quality Metrics"
        id: aggregate
        run: |
          # Calculate aggregate metrics from code-quality and security-scan jobs
          CODE_QUALITY_STATUS="${{ needs.code-quality.result }}"
          SECURITY_STATUS="${{ needs.security-scan.result }}"

          # Aggregate test counts (quality checks as "tests")
          # Each quality check (ESLint, HTMLHint, Markdown, Structure, Formatting, Security) counts as a test
          TOTAL_CHECKS=6
          PASSED_CHECKS=0
          FAILED_CHECKS=0

          # Code quality checks (5 checks)
          if [ "$CODE_QUALITY_STATUS" = "success" ]; then
            PASSED_CHECKS=$((PASSED_CHECKS + 5))
          else
            FAILED_CHECKS=$((FAILED_CHECKS + 5))
          fi

          # Security check (1 check)
          if [ "$SECURITY_STATUS" = "success" ]; then
            PASSED_CHECKS=$((PASSED_CHECKS + 1))
          else
            FAILED_CHECKS=$((FAILED_CHECKS + 1))
          fi

          # Overall exit code
          if [ "$FAILED_CHECKS" -gt 0 ]; then
            EXIT_CODE=1
          else
            EXIT_CODE=0
          fi

          # Set outputs
          echo "test_exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          echo "parsing_method=aggregate_quality_checks" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_CHECKS" >> $GITHUB_OUTPUT
          echo "passing_tests=$PASSED_CHECKS" >> $GITHUB_OUTPUT
          echo "failing_tests=$FAILED_CHECKS" >> $GITHUB_OUTPUT
          echo "skipped_tests=0" >> $GITHUB_OUTPUT
          echo "duration=0" >> $GITHUB_OUTPUT

          echo "ðŸ“Š Quality Metrics Aggregated:"
          echo "  Total Checks: $TOTAL_CHECKS"
          echo "  Passed: $PASSED_CHECKS"
          echo "  Failed: $FAILED_CHECKS"
          echo "  Exit Code: $EXIT_CODE"

      - name: "ðŸ“Š Generate Quality Report"
        continue-on-error: true
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const codeQuality = '${{ needs.code-quality.result }}';
            const securityScan = '${{ needs.security-scan.result }}';

            // Get output values from jobs
            const codeQualityOutputs = ${{ toJSON(needs.code-quality.outputs) }};
            const securityOutputs = ${{ toJSON(needs.security-scan.outputs) }};

            const overallStatus = (codeQuality === 'success' && securityScan === 'success') ? 'âœ…' : 'âŒ';
            const statusText = (codeQuality === 'success' && securityScan === 'success') ? 'PASSED' : 'FAILED';

            const comment = `## ${overallStatus} Quality Gates ${statusText}

            ### ðŸ“Š Quality Check Summary
            | Check | Status | Details |
            |-------|--------|---------|
            | **Code Quality** | ${codeQuality === 'success' ? 'âœ… Passed' : 'âŒ Failed'} | ESLint, HTMLHint, Markdown, Structure, Formatting |
            | **Security Scan** | ${securityScan === 'success' ? 'âœ… Passed' : 'âŒ Failed'} | Vulnerability scan, Secret detection |

            ### ðŸ“‹ Detailed Results
            **Code Quality:**
            - **ESLint**: ${codeQualityOutputs?.eslint_errors || '0'} errors, ${codeQualityOutputs?.eslint_warnings || '0'} warnings
            - **HTMLHint**: ${codeQualityOutputs?.html_errors || '0'} errors, ${codeQualityOutputs?.html_warnings || '0'} warnings
            - **Markdown**: ${codeQualityOutputs?.markdown_issues || '0'} style issues
            - **Structure**: ${codeQualityOutputs?.structure_valid === 'true' ? 'Valid' : 'Issues detected'}
            - **Formatting**: ${codeQualityOutputs?.formatting_issues || '0'} files need formatting

            **Security:**
            - **Critical**: ${securityOutputs?.vuln_critical || '0'} vulnerabilities
            - **High**: ${securityOutputs?.vuln_high || '0'} vulnerabilities
            - **Moderate**: ${securityOutputs?.vuln_moderate || '0'} vulnerabilities
            - **Low**: ${securityOutputs?.vuln_low || '0'} vulnerabilities

            ### ðŸ› ï¸ Quality Standards
            - **YAML Formatting**: All workflow files follow strict formatting standards
            - **Job Outputs**: All CI/CD jobs properly expose test results and metrics
            - **Robust Parsing**: Test result parsing handles multiple output formats
            - **Error Handling**: Comprehensive error detection and reporting
            - **Security**: Automated vulnerability scanning and secret detection

            ${overallStatus === 'âœ…' ?
              '### ðŸŽ‰ All quality checks passed!' :
              '### âš ï¸ Some quality checks failed. Please review the workflow output.'}
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('Quality Gates')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: "ðŸ“Š Create Test Metadata"
        if: always()
        run: |
          cat <<EOF > test-metadata.json
          {
            "workflow": "${{ github.workflow }}",
            "job": "${{ github.job }}",
            "run_id": "${{ github.run_id }}",
            "run_attempt": "${{ github.run_attempt }}",
            "test_type": "quality",
            "matrix": {},
            "exit_code": ${{ steps.aggregate.outputs.test_exit_code || 'null' }},
            "parsing_method": "${{ steps.aggregate.outputs.parsing_method || 'unknown' }}",
            "counts": {
              "total": ${{ steps.aggregate.outputs.total_tests || '0' }},
              "passed": ${{ steps.aggregate.outputs.passing_tests || '0' }},
              "failed": ${{ steps.aggregate.outputs.failing_tests || '0' }},
              "skipped": ${{ steps.aggregate.outputs.skipped_tests || '0' }}
            },
            "duration_ms": ${{ steps.aggregate.outputs.duration || '0' }},
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "code_quality_status": "${{ needs.code-quality.result }}",
            "security_status": "${{ needs.security-scan.result }}"
          }
          EOF

          echo "Metadata created:"
          cat test-metadata.json | jq '.'

      - name: "ðŸ“¤ Upload Quality Metadata"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-gates-metadata
          path: test-metadata.json
          retention-days: 7