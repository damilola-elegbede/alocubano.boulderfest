name: "🔗 Integration Tests"

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches: [main, develop]
  workflow_dispatch:

concurrency:
  group: integration-tests-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write

env:
  NODE_ENV: test
  CI: true
  NODE_OPTIONS: "--max-old-space-size=4096"

jobs:
  test:
    name: "🔗 Integration Tests (Node ${{ matrix.node-version }})"
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      fail-fast: false
      matrix:
        node-version: ['20.x', '22.x']

    env:
      # Use in-memory SQLite for perfect test isolation (no lock contention!)
      DATABASE_URL: ":memory:"
      INTEGRATION_TEST_MODE: "true"
      VERCEL: ""
      # Admin authentication - CRITICAL for integration tests
      TEST_ADMIN_PASSWORD: ${{ secrets.TEST_ADMIN_PASSWORD || 'test-admin-password-123' }}
      # Do NOT set ADMIN_PASSWORD in test environments - use TEST_ADMIN_PASSWORD instead
      ADMIN_SECRET: ${{ secrets.TEST_ADMIN_SECRET || 'test_admin_secret_minimum_32_characters_for_jwt_signing' }}
      # Test environment variables for integration tests
      STRIPE_SECRET_KEY: ${{ secrets.TEST_STRIPE_SECRET_KEY || 'sk_test_dummy_integration_key' }}
      STRIPE_PUBLISHABLE_KEY: ${{ secrets.TEST_STRIPE_PUBLISHABLE_KEY || 'pk_test_dummy_integration_key' }}
      BREVO_API_KEY: ${{ secrets.TEST_BREVO_API_KEY || 'test_brevo_integration_key' }}
      BREVO_NEWSLETTER_LIST_ID: "1"
      BREVO_WEBHOOK_SECRET: ${{ secrets.TEST_BREVO_WEBHOOK_SECRET || 'test_webhook_secret' }}
      INTERNAL_API_KEY: ${{ secrets.TEST_INTERNAL_API_KEY || 'test-internal-api-key-32-chars-min' }}
      WALLET_AUTH_SECRET: ${{ secrets.TEST_WALLET_AUTH_SECRET || 'test_wallet_auth_secret_minimum_32_chars' }}
      APPLE_PASS_KEY: ${{ secrets.TEST_APPLE_PASS_KEY || 'dGVzdF9hcHBsZV9wYXNzX2tleQ==' }}
      # Integration test timeouts
      VITEST_TEST_TIMEOUT: 60000
      VITEST_HOOK_TIMEOUT: 45000
      VITEST_SETUP_TIMEOUT: 20000
      VITEST_CLEANUP_TIMEOUT: 10000
      VITEST_REQUEST_TIMEOUT: 45000

    steps:
      - name: "📥 Checkout Code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "🔧 Setup Node.js ${{ matrix.node-version }}"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}

      - name: "📦 Cache node_modules"
        id: cache-modules
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            ~/.npm
          key: ${{ runner.os }}-node-${{ matrix.node-version }}-integration-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-${{ matrix.node-version }}-integration-

      - name: "📦 Install Dependencies"
        if: steps.cache-modules.outputs.cache-hit != 'true'
        env:
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: '1'
          NGROK_SKIP_DOWNLOAD: '1'
          PUPPETEER_SKIP_DOWNLOAD: '1'
          SKIP_HEAVYWEIGHT_DOWNLOADS: '1'
        run: |
          echo "📦 Installing minimal dependencies for integration tests..."

          # Skip heavy dev dependencies not needed for integration tests
          # This saves ~77MB and 2+ minutes of installation time
          npm ci --prefer-offline --no-audit --no-fund \
            --omit=optional 2>/dev/null || {
              # If omit fails, do regular install
              npm ci --prefer-offline --no-audit --no-fund
            }

          # Only install the specific LibSQL binary we need
          if [ "$RUNNER_OS" = "Linux" ]; then
            if [ ! -d "node_modules/@libsql/linux-x64-gnu" ]; then
              echo "🔧 Installing LibSQL Linux binary..."
              npm install @libsql/linux-x64-gnu@0.5.22 --no-save --no-audit
            fi
          fi

          echo "✅ Minimal dependencies installed"

      - name: "🔄 Verify Cached Dependencies"
        if: steps.cache-modules.outputs.cache-hit == 'true'
        run: |
          echo "📦 Using cached dependencies"
          # Quick verification that critical packages exist
          if [ ! -d "node_modules/@libsql" ]; then
            echo "⚠️ Cache corrupted, reinstalling..."
            npm ci --prefer-offline --no-audit --no-fund
          else
            echo "✅ Cached dependencies verified"
          fi

      - name: "🧽 Optimize for Integration Tests"
        run: |
          echo "🧽 Removing unnecessary heavyweight dependencies..."
          # Remove packages not needed for integration tests (saves ~77MB)
          # These are only needed for E2E tests, local dev, or deployment
          rm -rf node_modules/playwright* 2>/dev/null || true
          rm -rf node_modules/lighthouse* 2>/dev/null || true
          rm -rf node_modules/ngrok* 2>/dev/null || true
          rm -rf node_modules/vercel 2>/dev/null || true
          rm -rf node_modules/@axe-core 2>/dev/null || true
          rm -rf node_modules/puppeteer* 2>/dev/null || true
          rm -rf node_modules/chrome-* 2>/dev/null || true

          # Report space saved
          echo "🎟️ Optimized for integration tests (removed E2E/dev-only deps)"

      - name: "🗃️ Setup SQLite Database"
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🗃️ Setting up Integration Test Database"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          # No database setup needed for in-memory SQLite!
          # Each test worker gets its own isolated in-memory database
          echo "✅ Using in-memory SQLite for perfect test isolation"
          echo "🚀 No file cleanup or setup needed"
          echo "⚡ Tests can run in parallel without lock contention"

      - name: "🔗 Run Integration Tests"
        id: test
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🔗 Running Integration Test Suite"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🎯 Timeout: 5 minutes total (stage limit)"
          echo "🗃️ Database: SQLite with real file storage"
          echo "🌐 APIs: Limited external service integration"
          echo "🧪 Expected: ~30-50 integration tests"
          echo "⚙️  Environment: Test credentials configured"
          echo "⏱️  Individual Test Timeout: 60 seconds"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          # Record start time
          start_time=$(date +%s%3N)

          # Run integration tests with increased timeout (280 seconds, 20 seconds buffer)
          timeout 280s npm run test:integration 2>&1 | tee integration-test-output.log
          test_exit_code=${PIPESTATUS[0]}

          # Record end time
          end_time=$(date +%s%3N)
          duration=$((end_time - start_time))

          # Extract test counts - Fix: Calculate total_tests correctly
          passing_tests=$(grep -E "Tests.*passed" integration-test-output.log | sed -E "s/.*Tests[[:space:]]+([0-9]+)[[:space:]]+passed.*/\1/" | head -1 || echo "0")
          failing_tests=$(grep -E "Tests.*failed" integration-test-output.log | sed -E "s/.*Tests[[:space:]]+([0-9]+)[[:space:]]+failed.*/\1/" | head -1 || echo "0")
          total_tests=$((passing_tests + failing_tests))

          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📊 Integration Test Results"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Tests Passed: $passing_tests"
          echo "❌ Tests Failed: $failing_tests"
          echo "📈 Total Tests: $total_tests"
          echo "⏱️  Duration: ${duration}ms"
          echo "🗃️ Database: File-based SQLite"

          # Timeout evaluation
          if [ "$duration" -lt 300000 ]; then
            echo "🏆 EXCELLENT: Integration tests completed within 5-minute limit!"
          else
            echo "⚠️  WARNING: Integration tests exceeded 5-minute timeout limit"
          fi

          exit $test_exit_code

      - name: "🧹 Database Cleanup"
        if: always()
        run: |
          echo "✅ No cleanup needed for in-memory databases"
          echo "✅ Integration test database cleanup completed"

      - name: "📤 Upload Test Results"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results-node-${{ matrix.node-version }}
          path: integration-test-output.log
          retention-days: 7

      - name: "📊 Report to PR"
        if: github.event_name == 'pull_request' && always()
        continue-on-error: true
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const output = fs.readFileSync('integration-test-output.log', 'utf8');
            const nodeVersion = '${{ matrix.node-version }}';
            const status = '${{ job.status }}';

            const statusIcon = status === 'success' ? '✅' : '❌';
            const statusText = status === 'success' ? 'PASSED' : 'FAILED';

            const comment = `## ${statusIcon} Integration Tests ${statusText} (Node ${nodeVersion})

            <details>
            <summary>View Test Output</summary>

            \`\`\`
            ${output.slice(-3000)}
            \`\`\`

            </details>
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes(`Integration Tests`) &&
              comment.body.includes(`Node ${nodeVersion}`)
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }