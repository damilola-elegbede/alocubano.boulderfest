---
name: üé≠ E2E Tests - Preview Deployments

# Modern E2E testing workflow using Vercel Preview Deployments
# - Triggers ONLY on successful Vercel deployments to prevent duplicate runs
# - Real production environment testing with serverless functions
# - No local server management or port conflicts
# - Improved reliability and faster execution
# - Single run per deployment with deployment_status trigger

on:
  # Only trigger when Vercel deployments succeed - prevents duplicate runs
  deployment_status:
    # Trigger when Vercel deployments succeed
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'standard'
        type: choice
        options:
          - 'standard'      # Basic flows (6 tests)
          - 'advanced'      # All flows (12 tests)
          - 'performance'   # Performance-focused tests
          - 'accessibility' # Mobile experience tests
          - 'security'      # Admin and security tests
      browsers:
        description: 'Browser matrix to test'
        required: false
        default: 'standard'
        type: choice
        options:
          - 'standard'      # Chromium + Firefox
          - 'extended'      # + Safari (WebKit)
          - 'mobile'        # + Mobile browsers
          - 'chromium-only' # Chromium only (fastest)
      preview_url:
        description: 'Specific preview URL to test (optional)'
        required: false
        type: string

# Prevent multiple E2E runs for the same deployment
concurrency:
  group: e2e-preview-${{ github.event.deployment.sha || github.sha }}-${{ github.workflow }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  deployments: read

env:
  NODE_VERSION: "20.19.5"
  NODE_ENV: test
  CI: true
  NODE_OPTIONS: "--max-old-space-size=4096"
  npm_config_build_from_source: "false"
  PLAYWRIGHT_BROWSERS_PATH: ${{ github.workspace }}/.playwright-browsers

jobs:
  # Extract preview URL - simplified since deployment is already complete
  get-preview-url:
    name: üîç Extract Preview URL
    runs-on: ubuntu-latest
    if: >-
      (github.event_name == 'deployment_status' &&
       github.event.deployment_status.state == 'success' &&
       github.event.deployment.environment == 'Preview') ||
      (github.event_name == 'workflow_dispatch')
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    outputs:
      preview-url: ${{ steps.extract-url.outputs.preview-url }}
    steps:
      - name: üîç Extract Preview URL
        id: extract-url
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] && [ -n "${{ inputs.preview_url }}" ]; then
            # Manual trigger with specific URL
            PREVIEW_URL="${{ inputs.preview_url }}"
            echo "preview-url=$PREVIEW_URL" >> $GITHUB_OUTPUT
            echo "üìç Using provided preview URL: $PREVIEW_URL"
          elif [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            # Manual trigger without URL - auto-discover from GitHub deployments
            echo "üîç Auto-discovering preview URL for current commit..."

            # Find the most recent Preview deployment for this commit, then its latest successful status
            DEPLOYMENT_ID=$(gh api "/repos/${{ github.repository }}/deployments?sha=${{ github.sha }}&environment=Preview&per_page=1" | jq -r '.[0].id // empty')
            if [ -n "$DEPLOYMENT_ID" ]; then
              PREVIEW_URL=$(
                gh api "/repos/${{ github.repository }}/deployments/${DEPLOYMENT_ID}/statuses?per_page=20" \
                | jq -r 'map(select(.state=="success")) | first | .target_url // empty'
              )
              if [ -n "$PREVIEW_URL" ]; then
                echo "preview-url=$PREVIEW_URL" >> $GITHUB_OUTPUT
                echo "‚úÖ Found preview URL from deployment: $PREVIEW_URL"
              else
                # Fallback: try any recent successful Preview deployment
                echo "‚ö†Ô∏è No deployment status found for current commit, checking recent deployments..."
                PREVIEW_URL=$(
                  gh api "/repos/${{ github.repository }}/deployments?environment=Preview&per_page=5" \
                  | jq -r '.[].id' \
                  | xargs -I {} sh -c 'gh api "/repos/${{ github.repository }}/deployments/{}/statuses?per_page=20" \
                    | jq -r "map(select(.state==\"success\")) | first | .target_url // empty" | head -1' \
                  | grep -m1 .
                )

                if [ -n "$PREVIEW_URL" ]; then
                  echo "preview-url=$PREVIEW_URL" >> $GITHUB_OUTPUT
                  echo "‚ö†Ô∏è Using recent preview deployment (may not match current commit): $PREVIEW_URL"
                else
                  echo "‚ùå No successful preview deployments found. Please provide a preview URL or wait for deployment."
                  exit 1
                fi
              fi
            else
              echo "‚ùå No deployments found for current commit. Please wait for Vercel deployment to complete."
              exit 1
            fi
          elif [ "${{ github.event_name }}" = "deployment_status" ]; then
            # Deployment trigger - FIX: Use environment_url with fallback to target_url
            PREVIEW_URL="${{ github.event.deployment_status.environment_url || github.event.deployment_status.target_url }}"
            echo "preview-url=$PREVIEW_URL" >> $GITHUB_OUTPUT
            echo "üìç Using deployment status URL: $PREVIEW_URL"
          else
            echo "‚ùå Unexpected event type: ${{ github.event_name }}"
            exit 1
          fi

      - name: üìã URL Extraction Summary
        run: |
          echo "üéØ URL Extraction Results:"
          echo "  Event: ${{ github.event_name }}"
          echo "  URL: ${{ steps.extract-url.outputs.preview-url }}"
          echo "  Deployment State: ${{ github.event.deployment_status.state || 'N/A' }}"
          echo "  Environment: ${{ github.event.deployment.environment || 'N/A' }}"


  # Build test matrix (runs in parallel with URL extraction)
  build-test-matrix:
    name: üìã Build Test Matrix
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
      test-pattern: ${{ steps.matrix.outputs.test-pattern }}
      browsers-json: ${{ steps.matrix.outputs.browsers-json }}
      test-suite: ${{ steps.matrix.outputs.test-suite }}
    steps:
      - name: üßÆ Build Test Matrix
        id: matrix
        run: |
          # Set defaults with proper fallback
          TEST_SUITE="${{ inputs.test_suite }}"
          BROWSERS="${{ inputs.browsers }}"

          # Apply defaults if empty or null
          if [ -z "$TEST_SUITE" ] || [ "$TEST_SUITE" = "null" ]; then
            TEST_SUITE="standard"
          fi

          if [ -z "$BROWSERS" ] || [ "$BROWSERS" = "null" ]; then
            BROWSERS="standard"
          fi

          echo "üîß Input Processing:"
          echo "  Test Suite: $TEST_SUITE (from: '${{ inputs.test_suite }}')"
          echo "  Browsers: $BROWSERS (from: '${{ inputs.browsers }}')"

          # Build browser matrix with validation
          BROWSER_LIST=""
          case "$BROWSERS" in
            "standard")
              BROWSER_LIST="chromium firefox"
              ;;
            "extended")
              BROWSER_LIST="chromium firefox webkit"
              ;;
            "mobile")
              # Mobile browsers use device emulation in Playwright projects
              BROWSER_LIST="chromium firefox mobile-chrome mobile-safari"
              ;;
            "chromium-only")
              BROWSER_LIST="chromium"
              ;;
            *)
              echo "‚ö†Ô∏è Unknown browser selection: $BROWSERS, defaulting to standard"
              BROWSER_LIST="chromium firefox"
              ;;
          esac

          # Convert to JSON array
          BROWSER_JSON_ARRAY=""
          for browser in $BROWSER_LIST; do
            if [ -z "$BROWSER_JSON_ARRAY" ]; then
              BROWSER_JSON_ARRAY="\"$browser\""
            else
              BROWSER_JSON_ARRAY="$BROWSER_JSON_ARRAY, \"$browser\""
            fi
          done

          BROWSER_MATRIX="{\"browser\": [$BROWSER_JSON_ARRAY]}"

          # Determine test pattern for file filtering
          # NOTE: Patterns match test.describe() titles, not filenames
          TEST_PATTERN=""
          case "$TEST_SUITE" in
            "standard")
              TEST_PATTERN="Basic Navigation|Cart Functionality|Admin Authentication|Gallery Basic Browsing|Registration Process|API Endpoints Health Check"
              ;;
            "advanced")
              TEST_PATTERN="" # Run all tests - no filter
              ;;
            "performance")
              TEST_PATTERN="Gallery Performance & Functionality|User Engagement Metrics"
              ;;
            "accessibility")
              TEST_PATTERN="Mobile Registration Experience"
              ;;
            "security")
              TEST_PATTERN="Admin Authentication|Admin Dashboard & Security|Ticket Validation|API Endpoints Health Check"
              ;;
            *)
              echo "‚ö†Ô∏è Unknown test suite: $TEST_SUITE, defaulting to standard"
              TEST_PATTERN="Basic Navigation|Cart Functionality|Admin Authentication|Gallery Basic Browsing|Registration Process|API Endpoints Health Check"
              ;;
          esac

          # Validate JSON structure
          if ! echo "$BROWSER_MATRIX" | jq . > /dev/null 2>&1; then
            echo "‚ùå Invalid JSON generated for browser matrix: $BROWSER_MATRIX"
            exit 1
          fi

          # Output results
          echo "matrix=$BROWSER_MATRIX" >> $GITHUB_OUTPUT
          echo "test-pattern=$TEST_PATTERN" >> $GITHUB_OUTPUT
          echo "browsers-json=[$BROWSER_JSON_ARRAY]" >> $GITHUB_OUTPUT
          echo "test-suite=$TEST_SUITE" >> $GITHUB_OUTPUT

          echo "üìã Test Matrix Configuration:"
          echo "  Browser Matrix JSON: $BROWSER_MATRIX"
          echo "  Browser List: $BROWSER_LIST"
          echo "  Test Pattern: $TEST_PATTERN"
          echo "  Test Suite: $TEST_SUITE"

          # Validate matrix will work
          BROWSER_COUNT=$(echo "$BROWSER_MATRIX" | jq '.browser | length')
          echo "  Browser Count: $BROWSER_COUNT"

          if [ "$BROWSER_COUNT" -eq 0 ]; then
            echo "‚ùå No browsers in matrix!"
            exit 1
          fi

  # Main E2E testing job - simplified since deployment is already complete
  e2e-tests:
    name: üé≠ E2E Tests (${{ matrix.browser }})
    runs-on: ubuntu-latest
    needs: [get-preview-url, build-test-matrix]
    # Only run if URL extraction succeeded
    if: needs.get-preview-url.result == 'success'
    outputs:
      test_exit_code: ${{ steps.run-tests.outputs.test_exit_code }}
      parsing_method: ${{ steps.run-tests.outputs.parsing_method }}
      total_tests: ${{ steps.run-tests.outputs.total_tests }}
      passed_tests: ${{ steps.run-tests.outputs.passing_tests }}
      failed_tests: ${{ steps.run-tests.outputs.failing_tests }}
      skipped_tests: ${{ steps.run-tests.outputs.skipped_tests }}
      duration_ms: ${{ steps.run-tests.outputs.duration }}
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.build-test-matrix.outputs.matrix) }}
    timeout-minutes: 20
    env:
      # Admin Authentication Test Variables
      TEST_ADMIN_PASSWORD: ${{ secrets.TEST_ADMIN_PASSWORD }}
      ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD }}
      ADMIN_SECRET: ${{ secrets.ADMIN_SECRET }}
      # Database Configuration (using repository secret for URL)
      TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
      TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
      # Google Drive Service Account Configuration
      GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
      GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
      GOOGLE_DRIVE_GALLERY_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_GALLERY_FOLDER_ID }}
      # Payment Processing (if available)
      STRIPE_PUBLISHABLE_KEY: ${{ secrets.STRIPE_PUBLISHABLE_KEY }}
      STRIPE_SECRET_KEY: ${{ secrets.STRIPE_SECRET_KEY }}
      STRIPE_WEBHOOK_SECRET: ${{ secrets.STRIPE_WEBHOOK_SECRET }}
      # Email Service (if available)
      BREVO_API_KEY: ${{ secrets.BREVO_API_KEY }}
      BREVO_NEWSLETTER_LIST_ID: ${{ secrets.BREVO_NEWSLETTER_LIST_ID }}
      BREVO_WEBHOOK_SECRET: ${{ secrets.BREVO_WEBHOOK_SECRET }}
      # Vercel Configuration (for deployment operations)
      VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
      VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
      VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
      # GitHub Token (automatically provided by GitHub Actions)
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      # Test Environment Configuration
      E2E_TEST_MODE: 'true'
      NODE_ENV: 'test'
      # Dynamic URL variables (fixed: reference proper outputs from get-preview-url job)
      PLAYWRIGHT_BASE_URL: ${{ needs.get-preview-url.outputs.preview-url }}
      PREVIEW_URL: ${{ needs.get-preview-url.outputs.preview-url }}

    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üéØ Set Target URL
        id: target-url
        run: |
          TARGET_URL="${{ needs.get-preview-url.outputs.preview-url }}"

          # Validate we have a URL
          if [ -z "$TARGET_URL" ] || [ "$TARGET_URL" = "null" ]; then
            echo "‚ùå No valid preview URL available"
            exit 1
          fi

          echo "target-url=$TARGET_URL" >> $GITHUB_OUTPUT
          echo "PLAYWRIGHT_BASE_URL=$TARGET_URL" >> $GITHUB_ENV
          echo "PREVIEW_URL=$TARGET_URL" >> $GITHUB_ENV
          echo "‚ÑπÔ∏è Using preview URL: $TARGET_URL"

      - name: üìã Test Configuration
        run: |
          echo "üéØ E2E Test Configuration:"
          echo "  Target URL: ${{ steps.target-url.outputs.target-url }}"
          echo "  Browser: ${{ matrix.browser }}"
          echo "  Test Suite: ${{ needs.build-test-matrix.outputs.test-suite }}"
          echo "  Test Pattern: ${{ needs.build-test-matrix.outputs.test-pattern }}"
          echo "  NODE_OPTIONS: $NODE_OPTIONS"
          echo "  Event Type: ${{ github.event_name }}"
          echo "  Deployment State: ${{ github.event.deployment_status.state || 'N/A' }}"

      - name: üü¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'package-lock.json'

      - name: üì¶ Install Dependencies
        run: |
          npm ci --prefer-offline --no-audit
          echo "‚úÖ Dependencies installed"

      - name: üß© Cache Playwright Browsers
        uses: actions/cache@v4
        with:
          path: .playwright-browsers
          # Fixed: More specific cache key per browser for better matrix handling
          key: ${{ runner.os }}-playwright-${{ matrix.browser }}-${{ hashFiles('package-lock.json') }}-v2
          restore-keys: |
            ${{ runner.os }}-playwright-${{ matrix.browser }}-
            ${{ runner.os }}-playwright-

      - name: üé≠ Install Playwright Browsers
        run: |
          # Fixed: Map mobile browsers to actual Playwright browser names
          BROWSER="${{ matrix.browser }}"
          case "$BROWSER" in
            mobile-chrome)
              INSTALL_BROWSER="chromium"
              echo "üì± Mobile Chrome will use Chromium with mobile device emulation"
              ;;
            mobile-safari)
              INSTALL_BROWSER="webkit"
              echo "üì± Mobile Safari will use WebKit with mobile device emulation"
              ;;
            chromium|firefox|webkit)
              INSTALL_BROWSER="$BROWSER"
              echo "üñ•Ô∏è Desktop browser: $INSTALL_BROWSER"
              ;;
            *)
              echo "‚ùå Unknown browser type: $BROWSER"
              exit 1
              ;;
          esac

          # Always install browsers to ensure tests can run
          echo "üîÑ Installing Playwright browser: $INSTALL_BROWSER..."
          npx playwright install --with-deps "$INSTALL_BROWSER"
          echo "‚úÖ Browser installed: $INSTALL_BROWSER"

      - name: üîç Validate Required Secrets
        run: |
          echo "üîê Validating Required Secrets for E2E Tests"
          echo "======================================="
          echo ""

          # Track missing secrets
          MISSING_CRITICAL=0
          MISSING_OPTIONAL=0

          # Function to check secret availability without exposing values
          check_secret() {
            local secret_name="$1"
            local secret_value="$2"
            local is_critical="$3"  # "critical" or "optional"

            if [ -z "$secret_value" ] || [ "$secret_value" = "" ]; then
              echo "‚ùå $secret_name: NOT AVAILABLE"
              if [ "$is_critical" = "critical" ]; then
                MISSING_CRITICAL=$((MISSING_CRITICAL + 1))
              else
                MISSING_OPTIONAL=$((MISSING_OPTIONAL + 1))
              fi
              return 1
            else
              # Show only first 3 and last 3 characters for verification
              local masked_value="${secret_value:0:3}***${secret_value: -3}"
              echo "‚úÖ $secret_name: AVAILABLE (${masked_value})"
              return 0
            fi
          }

          echo "üîë Critical Admin Authentication Secrets:"
          check_secret "TEST_ADMIN_PASSWORD" "${{ secrets.TEST_ADMIN_PASSWORD }}" "critical"
          check_secret "ADMIN_SECRET" "${{ secrets.ADMIN_SECRET }}" "critical"

          echo ""
          echo "üóÑÔ∏è Critical Database Configuration Secrets:"
          check_secret "TURSO_DATABASE_URL" "${{ secrets.TURSO_DATABASE_URL }}" "critical"
          check_secret "TURSO_AUTH_TOKEN" "${{ secrets.TURSO_AUTH_TOKEN }}" "critical"

          echo ""
          echo "‚òÅÔ∏è Optional Google Drive Service Account Secrets:"
          check_secret "GOOGLE_SERVICE_ACCOUNT_EMAIL" "${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}" "optional"
          check_secret "GOOGLE_PRIVATE_KEY" "${{ secrets.GOOGLE_PRIVATE_KEY }}" "optional"
          check_secret "GOOGLE_DRIVE_GALLERY_FOLDER_ID" "${{ secrets.GOOGLE_DRIVE_GALLERY_FOLDER_ID }}" "optional"

          echo ""
          echo "üí≥ Optional Payment Processing Secrets:"
          check_secret "STRIPE_PUBLISHABLE_KEY" "${{ secrets.STRIPE_PUBLISHABLE_KEY }}" "optional"
          check_secret "STRIPE_SECRET_KEY" "${{ secrets.STRIPE_SECRET_KEY }}" "optional"
          check_secret "STRIPE_WEBHOOK_SECRET" "${{ secrets.STRIPE_WEBHOOK_SECRET }}" "optional"

          echo ""
          echo "üìß Optional Email Service Secrets:"
          check_secret "BREVO_API_KEY" "${{ secrets.BREVO_API_KEY }}" "optional"
          check_secret "BREVO_NEWSLETTER_LIST_ID" "${{ secrets.BREVO_NEWSLETTER_LIST_ID }}" "optional"
          check_secret "BREVO_WEBHOOK_SECRET" "${{ secrets.BREVO_WEBHOOK_SECRET }}" "optional"

          echo ""
          echo "======================================="
          echo "üìä Secret Validation Summary:"
          echo "  Critical secrets missing: $MISSING_CRITICAL"
          echo "  Optional secrets missing: $MISSING_OPTIONAL"

          if [ $MISSING_CRITICAL -gt 0 ]; then
            echo ""
            echo "‚ùå CRITICAL: $MISSING_CRITICAL required secret(s) are missing!"
            echo "E2E tests cannot run without these secrets."
            echo "Please configure the missing secrets in GitHub repository settings."
            exit 1
          elif [ $MISSING_OPTIONAL -gt 0 ]; then
            echo ""
            echo "‚ö†Ô∏è  WARNING: $MISSING_OPTIONAL optional secret(s) are missing."
            echo "Some tests may be skipped, but core tests will run."
          else
            echo ""
            echo "‚úÖ All required secrets are configured correctly!"
          fi
          echo "======================================="

      - name: üåê Test Preview URL Connectivity
        run: |
          echo "üåê Preview URL Connectivity Test"
          echo "==============================="
          echo "Target URL: ${{ env.PREVIEW_URL }}"
          echo ""

          # Basic connectivity test
          echo "üîç Testing basic connectivity..."
          if curl -s --connect-timeout 10 --max-time 30 "${{ env.PREVIEW_URL }}" > /dev/null; then
            echo "‚úÖ Preview URL is accessible"
          else
            echo "‚ùå Preview URL is NOT accessible"
            echo "This may cause E2E test failures"
          fi

          # Test specific endpoints critical for E2E tests
          echo ""
          echo "üîç Testing critical API endpoints..."

          # Health check endpoint
          HEALTH_URL="${{ env.PREVIEW_URL }}/api/health/check"
          echo "Testing: $HEALTH_URL"
          if curl -s --connect-timeout 5 --max-time 15 "$HEALTH_URL" | grep -q "ok\|healthy\|success"; then
            echo "‚úÖ Health endpoint responding"
          else
            echo "‚ö†Ô∏è Health endpoint not responding properly"
          fi

          # Gallery API (for gallery tests)
          GALLERY_URL="${{ env.PREVIEW_URL }}/api/gallery"
          echo "Testing: $GALLERY_URL"
          GALLERY_RESPONSE=$(curl -s --connect-timeout 5 --max-time 15 "$GALLERY_URL" || echo "FAILED")
          if echo "$GALLERY_RESPONSE" | grep -q -E '(\[|\{|"photos"|"error")'; then
            echo "‚úÖ Gallery API responding"
          else
            echo "‚ö†Ô∏è Gallery API not responding properly"
          fi

          # Admin login page (for admin tests)
          ADMIN_URL="${{ env.PREVIEW_URL }}/pages/admin/login.html"
          echo "Testing: $ADMIN_URL"
          if curl -s --connect-timeout 5 --max-time 15 "$ADMIN_URL" | grep -q -i "admin\|login\|password"; then
            echo "‚úÖ Admin login page accessible"
          else
            echo "‚ö†Ô∏è Admin login page not accessible"
          fi

          echo ""
          echo "==============================="

      - name: üî¨ Secrets Configuration Debug
        run: |
          echo "üî¨ Secrets Configuration Debug"
          echo "============================="
          echo ""

          echo "üé≠ Playwright Configuration:"
          echo "  PLAYWRIGHT_BASE_URL: $PLAYWRIGHT_BASE_URL"
          echo "  PREVIEW_URL: $PREVIEW_URL"
          echo "  E2E_TEST_MODE: $E2E_TEST_MODE"
          echo "  NODE_ENV: $NODE_ENV"
          echo "  CI: $CI"

          echo ""
          echo "üîê Authentication Secrets (masked):"
          if [ -n "$TEST_ADMIN_PASSWORD" ]; then
            echo "  TEST_ADMIN_PASSWORD: ${TEST_ADMIN_PASSWORD:0:3}***${TEST_ADMIN_PASSWORD: -3}"
          else
            echo "  TEST_ADMIN_PASSWORD: ‚ùå NOT SET"
          fi

          if [ -n "$ADMIN_SECRET" ]; then
            echo "  ADMIN_SECRET: ${ADMIN_SECRET:0:3}***${ADMIN_SECRET: -3}"
          else
            echo "  ADMIN_SECRET: ‚ùå NOT SET"
          fi

          echo ""
          echo "üóÑÔ∏è Database Secrets (masked):"
          if [ -n "$TURSO_DATABASE_URL" ]; then
            echo "  TURSO_DATABASE_URL: ${TURSO_DATABASE_URL:0:10}***${TURSO_DATABASE_URL: -10}"
          else
            echo "  TURSO_DATABASE_URL: ‚ùå NOT SET"
          fi

          if [ -n "$TURSO_AUTH_TOKEN" ]; then
            echo "  TURSO_AUTH_TOKEN: ${TURSO_AUTH_TOKEN:0:5}***${TURSO_AUTH_TOKEN: -5}"
          else
            echo "  TURSO_AUTH_TOKEN: ‚ùå NOT SET"
          fi

          echo ""
          echo "‚òÅÔ∏è Google Drive Secrets (masked):"
          if [ -n "$GOOGLE_SERVICE_ACCOUNT_EMAIL" ]; then
            echo "  GOOGLE_SERVICE_ACCOUNT_EMAIL: ${GOOGLE_SERVICE_ACCOUNT_EMAIL:0:5}***${GOOGLE_SERVICE_ACCOUNT_EMAIL: -10}"
          else
            echo "  GOOGLE_SERVICE_ACCOUNT_EMAIL: ‚ùå NOT SET"
          fi

          if [ -n "$GOOGLE_PRIVATE_KEY" ]; then
            echo "  GOOGLE_PRIVATE_KEY: ${GOOGLE_PRIVATE_KEY:0:10}***[PRIVATE_KEY]"
          else
            echo "  GOOGLE_PRIVATE_KEY: ‚ùå NOT SET"
          fi

          if [ -n "$GOOGLE_DRIVE_GALLERY_FOLDER_ID" ]; then
            echo "  GOOGLE_DRIVE_GALLERY_FOLDER_ID: ${GOOGLE_DRIVE_GALLERY_FOLDER_ID:0:3}***${GOOGLE_DRIVE_GALLERY_FOLDER_ID: -3}"
          else
            echo "  GOOGLE_DRIVE_GALLERY_FOLDER_ID: ‚ùå NOT SET"
          fi

          echo ""
          echo "============================="

      - name: üé≠ Run E2E Tests
        id: run-tests
        env:
          # Admin Authentication Test Variables
          TEST_ADMIN_PASSWORD: ${{ secrets.TEST_ADMIN_PASSWORD }}
          ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD }}
          ADMIN_SECRET: ${{ secrets.ADMIN_SECRET }}
          # Database Configuration (using repository secret for URL)
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
          # Google Drive Service Account Configuration
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
          GOOGLE_DRIVE_GALLERY_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_GALLERY_FOLDER_ID }}
          # Payment Processing (if available)
          STRIPE_PUBLISHABLE_KEY: ${{ secrets.STRIPE_PUBLISHABLE_KEY }}
          STRIPE_SECRET_KEY: ${{ secrets.STRIPE_SECRET_KEY }}
          STRIPE_WEBHOOK_SECRET: ${{ secrets.STRIPE_WEBHOOK_SECRET }}
          # Email Service (if available)
          BREVO_API_KEY: ${{ secrets.BREVO_API_KEY }}
          BREVO_NEWSLETTER_LIST_ID: ${{ secrets.BREVO_NEWSLETTER_LIST_ID }}
          BREVO_WEBHOOK_SECRET: ${{ secrets.BREVO_WEBHOOK_SECRET }}
          # Vercel Configuration (for deployment operations)
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
          # GitHub Token (automatically provided by GitHub Actions)
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          # Test Environment Configuration
          E2E_TEST_MODE: 'true'
          NODE_ENV: 'test'
          # Dynamic URL variables (fixed: these are already set in the job's env section above)
        run: |
          # Determine the correct Playwright project name for the browser
          PROJECT_NAME="${{ matrix.browser }}"

          # Export NODE_OPTIONS for the Playwright process
          export NODE_OPTIONS="--max-old-space-size=2048"

          # Record start time
          start_time=$(date +%s%3N)

          # Validate test discovery before execution
          echo "üìã Test Discovery Validation"
          echo "=============================="
          npx playwright test --config tests/config/playwright-e2e-optimized.config.js --project="$PROJECT_NAME" --list 2>&1 | tee test_plan.log

          EXPECTED_TESTS=$(grep -oP '\d+(?=\s+tests?)' test_plan.log | head -1 || echo "0")
          echo "Expected tests to run: $EXPECTED_TESTS"
          echo ""

          if [ "$EXPECTED_TESTS" -eq 0 ]; then
            echo "‚ùå CRITICAL: No tests found to execute!"
            echo ""
            echo "This indicates:"
            echo "  - Test pattern filter matched no tests"
            echo "  - Test files are not being discovered"
            echo "  - Playwright configuration issue"
            echo "  - testDir path is incorrect"
            echo ""
            echo "üîç Debug Information:"
            echo "  Project: $PROJECT_NAME"
            echo "  Config: tests/config/playwright-e2e-optimized.config.js"
            echo "  testDir: ./tests/e2e/flows"
            echo ""
            echo "üìÅ Available test files:"
            find tests/e2e/flows -name "*.test.js" 2>/dev/null | head -10 || echo "  ERROR: Cannot find test directory!"
            exit 1
          fi

          echo "‚úÖ Test discovery successful: $EXPECTED_TESTS test(s) will be executed"
          echo ""

          # Build Playwright test command with explicit file paths (not grep pattern)
          TEST_SUITE="${{ needs.build-test-matrix.outputs.test-suite }}"

          # Map test suite to explicit file paths
          TEST_FILES=""
          case "$TEST_SUITE" in
            "standard")
              TEST_FILES="tests/e2e/flows/basic-navigation.test.js tests/e2e/flows/cart-functionality.test.js tests/e2e/flows/newsletter-simple.test.js tests/e2e/flows/admin-auth.test.js tests/e2e/flows/gallery-basic.test.js tests/e2e/flows/registration-flow.test.js"
              ;;
            "advanced"|"")
              TEST_FILES="" # Run all tests - no filter
              ;;
            "performance")
              TEST_FILES="tests/e2e/flows/gallery-browsing.test.js tests/e2e/flows/user-engagement.test.js"
              ;;
            "accessibility")
              TEST_FILES="tests/e2e/flows/mobile-registration-experience.test.js"
              ;;
            "security")
              TEST_FILES="tests/e2e/flows/admin-auth.test.js tests/e2e/flows/admin-dashboard.test.js tests/e2e/flows/ticket-validation.test.js"
              ;;
            *)
              echo "‚ö†Ô∏è Unknown test suite: $TEST_SUITE, defaulting to standard"
              TEST_FILES="tests/e2e/flows/basic-navigation.test.js tests/e2e/flows/cart-functionality.test.js tests/e2e/flows/newsletter-simple.test.js tests/e2e/flows/admin-auth.test.js tests/e2e/flows/gallery-basic.test.js tests/e2e/flows/registration-flow.test.js"
              ;;
          esac

          # Run Playwright with explicit file paths
          if [ -n "$TEST_FILES" ]; then
            echo "üéØ Running specific test files for suite: $TEST_SUITE"
            echo "üìÅ Files: $(echo $TEST_FILES | tr ' ' '\n' | wc -l) test files"
            npx playwright test --config tests/config/playwright-e2e-optimized.config.js --project="$PROJECT_NAME" $TEST_FILES 2>&1 | tee test_output.log
          else
            echo "üéØ Running all E2E tests (no file filter)"
            npx playwright test --config tests/config/playwright-e2e-optimized.config.js --project="$PROJECT_NAME" 2>&1 | tee test_output.log
          fi

          # Record end time
          end_time=$(date +%s%3N)
          duration=$((end_time - start_time))

          echo "üåê Target URL: ${{ env.PREVIEW_URL }}"
          echo "ü¶ä Browser Project: $PROJECT_NAME"
          TEST_EXIT_CODE=${PIPESTATUS[0]}

          # FIX #3: Check if Playwright failed to load config
          if grep -q "Cannot find module\|Error:" test_output.log; then
            echo ""
            echo "‚ùå CRITICAL: Playwright configuration error detected"
            grep -A 5 "Cannot find module\|Error:" test_output.log
            echo ""
            echo "Tests cannot run with configuration errors"
            exit 1
          fi

          # FIX #1: Extract test statistics with proper default values
          PASSED_TESTS=$(grep -oP '\d+(?= passed)' test_output.log 2>/dev/null | head -1)
          PASSED_TESTS=${PASSED_TESTS:-0}

          FAILED_TESTS=$(grep -oP '\d+(?= failed)' test_output.log 2>/dev/null | head -1)
          FAILED_TESTS=${FAILED_TESTS:-0}

          SKIPPED_TESTS=$(grep -oP '\d+(?= skipped)' test_output.log 2>/dev/null | head -1)
          SKIPPED_TESTS=${SKIPPED_TESTS:-0}

          TOTAL_TESTS=$((PASSED_TESTS + FAILED_TESTS + SKIPPED_TESTS))

          # Set outputs for downstream jobs
          echo "test_exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT
          echo "parsing_method=playwright_summary" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          echo "passing_tests=$PASSED_TESTS" >> $GITHUB_OUTPUT
          echo "failing_tests=$FAILED_TESTS" >> $GITHUB_OUTPUT
          echo "skipped_tests=$SKIPPED_TESTS" >> $GITHUB_OUTPUT
          echo "duration=$duration" >> $GITHUB_OUTPUT

          echo ""
          echo "üìä Test Execution Summary:"
          echo "  Exit Code: $TEST_EXIT_CODE"
          echo "  Tests Passed: $PASSED_TESTS"
          echo "  Tests Failed: $FAILED_TESTS"
          echo "  Tests Skipped: $SKIPPED_TESTS"
          echo "  Duration: ${duration}ms"

          # FIX #2: Validate test count matches expected pattern
          if [ -n "$TEST_PATTERN" ]; then
            EXPECTED_FILES=$(echo "$TEST_PATTERN" | tr '|' '\n' | wc -l)
            TOTAL_EXECUTED=$((PASSED_TESTS + FAILED_TESTS + SKIPPED_TESTS))

            if [ "$TOTAL_EXECUTED" -lt "$EXPECTED_FILES" ]; then
              echo ""
              echo "‚ùå ERROR: Expected $EXPECTED_FILES test files but only $TOTAL_EXECUTED executed"
              echo "This indicates a test configuration or loading issue"
              exit 1
            fi
          fi

          # Determine if tests actually passed or failed
          if [ $TEST_EXIT_CODE -eq 0 ]; then
            # Exit code 0 = success, BUT need to check if tests actually passed

            # CRITICAL FIX: Check FAILED_TESTS even when exit code is 0
            # Playwright can return 0 even with failures in some configurations
            if [ "$FAILED_TESTS" -gt 0 ]; then
              echo ""
              echo "‚ùå FAILED: $FAILED_TESTS test(s) failed for ${{ matrix.browser }}"
              echo "‚ö†Ô∏è  Exit code was 0 but tests failed - Playwright configuration issue!"
              echo ""
              echo "üìä Full test summary:"
              grep -E "([0-9]+ passed|[0-9]+ failed|[0-9]+ skipped)" test_output.log || echo "  No summary found in output"
              exit 1
            fi

            if [ "$PASSED_TESTS" -eq 0 ] && [ "$FAILED_TESTS" -eq 0 ]; then
              echo ""
              echo "‚ö†Ô∏è  WARNING: No tests were executed!"

              if [ "$SKIPPED_TESTS" -gt 0 ]; then
                echo "‚ùå CRITICAL: All $SKIPPED_TESTS tests were skipped!"
                echo ""
                echo "This indicates:"
                echo "  - Missing required secrets in Preview environment"
                echo "  - Preview deployment environment variables not configured"
                echo "  - E2E_TEST_MODE not set to 'true' in Vercel Preview"
                echo ""
                echo "üîß Required Actions:"
                echo "  1. Go to Vercel Dashboard ‚Üí Settings ‚Üí Environment Variables"
                echo "  2. Set E2E_TEST_MODE=true for Preview environment (NOT Production)"
                echo "  3. Configure TEST_ADMIN_PASSWORD and ADMIN_SECRET for Preview"
                echo "  4. Verify TURSO_DATABASE_URL and TURSO_AUTH_TOKEN are set for Preview"
                echo ""
                echo "üí° Note: All-skipped tests are NOT acceptable and indicate configuration issues"
                exit 1  # FAIL the job - all skipped is NOT acceptable
              else
                echo "‚ùå No tests executed and none skipped - configuration error!"
                exit 1
              fi
            else
              echo "‚úÖ All tests passed successfully for ${{ matrix.browser }}"
              exit 0
            fi
          else
            # Exit code != 0 = potential failure
            echo "üîç Non-zero exit code detected: $TEST_EXIT_CODE"

            # Check for actual test failures
            if [ "$FAILED_TESTS" -gt 0 ]; then
              echo ""
              echo "‚ùå FAILED: $FAILED_TESTS test(s) failed for ${{ matrix.browser }}"
              echo ""
              echo "üìä Full test summary:"
              grep -E "([0-9]+ passed|[0-9]+ failed|[0-9]+ skipped)" test_output.log || echo "  No summary found in output"
              echo ""
              echo "üîç Check the test output above for details on which tests failed."
              exit 1
            elif [ "$PASSED_TESTS" -gt 0 ]; then
              # Tests passed but exit code is non-zero
              # This can happen with Playwright's --grep when some files have no matching tests
              echo ""
              echo "‚ö†Ô∏è  Non-zero exit code BUT all tests passed ($PASSED_TESTS passed, 0 failed)"

              if [ -n "$TEST_PATTERN" ] && [ "$TEST_PATTERN" != "" ]; then
                echo "üìù This is expected with --grep filtering when some test files have no matches"
                echo "‚úÖ Marking as success since no tests actually failed"
                exit 0
              else
                echo "‚ö†Ô∏è  Unexpected: non-zero exit without grep filtering"
                echo "This might indicate an issue with the test runner"
                echo "‚ö†Ô∏è  Proceeding with caution - treating as success since tests passed"
                exit 0
              fi
            else
              # No passed tests, no failed tests, but non-zero exit code
              echo ""
              echo "‚ùå ERROR: No tests executed and exit code is non-zero"
              echo "This indicates a test infrastructure failure, not a test failure."
              echo ""
              echo "üîç Possible causes:"
              echo "  - Playwright configuration error"
              echo "  - Test file syntax errors"
              echo "  - Missing dependencies"
              echo "  - Browser launch failures"
              echo ""
              echo "üìã Available test files:"
              find tests/e2e/flows -name "*.test.js" | head -10
              exit 1
            fi
          fi

      - name: "üìä Create Test Metadata"
        if: always()
        run: |
          cat <<EOF > test-metadata.json
          {
            "workflow": "${{ github.workflow }}",
            "job": "${{ github.job }}",
            "run_id": "${{ github.run_id }}",
            "run_attempt": "${{ github.run_attempt }}",
            "test_type": "e2e",
            "matrix": $(echo '${{ toJSON(matrix) }}' | jq -c),
            "exit_code": ${{ steps.run-tests.outputs.test_exit_code || 'null' }},
            "parsing_method": "${{ steps.run-tests.outputs.parsing_method || 'unknown' }}",
            "counts": {
              "total": ${{ steps.run-tests.outputs.total_tests || '0' }},
              "passed": ${{ steps.run-tests.outputs.passing_tests || '0' }},
              "failed": ${{ steps.run-tests.outputs.failing_tests || '0' }},
              "skipped": ${{ steps.run-tests.outputs.skipped_tests || '0' }}
            },
            "duration_ms": ${{ steps.run-tests.outputs.duration || '0' }},
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "browser": "${{ matrix.browser }}",
            "preview_url": "${{ env.PREVIEW_URL }}"
          }
          EOF

          echo "Metadata created:"
          cat test-metadata.json | jq '.'

      - name: üìä Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.browser }}
          path: |
            playwright-report/
            test-results/
            test-metadata.json
          retention-days: 7

      - name: üìã Test Summary
        if: always()
        run: |
          echo "üìä E2E Test Summary (${{ matrix.browser }}):"
          echo "  Target URL: ${{ env.PREVIEW_URL }}"
          echo "  Test Status: ${{ job.status }}"
          echo "  Browser: ${{ matrix.browser }}"
          echo "  Test Suite: ${{ needs.build-test-matrix.outputs.test-suite }}"

          # Browser-specific debugging information
          case "${{ matrix.browser }}" in
            firefox)
              echo "ü¶ä Firefox-specific debug information:"
              echo "  Firefox timeout: 20s actions, 45s navigation"
              echo "  User Agent: Playwright-E2E-Tests-Preview-Firefox"
              ;;
            webkit)
              echo "üß≠ Safari/WebKit debug information:"
              echo "  WebKit project with Safari engine"
              ;;
            mobile-*)
              echo "üì± Mobile browser debug information:"
              echo "  Using device emulation for mobile testing"
              ;;
          esac

          # Fixed: Check if test-results is a directory, not a file
          if [ -d "test-results" ]; then
            echo "  Timeout errors:"
            find test-results -name "*.txt" -exec grep -l "timeout\|TimeoutError" {} \; 2>/dev/null || echo "    No timeout errors found"
          fi

          if [ -f "playwright-report/index.html" ]; then
            echo "  Report: Available in artifacts"

            # Count test results for summary
            TOTAL_TESTS=$(find test-results -name "*.zip" 2>/dev/null | wc -l || echo "0")
            echo "  Test artifacts: $TOTAL_TESTS"
          fi

  # Collect and report results
  e2e-results:
    name: üìä E2E Results Summary
    runs-on: ubuntu-latest
    needs: [e2e-tests, build-test-matrix]
    if: always()
    steps:
      - name: üì• Checkout
        uses: actions/checkout@v4

      - name: üìä Collect Test Results
        id: collect
        uses: ./.github/actions/collect-test-results
        with:
          # Fixed: Add missing test-type input
          test-type: "e2e"
          test-results-path: "test-results-summary.json"

      - name: üìÑ Read Summary JSON
        id: read-results
        run: |
          echo "json=$(jq -c . test-results-summary.json 2>/dev/null || echo '{}')" >> "$GITHUB_OUTPUT"

      - name: üí¨ Post Results to PR
        if: github.event_name == 'deployment_status' && github.event.deployment.environment == 'Preview'
        uses: ./.github/actions/post-test-comment
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          commit-sha: ${{ github.sha }}
          workflow-run-id: ${{ github.run_id }}
          e2e-test-results: ${{ steps.read-results.outputs.json }}

      - name: üìã Final Summary
        run: |
          echo "üéØ E2E Test Suite Complete"
          echo "  Suite: ${{ needs.build-test-matrix.outputs.test-suite }}"
          echo "  Browsers: ${{ needs.build-test-matrix.outputs.browsers-json }}"
          echo "  Pattern: ${{ needs.build-test-matrix.outputs.test-pattern }}"
          echo "  Status: ${{ needs.e2e-tests.result }}"

          # Handle case where e2e-tests job was skipped
          if [ "${{ needs.e2e-tests.result }}" = "skipped" ]; then
            echo "‚ö†Ô∏è E2E tests were skipped - no deployment URL available"
            echo "This can happen when the deployment trigger fails or the URL extraction job is skipped."
            exit 0  # Don't fail the workflow for skipped tests
          elif [ "${{ needs.e2e-tests.result }}" = "success" ]; then
            echo "‚úÖ All E2E tests passed!"
          else
            echo "‚ùå Some E2E tests failed - check individual job results"
            exit 1
          fi
