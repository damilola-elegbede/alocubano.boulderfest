---
name: ðŸŽ­ E2E Tests (Vercel Preview)

# Enhanced E2E workflow that tests against live Vercel preview deployments
# instead of starting local servers. Provides better reliability and 
# production-like testing environment.

on:
  pull_request:
    branches: [main, develop, feature/phase4-*]
    types: [opened, synchronize, reopened, ready_for_review]
  push:
    branches: [main, feature/phase4-*]
  schedule:
    # Nightly comprehensive testing at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'standard'
        type: choice
        options:
          - 'standard'      # Core flows
          - 'advanced'      # All tests
          - 'performance'   # Performance-focused
          - 'accessibility' # WCAG compliance
          - 'security'      # Security-focused
      test_pattern:
        description: 'Test pattern filter (e.g., "gallery" or "admin")'
        required: false
        default: ''
        type: string
      browsers:
        description: 'Browser matrix to test'
        required: false
        default: 'standard'
        type: choice
        options:
          - 'standard'     # chromium, firefox
          - 'extended'     # chromium, firefox, webkit  
          - 'full'         # all including mobile
          - 'chromium-only' # Chromium only
      preview_url:
        description: 'Override preview URL (optional)'
        required: false
        default: ''
        type: string

# Prevent concurrent E2E runs for the same PR/branch
concurrency:
  group: e2e-preview-${{ github.head_ref || github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: "20"
  NODE_ENV: test
  CI: true
  # Memory optimization
  NODE_OPTIONS: "--max-old-space-size=4096"
  # Preview testing configuration
  E2E_PREVIEW_MODE: true
  E2E_TEST_MODE: true

jobs:
  # Extract Vercel preview URL and validate deployment
  extract-preview-url:
    name: ðŸ”— Extract Vercel Preview URL
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      preview_url: ${{ steps.extract.outputs.preview_url }}
      deployment_ready: ${{ steps.extract.outputs.deployment_ready }}
      should_run_tests: ${{ steps.validate.outputs.should_run_tests }}
    
    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2

      - name: ðŸ”§ Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ðŸ“¦ Install Dependencies (Minimal)
        run: |
          # Install only what's needed for URL extraction
          npm ci --prefer-offline --no-audit --no-fund --production=false
          echo "âœ… Dependencies installed for URL extraction"

      - name: ðŸ”— Extract Vercel Preview URL
        id: extract
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_PR_NUMBER: ${{ github.event.pull_request.number }}
          GITHUB_SHA: ${{ github.sha }}
          PREVIEW_URL_OVERRIDE: ${{ inputs.preview_url }}
        run: |
          echo "ðŸ”— Extracting Vercel preview URL..."
          
          # Use override URL if provided
          if [ -n "$PREVIEW_URL_OVERRIDE" ]; then
            echo "ðŸŽ¯ Using manual preview URL override: $PREVIEW_URL_OVERRIDE"
            PREVIEW_URL="$PREVIEW_URL_OVERRIDE"
          else
            # Extract URL using our utility
            echo "ðŸ¤– Extracting URL from GitHub PR/deployments..."
            PREVIEW_URL=$(node scripts/get-vercel-preview-url.js 2>/dev/null || echo "")
          fi
          
          if [ -n "$PREVIEW_URL" ]; then
            echo "âœ… Preview URL extracted: $PREVIEW_URL"
            echo "preview_url=$PREVIEW_URL" >> $GITHUB_OUTPUT
            
            # Validate deployment is ready
            echo "ðŸ¥ Validating deployment readiness..."
            for i in {1..20}; do
              if curl -f --max-time 10 "$PREVIEW_URL/api/health/check" >/dev/null 2>&1; then
                echo "âœ… Deployment is ready (attempt $i/20)"
                echo "deployment_ready=true" >> $GITHUB_OUTPUT
                break
              fi
              if [ $i -eq 20 ]; then
                echo "âŒ Deployment not ready after 20 attempts"
                echo "deployment_ready=false" >> $GITHUB_OUTPUT
                exit 1
              fi
              echo "â³ Attempt $i/20: Deployment not ready, waiting 15s..."
              sleep 15
            done
          else
            echo "âŒ Could not extract preview URL"
            echo "preview_url=" >> $GITHUB_OUTPUT
            echo "deployment_ready=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: ðŸ” Validate Test Execution
        id: validate
        run: |
          SHOULD_RUN="true"
          
          # Skip for draft PRs unless explicit pattern provided
          if [ "${{ github.event.pull_request.draft }}" == "true" ] && [ -z "${{ inputs.test_pattern }}" ]; then
            echo "Draft PR detected - skipping E2E tests"
            SHOULD_RUN="false"
          fi
          
          # Require successful deployment
          if [ "${{ steps.extract.outputs.deployment_ready }}" != "true" ]; then
            echo "Deployment not ready - skipping E2E tests"
            SHOULD_RUN="false"
          fi
          
          echo "should_run_tests=$SHOULD_RUN" >> $GITHUB_OUTPUT
          echo "ðŸŽ¯ Will run E2E tests: $SHOULD_RUN"

  # Run E2E tests against Vercel preview deployment
  e2e-preview-tests:
    name: ðŸŽ­ E2E Tests (${{ matrix.browser-name }})
    runs-on: ubuntu-latest
    needs: extract-preview-url
    if: needs.extract-preview-url.outputs.should_run_tests == 'true'
    timeout-minutes: 25
    
    strategy:
      fail-fast: false
      max-parallel: 2
      matrix:
        include: >-
          ${{
            (inputs.browsers == 'chromium-only' && fromJson('[
              {"browser": "chromium", "browser-name": "Chrome", "timeout-minutes": 15}
            ]')) ||
            (inputs.browsers == 'standard' && fromJson('[
              {"browser": "chromium", "browser-name": "Chrome", "timeout-minutes": 20},
              {"browser": "firefox", "browser-name": "Firefox", "timeout-minutes": 25}
            ]')) ||
            (inputs.browsers == 'extended' && fromJson('[
              {"browser": "chromium", "browser-name": "Chrome", "timeout-minutes": 20},
              {"browser": "firefox", "browser-name": "Firefox", "timeout-minutes": 25},
              {"browser": "webkit", "browser-name": "Safari", "timeout-minutes": 25}
            ]')) ||
            fromJson('[
              {"browser": "chromium", "browser-name": "Chrome", "timeout-minutes": 20},
              {"browser": "firefox", "browser-name": "Firefox", "timeout-minutes": 25},
              {"browser": "webkit", "browser-name": "Safari", "timeout-minutes": 25},
              {"browser": "mobile-chrome", "browser-name": "Mobile Chrome", "timeout-minutes": 25},
              {"browser": "mobile-safari", "browser-name": "Mobile Safari", "timeout-minutes": 25}
            ]')
          }}

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ”§ Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ðŸ“¦ Install Dependencies
        run: |
          npm ci --prefer-offline --no-audit --no-fund
          echo "âœ… Dependencies installed for E2E testing"

      - name: ðŸŽ­ Cache Playwright Browsers
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-preview-${{ runner.os }}-${{ hashFiles('package-lock.json') }}-${{ matrix.browser }}
          restore-keys: |
            playwright-preview-${{ runner.os }}-${{ matrix.browser }}

      - name: ðŸŽ¬ Install Playwright Browsers (Selective)
        if: steps.playwright-cache.outputs.cache-hit != 'true'
        run: |
          echo "Installing Playwright browser: ${{ matrix.browser }}"
          npx playwright install ${{ matrix.browser }} --with-deps
          echo "âœ… Browser ${{ matrix.browser }} installed"

      - name: ðŸŽ¬ Update Browser Dependencies
        if: steps.playwright-cache.outputs.cache-hit == 'true'
        run: |
          echo "Using cached browsers, updating system dependencies..."
          npx playwright install-deps ${{ matrix.browser }}

      - name: ðŸ“ Prepare Preview Test Environment
        env:
          PREVIEW_URL: ${{ needs.extract-preview-url.outputs.preview_url }}
        run: |
          echo "ðŸŽ¯ Configuring E2E tests for preview deployment"
          echo "Preview URL: $PREVIEW_URL"
          
          # Create directories
          mkdir -p test-results playwright-report-preview .tmp
          
          # Set environment variables for testing
          echo "PREVIEW_URL=$PREVIEW_URL" >> $GITHUB_ENV
          echo "PLAYWRIGHT_BASE_URL=$PREVIEW_URL" >> $GITHUB_ENV
          echo "E2E_PREVIEW_MODE=true" >> $GITHUB_ENV
          echo "E2E_TEST_MODE=true" >> $GITHUB_ENV
          echo "PLAYWRIGHT_BROWSER=${{ matrix.browser }}" >> $GITHUB_ENV
          
          # Create preview environment file
          cat > .env.preview << EOF
          # E2E Preview Testing Configuration
          PREVIEW_URL=$PREVIEW_URL
          PLAYWRIGHT_BASE_URL=$PREVIEW_URL
          E2E_PREVIEW_MODE=true
          E2E_TEST_MODE=true
          NODE_ENV=test
          CI=true
          
          # Test credentials
          TEST_ADMIN_PASSWORD=${{ secrets.TEST_ADMIN_PASSWORD || 'test-admin-password' }}
          ADMIN_SECRET=${{ secrets.ADMIN_SECRET_TEST || 'test-admin-secret-key-minimum-32-characters' }}
          
          # Browser configuration
          PLAYWRIGHT_BROWSER=${{ matrix.browser }}
          
          # Test suite configuration
          TEST_SUITE=${{ inputs.test_suite || 'standard' }}
          PERFORMANCE_TESTING=${{ inputs.test_suite == 'performance' }}
          ACCESSIBILITY_TESTING=${{ inputs.test_suite == 'accessibility' }}
          SECURITY_TESTING=${{ inputs.test_suite == 'security' }}
          EOF
          
          echo "âœ… Preview environment configured"

      - name: ðŸ§ª Run E2E Tests (Preview Mode)
        env:
          PREVIEW_URL: ${{ needs.extract-preview-url.outputs.preview_url }}
          PLAYWRIGHT_BASE_URL: ${{ needs.extract-preview-url.outputs.preview_url }}
          PLAYWRIGHT_BROWSER: ${{ matrix.browser }}
          NODE_OPTIONS: ${{ env.NODE_OPTIONS }}
          # Test configuration
          TEST_SUITE: ${{ inputs.test_suite || 'standard' }}
          PERFORMANCE_TESTING: ${{ inputs.test_suite == 'performance' }}
          ACCESSIBILITY_TESTING: ${{ inputs.test_suite == 'accessibility' }}
          SECURITY_TESTING: ${{ inputs.test_suite == 'security' }}
          # Test credentials
          TEST_ADMIN_PASSWORD: ${{ secrets.TEST_ADMIN_PASSWORD || 'test-admin-password' }}
          ADMIN_SECRET: ${{ secrets.ADMIN_SECRET_TEST || 'test-admin-secret-key-minimum-32-characters' }}
        run: |
          echo "ðŸ§ª Running E2E tests against Vercel preview deployment"
          echo "Preview URL: $PREVIEW_URL"
          echo "Browser: ${{ matrix.browser-name }}"
          echo "Test Suite: ${{ inputs.test_suite || 'standard' }}"
          
          # Construct test command using preview configuration
          TEST_CMD="npx playwright test --config=playwright-e2e-preview.config.js --project=${{ matrix.browser }}"
          
          # Add test pattern if specified
          if [ -n "${{ inputs.test_pattern }}" ]; then
            TEST_CMD="$TEST_CMD tests/e2e/flows/*${{ inputs.test_pattern }}*"
            echo "ðŸ“Š Running filtered tests: ${{ inputs.test_pattern }}"
          elif [ "${{ inputs.test_suite }}" == "performance" ]; then
            TEST_CMD="$TEST_CMD --grep=\"performance|load|gallery-browsing\""
            echo "âš¡ Running performance-focused tests"
          elif [ "${{ inputs.test_suite }}" == "accessibility" ]; then
            TEST_CMD="$TEST_CMD --grep=\"accessibility|mobile-registration\""
            echo "â™¿ Running accessibility compliance tests"
          elif [ "${{ inputs.test_suite }}" == "security" ]; then
            TEST_CMD="$TEST_CMD --grep=\"security|admin|stripe\""
            echo "ðŸ”’ Running security-focused tests"
          elif [ "${{ inputs.test_suite }}" == "standard" ]; then
            TEST_CMD="$TEST_CMD --grep=\"basic-navigation|cart-functionality|registration-flow|admin-auth|gallery-basic|newsletter-simple\""
            echo "ðŸ“Š Running standard test suite (core flows)"
          fi
          
          # Configure reporters and timeouts
          TEST_CMD="$TEST_CMD --reporter=list,html --timeout=120000 --retries=2"
          
          echo "ðŸ“‹ Executing: $TEST_CMD"
          
          # Run tests with timeout protection
          if timeout 20m $TEST_CMD; then
            echo "âœ… E2E tests completed successfully for ${{ matrix.browser-name }} against preview deployment"
          else
            EXIT_CODE=$?
            echo "âŒ E2E tests failed for ${{ matrix.browser-name }} (exit code: $EXIT_CODE)"
            exit $EXIT_CODE
          fi

      - name: ðŸ“Š Analyze Preview Test Results
        if: always()
        run: |
          echo "ðŸ“Š Analyzing preview test results for ${{ matrix.browser-name }}..."
          echo "Preview URL tested: ${{ needs.extract-preview-url.outputs.preview_url }}"
          
          # Check for test results
          if [ -d "playwright-report-preview" ]; then
            echo "âœ… Test reports generated"
            
            # Count test files
            TEST_COUNT=$(find playwright-report-preview -name "*.html" | wc -l)
            echo "Test report files: $TEST_COUNT"
          fi
          
          # Check for failures
          if [ "${{ job.status }}" != "success" ] && [ -d "test-results" ]; then
            echo "âŒ Test failures detected against preview deployment"
            find test-results -name "*.png" -o -name "*.webm" | head -10 | while read -r file; do
              echo "  Artifact: $(basename "$file")"
            done
          fi

      - name: ðŸ“¤ Upload Preview Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-preview-results-${{ matrix.browser }}-${{ github.run_number }}
          path: |
            playwright-report-preview/
            test-results/
          retention-days: ${{ github.event_name == 'schedule' && 14 || 7 }}
          if-no-files-found: ignore

      - name: ðŸ“¸ Upload Failure Artifacts
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-preview-failures-${{ matrix.browser }}-${{ github.run_number }}
          path: |
            test-results/
            playwright-report-preview/
            .env.preview
            .tmp/
          retention-days: 14
          if-no-files-found: ignore

  # Results summary and reporting
  preview-test-results:
    name: ðŸ“‹ Preview Test Results Summary
    runs-on: ubuntu-latest
    needs: [extract-preview-url, e2e-preview-tests]
    if: always()
    timeout-minutes: 5

    steps:
      - name: ðŸ“Š Generate Preview Test Summary
        run: |
          echo "# ðŸŽ­ E2E Preview Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Preview URL**: ${{ needs.extract-preview-url.outputs.preview_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployment Ready**: ${{ needs.extract-preview-url.outputs.deployment_ready }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Suite**: ${{ inputs.test_suite || 'standard' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Browser Matrix**: ${{ inputs.browsers || 'standard' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Test Pattern**: ${{ inputs.test_pattern || 'All tests' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Test Execution Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **URL Extraction**: ${{ needs.extract-preview-url.result == 'success' && 'âœ… Success' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **E2E Tests**: ${{ needs.e2e-preview-tests.result == 'success' && 'âœ… Passed' || (needs.e2e-preview-tests.result == 'failure' && 'âŒ Failed' || 'â­ï¸ Skipped') }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## ðŸŒŸ Preview Testing Benefits" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Production-like Environment**: Tests run against actual Vercel deployment" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **No Local Server Conflicts**: Eliminates port conflicts and resource issues" >> $GITHUB_STEP_SUMMARY  
          echo "âœ… **Better Reliability**: No server startup failures or timeouts" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Automatic Cleanup**: Preview deployments are ephemeral" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **True CI/CD Integration**: Tests exactly what will be deployed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.e2e-preview-tests.result }}" == "success" ]; then
            echo "## âœ… All Tests Passed!" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ðŸŽ‰ Your preview deployment has passed all E2E quality gates." >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.e2e-preview-tests.result }}" == "failure" ]; then
            echo "## âŒ Test Failures Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Some E2E tests failed against the preview deployment:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ” Debugging Steps:" >> $GITHUB_STEP_SUMMARY
            echo "1. Check test artifacts for detailed logs and screenshots" >> $GITHUB_STEP_SUMMARY
            echo "2. Visit preview URL directly: [${{ needs.extract-preview-url.outputs.preview_url }}](${{ needs.extract-preview-url.outputs.preview_url }})" >> $GITHUB_STEP_SUMMARY
            echo "3. Run locally: \`PREVIEW_URL=${{ needs.extract-preview-url.outputs.preview_url }} npm run test:e2e:preview\`" >> $GITHUB_STEP_SUMMARY
            echo "4. Check deployment logs in Vercel dashboard" >> $GITHUB_STEP_SUMMARY
          fi