name: "ğŸ” Test Result Validation"

on:
  workflow_run:
    workflows:
      - "ğŸ§ª Unit Tests"
      - "ğŸ”— Integration Tests"
      - "ğŸ­ E2E Tests - Preview Deployments"
      - "ğŸ›¡ï¸ Quality Gates & Code Standards"
    types: [completed]
  workflow_dispatch:
    inputs:
      debug:
        description: 'Enable debug output'
        required: false
        default: 'false'
        type: boolean

concurrency:
  group: test-validation-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false  # Let validations complete

permissions:
  contents: read
  actions: read  # Download artifacts
  pull-requests: write  # Post comments
  checks: write  # Create check runs

env:
  NODE_VERSION: "20.19.5"

jobs:
  validate-test-results:
    name: "ğŸ” Validate Test Results for False Positives"
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion != 'cancelled' || github.event_name == 'workflow_dispatch' }}
    timeout-minutes: 10

    steps:
      - name: "ğŸ“¥ Checkout Code"
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha || github.sha }}

      - name: "ğŸ”§ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: "ğŸ“¦ Install Dependencies"
        run: npm ci --prefer-offline --no-audit

      - name: "ğŸ“ Create Validation Directory"
        run: mkdir -p .tmp/test-validation

      - name: "ğŸ“¥ Download Test Artifacts"
        id: download
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          path: .tmp/test-validation
          pattern: '*-test-results-*'
          merge-multiple: true
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}

      - name: "ğŸ“Š List Downloaded Artifacts"
        run: |
          echo "=== Artifacts Directory Structure ==="
          ls -laR .tmp/test-validation || echo "No artifacts directory found"
          echo ""
          echo "=== Test Metadata Files ==="
          find .tmp/test-validation -name "test-metadata.json" -type f -exec echo "Found: {}" \; || echo "No metadata files found"
          echo ""
          echo "=== Artifact Count ==="
          find .tmp/test-validation -name "test-metadata.json" -type f | wc -l

      - name: "ğŸ” Validate Test Results"
        id: validate
        run: |
          echo "Running test result validation..."

          # Run validation and capture output
          node scripts/validate-test-results.js > validation-report.json 2>&1 || true

          # Check if report was generated
          if [ -f validation-report.json ]; then
            echo "Validation report generated successfully"
            cat validation-report.json

            # Extract exit code from report
            EXIT_CODE=$(jq -r '.exit_code // 1' validation-report.json)
            echo "exit_code=${EXIT_CODE}" >> $GITHUB_OUTPUT

            # Extract summary for output
            P1_COUNT=$(jq -r '.summary.priority1_issues // 0' validation-report.json)
            P2_COUNT=$(jq -r '.summary.priority2_issues // 0' validation-report.json)
            P3_COUNT=$(jq -r '.summary.priority3_issues // 0' validation-report.json)

            echo "p1_issues=${P1_COUNT}" >> $GITHUB_OUTPUT
            echo "p2_issues=${P2_COUNT}" >> $GITHUB_OUTPUT
            echo "p3_issues=${P3_COUNT}" >> $GITHUB_OUTPUT

            # Set status based on issues
            if [ "${P1_COUNT}" -gt 0 ]; then
              echo "status=FAIL" >> $GITHUB_OUTPUT
              echo "âŒ CRITICAL: ${P1_COUNT} P1 issue(s) detected"
            elif [ "${P2_COUNT}" -gt 0 ]; then
              echo "status=WARN" >> $GITHUB_OUTPUT
              echo "âš ï¸ WARNING: ${P2_COUNT} P2 issue(s) detected"
            else
              echo "status=PASS" >> $GITHUB_OUTPUT
              echo "âœ… PASS: No critical issues detected"
            fi
          else
            echo "âŒ Failed to generate validation report"
            echo "exit_code=1" >> $GITHUB_OUTPUT
            echo "status=ERROR" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: "ğŸ“ Generate PR Comment"
        if: github.event.workflow_run.event == 'pull_request'
        id: generate-comment
        run: |
          node -e "
            const fs = require('fs');

            if (!fs.existsSync('validation-report.json')) {
              console.log('No validation report found');
              process.exit(1);
            }

            const report = JSON.parse(fs.readFileSync('validation-report.json', 'utf8'));

            // Build comment markdown
            let comment = '## ğŸ” Test Result Validation Report\n\n';

            // Status badge
            const status = report.validation_status;
            const badge = status === 'FAIL' ? 'ğŸ”´ FAIL' :
                         status === 'WARN' ? 'ğŸŸ¡ WARN' : 'ğŸŸ¢ PASS';
            comment += \`**Status:** \${badge}\n\n\`;

            // Summary
            comment += '### Summary\n\n';
            comment += '| Metric | Value |\n';
            comment += '|--------|-------|\n';
            comment += \`| Total Tests Validated | \${report.summary.total_tests_validated} |\n\`;
            comment += \`| Metadata Files | \${report.summary.metadata_files_processed} |\n\`;
            comment += \`| P1 Critical Issues | \${report.summary.priority1_issues} |\n\`;
            comment += \`| P2 Warnings | \${report.summary.priority2_issues} |\n\`;
            comment += \`| P3 Info | \${report.summary.priority3_issues} |\n\n\`;

            // Issues
            if (report.issues && report.issues.length > 0) {
              comment += '### Issues Detected\n\n';

              // Group by severity
              const p1 = report.issues.filter(i => i.severity === 'P1');
              const p2 = report.issues.filter(i => i.severity === 'P2');
              const p3 = report.issues.filter(i => i.severity === 'P3');

              if (p1.length > 0) {
                comment += '#### ğŸ”´ Critical Issues (P1)\n\n';
                p1.forEach((issue, idx) => {
                  comment += \`\${idx + 1}. **\${issue.pattern}** - \${issue.workflow}\n\`;
                  comment += \`   - \${issue.description}\n\`;
                  comment += \`   - ğŸ’¡ *\${issue.recommendation}*\n\n\`;
                });
              }

              if (p2.length > 0) {
                comment += '#### ğŸŸ¡ Warnings (P2)\n\n';
                p2.forEach((issue, idx) => {
                  comment += \`\${idx + 1}. **\${issue.pattern}** - \${issue.workflow}\n\`;
                  comment += \`   - \${issue.description}\n\`;
                  comment += \`   - ğŸ’¡ *\${issue.recommendation}*\n\n\`;
                });
              }

              if (p3.length > 0) {
                comment += '<details>\n';
                comment += '<summary>â„¹ï¸ Informational Issues (P3)</summary>\n\n';
                p3.forEach((issue, idx) => {
                  comment += \`\${idx + 1}. **\${issue.pattern}** - \${issue.workflow}\n\`;
                  comment += \`   - \${issue.description}\n\`;
                  comment += \`   - ğŸ’¡ *\${issue.recommendation}*\n\n\`;
                });
                comment += '</details>\n\n';
              }
            } else {
              comment += 'âœ… No issues detected. All test results validated successfully.\n\n';
            }

            // Recommendations
            if (report.recommendations && report.recommendations.length > 0) {
              comment += '### Recommendations\n\n';
              report.recommendations.forEach(rec => {
                const icon = rec.priority === 'CRITICAL' ? 'ğŸ”´' :
                            rec.priority === 'HIGH' ? 'ğŸŸ¡' :
                            rec.priority === 'MEDIUM' ? 'â„¹ï¸' : 'âœ…';
                comment += \`\${icon} **\${rec.action}**\n\`;
                comment += \`   \${rec.description}\n\n\`;
              });
            }

            // Metadata summary
            if (report.metadata_summary && report.metadata_summary.length > 0) {
              comment += '<details>\n';
              comment += '<summary>ğŸ“Š Test Metadata Summary</summary>\n\n';
              comment += '| Workflow | Type | Environment | Total | Pass | Fail | Skip | Duration |\n';
              comment += '|----------|------|-------------|-------|------|------|------|----------|\n';
              report.metadata_summary.forEach(m => {
                const env = m.browser || m.nodeVersion || 'unknown';
                const duration = m.duration ? \`\${m.duration}s\` : 'N/A';
                comment += \`| \${m.workflow || 'unknown'} | \${m.testType || 'unknown'} | \${env} | \${m.total || 0} | \${m.passed || 0} | \${m.failed || 0} | \${m.skipped || 0} | \${duration} |\n\`;
              });
              comment += '\n</details>\n\n';
            }

            comment += '---\n';
            comment += \`*Validation completed at \${new Date().toISOString()}*\n\`;

            // Save comment to file
            fs.writeFileSync('pr-comment.md', comment);
            console.log('PR comment generated successfully');
          "

      - name: "ğŸ’¬ Post Results to PR"
        if: github.event.workflow_run.event == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            if (!fs.existsSync('pr-comment.md')) {
              console.log('No PR comment file found');
              return;
            }

            const comment = fs.readFileSync('pr-comment.md', 'utf8');

            // Get PR number from workflow run
            const { data: workflowRun } = await github.rest.actions.getWorkflowRun({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.payload.workflow_run.id
            });

            const prNumber = workflowRun.pull_requests[0]?.number;

            if (!prNumber) {
              console.log('No PR number found for this workflow run');
              return;
            }

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('ğŸ” Test Result Validation Report')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
              console.log('Updated existing PR comment');
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: prNumber,
                body: comment
              });
              console.log('Created new PR comment');
            }

      - name: "ğŸ“Š Create Check Run"
        if: always()
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            let conclusion = 'success';
            let summary = 'No validation report available';
            let text = '';

            if (fs.existsSync('validation-report.json')) {
              const report = JSON.parse(fs.readFileSync('validation-report.json', 'utf8'));

              // Determine conclusion
              if (report.summary.priority1_issues > 0) {
                conclusion = 'failure';
              } else if (report.summary.priority2_issues > 0) {
                conclusion = 'neutral';
              } else {
                conclusion = 'success';
              }

              // Build summary
              summary = `Validated ${report.summary.total_tests_validated} tests across ${report.summary.metadata_files_processed} test suites`;

              // Build details
              text = `## Validation Results\n\n`;
              text += `- P1 Critical Issues: ${report.summary.priority1_issues}\n`;
              text += `- P2 Warnings: ${report.summary.priority2_issues}\n`;
              text += `- P3 Info: ${report.summary.priority3_issues}\n\n`;

              if (report.issues && report.issues.length > 0) {
                text += `### Issues\n\n`;
                report.issues.forEach((issue, idx) => {
                  text += `${idx + 1}. [${issue.severity}] ${issue.pattern}: ${issue.description}\n`;
                });
              }
            }

            // Create check run
            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Test Result Validation',
              head_sha: context.payload.workflow_run?.head_sha || context.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: 'Test Result Validation',
                summary: summary,
                text: text
              }
            });

      - name: "ğŸ“¤ Upload Validation Report"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-report
          path: validation-report.json
          retention-days: 30

      - name: "âŒ Fail on Critical Issues"
        if: steps.validate.outputs.exit_code == '1'
        run: |
          echo "âŒ Test validation failed with critical issues"
          echo "P1 Issues: ${{ steps.validate.outputs.p1_issues }}"
          echo "P2 Issues: ${{ steps.validate.outputs.p2_issues }}"
          echo ""
          echo "Review the validation report for details."
          exit 1

      - name: "âš ï¸ Warn on Non-Critical Issues"
        if: steps.validate.outputs.exit_code == '0' && steps.validate.outputs.p2_issues > '0'
        run: |
          echo "âš ï¸ Test validation passed but found warnings"
          echo "P2 Issues: ${{ steps.validate.outputs.p2_issues }}"
          echo "P3 Issues: ${{ steps.validate.outputs.p3_issues }}"
          echo ""
          echo "Consider reviewing these issues before merging."

      - name: "âœ… Success"
        if: steps.validate.outputs.exit_code == '0' && steps.validate.outputs.p2_issues == '0'
        run: |
          echo "âœ… Test validation passed with no issues"
          echo "All test results validated successfully."
