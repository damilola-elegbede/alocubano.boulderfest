---
name: 🧪 Reusable Test Suite

# Comprehensive test execution with intelligent routing and optimization
# Supports multiple test types: unit, integration, e2e, smoke, performance
# Implements parallel execution, smart failure handling, and detailed reporting

on:
  workflow_call:
    inputs:
      test-type:
        description: 'Type of tests to run'
        required: true
        type: string  # unit, integration, e2e, smoke, performance, all
      test-pattern:
        description: 'Test pattern filter (glob or regex)'
        required: false
        type: string
        default: ''
      node-version:
        description: 'Node.js version'
        required: false
        type: string
        default: '20'
      test-environment:
        description: 'Test environment configuration'
        required: false
        type: string
        default: 'ci'  # local, ci, staging, production
      parallel-workers:
        description: 'Number of parallel test workers'
        required: false
        type: string
        default: '2'
      timeout-minutes:
        description: 'Test timeout in minutes'
        required: false
        type: string
        default: '15'
      fail-fast:
        description: 'Stop on first failure'
        required: false
        type: boolean
        default: false
      coverage-enabled:
        description: 'Enable code coverage collection'
        required: false
        type: boolean
        default: true
      retry-count:
        description: 'Number of test retries on failure'
        required: false
        type: string
        default: '1'
      database-type:
        description: 'Database type for testing'
        required: false
        type: string
        default: 'sqlite'  # sqlite, turso, memory
      memory-limit:
        description: 'Memory limit for test execution (MB)'
        required: false
        type: string
        default: '3072'
      artifacts-retention:
        description: 'Test artifacts retention in days'
        required: false
        type: string
        default: '7'
    outputs:
      test-result:
        description: 'Overall test execution result'
        value: ${{ jobs.test-execution.outputs.test-result }}
      tests-passed:
        description: 'Number of tests that passed'
        value: ${{ jobs.test-execution.outputs.tests-passed }}
      tests-failed:
        description: 'Number of tests that failed'
        value: ${{ jobs.test-execution.outputs.tests-failed }}
      coverage-percentage:
        description: 'Code coverage percentage'
        value: ${{ jobs.test-execution.outputs.coverage-percentage }}
      execution-time:
        description: 'Total test execution time'
        value: ${{ jobs.test-execution.outputs.execution-time }}
      performance-metrics:
        description: 'Performance metrics (JSON)'
        value: ${{ jobs.test-execution.outputs.performance-metrics }}

env:
  NODE_OPTIONS: --max-old-space-size=${{ inputs.memory-limit }}
  CI: true
  NODE_ENV: test

jobs:
  test-execution:
    name: 🧪 Test Suite (${{ inputs.test-type }})
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJson(inputs.timeout-minutes) }}
    outputs:
      test-result: ${{ steps.test-results.outputs.test-result }}
      tests-passed: ${{ steps.test-results.outputs.tests-passed }}
      tests-failed: ${{ steps.test-results.outputs.tests-failed }}
      coverage-percentage: ${{ steps.coverage.outputs.coverage-percentage }}
      execution-time: ${{ steps.timing.outputs.execution-time }}
      performance-metrics: ${{ steps.performance.outputs.metrics }}
    
    strategy:
      fail-fast: ${{ inputs.fail-fast }}
      matrix:
        include:
          # Dynamic matrix based on test type
          - test-config: ${{
              (inputs.test-type == 'unit' && fromJson('{"name": "Unit Tests", "command": "test:simple", "database": "sqlite", "timeout": 5}')) ||
              (inputs.test-type == 'integration' && fromJson('{"name": "Integration Tests", "command": "test:integration", "database": "sqlite", "timeout": 8}')) ||
              (inputs.test-type == 'e2e' && fromJson('{"name": "E2E Tests", "command": "test:e2e:ci", "database": "turso", "timeout": 20}')) ||
              (inputs.test-type == 'smoke' && fromJson('{"name": "Smoke Tests", "command": "test:smoke", "database": "memory", "timeout": 3}')) ||
              (inputs.test-type == 'performance' && fromJson('{"name": "Performance Tests", "command": "test:performance", "database": "sqlite", "timeout": 15}')) ||
              fromJson('{"name": "All Tests", "command": "test:all", "database": "sqlite", "timeout": 25}')
            }}

    steps:
      - name: ⏱️ Start Test Timer
        id: timer-start
        run: echo "start-time=$(date +%s)" >> $GITHUB_OUTPUT

      - name: 📥 Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: ${{ inputs.test-type == 'performance' && '0' || '1' }}

      - name: 🔧 Setup Test Environment
        uses: ./.github/workflows/reusable/npm-setup.yml
        with:
          node-version: ${{ inputs.node-version }}
          cache-strategy: 'aggressive'
          optimization-profile: ${{ inputs.test-environment }}
          memory-limit: ${{ inputs.memory-limit }}

      - name: 🗄️ Setup Test Database
        id: database
        run: |
          echo "🗄️ Configuring test database: ${{ matrix.test-config.database }}"
          mkdir -p data test-results
          
          case "${{ matrix.test-config.database }}" in
            "sqlite")
              export DATABASE_URL="file:./data/${{ inputs.test-type }}-test.db"
              echo "DATABASE_URL=$DATABASE_URL" >> $GITHUB_ENV
              npm run migrate:up 2>/dev/null || echo "Migration completed or not needed"
              ;;
            "turso")
              # Use Turso for E2E tests if configured
              if [ -n "${{ secrets.TURSO_DATABASE_URL }}" ]; then
                export DATABASE_URL="${{ secrets.TURSO_DATABASE_URL }}"
                export TURSO_AUTH_TOKEN="${{ secrets.TURSO_AUTH_TOKEN }}"
              else
                echo "⚠️ Turso not configured, falling back to SQLite"
                export DATABASE_URL="file:./data/${{ inputs.test-type }}-test.db"
                npm run migrate:up 2>/dev/null || echo "Migration completed or not needed"
              fi
              echo "DATABASE_URL=$DATABASE_URL" >> $GITHUB_ENV
              ;;
            "memory")
              export DATABASE_URL=":memory:"
              echo "DATABASE_URL=$DATABASE_URL" >> $GITHUB_ENV
              ;;
          esac
          
          echo "✅ Database configured: ${{ matrix.test-config.database }}"

      - name: 🧪 Execute Tests (Optimized)
        id: test-execution
        run: |
          echo "🧪 Running ${{ matrix.test-config.name }}..."
          echo "Command: ${{ matrix.test-config.command }}"
          echo "Pattern: ${{ inputs.test-pattern || 'all' }}"
          echo "Workers: ${{ inputs.parallel-workers }}"
          
          # Set test-specific environment variables
          export TEST_PARALLEL_WORKERS="${{ inputs.parallel-workers }}"
          export TEST_TIMEOUT_MINUTES="${{ matrix.test-config.timeout }}"
          export TEST_RETRY_COUNT="${{ inputs.retry-count }}"
          
          # Configure test credentials
          export TEST_ADMIN_PASSWORD="${{ secrets.TEST_ADMIN_PASSWORD || 'test-admin-password' }}"
          export ADMIN_SECRET="${{ secrets.ADMIN_SECRET || 'test-admin-secret-key-minimum-32-characters' }}"
          
          # Additional environment for E2E tests
          if [ "${{ inputs.test-type }}" == "e2e" ]; then
            export PLAYWRIGHT_BASE_URL="http://localhost:3000"
            export E2E_TEST_MODE="true"
            export PERFORMANCE_TESTING="${{ inputs.test-type == 'performance' && 'true' || 'false' }}"
          fi
          
          # Execute tests with retry logic
          TEST_COMMAND="npm run ${{ matrix.test-config.command }}"
          
          # Add pattern filter if specified
          if [ -n "${{ inputs.test-pattern }}" ]; then
            case "${{ inputs.test-type }}" in
              "unit"|"integration")
                TEST_COMMAND="$TEST_COMMAND -- --reporter=verbose --testPathPattern='${{ inputs.test-pattern }}'"
                ;;
              "e2e")
                TEST_COMMAND="$TEST_COMMAND -- --grep='${{ inputs.test-pattern }}'"
                ;;
            esac
          fi
          
          echo "📋 Executing: $TEST_COMMAND"
          
          # Run tests with timeout and retry
          RETRY_COUNT=0
          MAX_RETRIES=${{ inputs.retry-count }}
          
          while [ $RETRY_COUNT -le $MAX_RETRIES ]; do
            if [ $RETRY_COUNT -gt 0 ]; then
              echo "🔄 Retry attempt $RETRY_COUNT/$MAX_RETRIES"
              sleep 5  # Brief pause between retries
            fi
            
            if timeout $((${{ matrix.test-config.timeout }} * 60)) bash -c "$TEST_COMMAND"; then
              echo "✅ Tests passed on attempt $((RETRY_COUNT + 1))"
              echo "test-success=true" >> $GITHUB_OUTPUT
              break
            else
              TEST_EXIT_CODE=$?
              echo "❌ Tests failed on attempt $((RETRY_COUNT + 1)) (exit code: $TEST_EXIT_CODE)"
              
              if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                echo "test-success=false" >> $GITHUB_OUTPUT
                echo "final-exit-code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT
                exit $TEST_EXIT_CODE
              fi
              
              RETRY_COUNT=$((RETRY_COUNT + 1))
            fi
          done
        env:
          # Test-specific optimizations
          VITEST_POOL_SIZE: ${{ inputs.parallel-workers }}
          PLAYWRIGHT_WORKERS: ${{ inputs.test-type == 'e2e' && inputs.parallel-workers || '1' }}

      - name: 📊 Analyze Test Results
        id: test-results
        if: always()
        run: |
          echo "📊 Analyzing test results..."
          
          # Initialize counters
          TESTS_PASSED=0
          TESTS_FAILED=0
          TEST_RESULT="unknown"
          
          # Parse results based on test type
          case "${{ inputs.test-type }}" in
            "unit"|"integration"|"smoke")
              if [ -f "test-results/results.json" ]; then
                TESTS_PASSED=$(jq -r '.numPassedTests // 0' test-results/results.json 2>/dev/null || echo "0")
                TESTS_FAILED=$(jq -r '.numFailedTests // 0' test-results/results.json 2>/dev/null || echo "0")
              fi
              ;;
            "e2e")
              if [ -f "test-results/e2e-results.json" ]; then
                TESTS_PASSED=$(jq -r '[.suites[].specs[].tests[]] | map(select(.outcome == "expected")) | length' test-results/e2e-results.json 2>/dev/null || echo "0")
                TESTS_FAILED=$(jq -r '[.suites[].specs[].tests[]] | map(select(.outcome == "unexpected")) | length' test-results/e2e-results.json 2>/dev/null || echo "0")
              fi
              ;;
          esac
          
          # Determine overall result
          if [ "${{ steps.test-execution.outputs.test-success }}" == "true" ]; then
            TEST_RESULT="success"
          else
            TEST_RESULT="failure"
          fi
          
          # Output results
          echo "tests-passed=$TESTS_PASSED" >> $GITHUB_OUTPUT
          echo "tests-failed=$TESTS_FAILED" >> $GITHUB_OUTPUT
          echo "test-result=$TEST_RESULT" >> $GITHUB_OUTPUT
          
          echo "📋 Test Results Summary:"
          echo "  Passed: $TESTS_PASSED"
          echo "  Failed: $TESTS_FAILED"
          echo "  Result: $TEST_RESULT"

      - name: 📈 Generate Coverage Report
        id: coverage
        if: inputs.coverage-enabled == true && (inputs.test-type == 'unit' || inputs.test-type == 'integration' || inputs.test-type == 'all')
        run: |
          echo "📈 Generating coverage report..."
          
          # Generate coverage if not already present
          if [ ! -d "coverage" ] && [ "${{ steps.test-execution.outputs.test-success }}" == "true" ]; then
            npm run test:coverage 2>/dev/null || echo "Coverage generation failed"
          fi
          
          # Parse coverage percentage
          COVERAGE_PCT=0
          if [ -f "coverage/coverage-summary.json" ]; then
            COVERAGE_PCT=$(jq -r '.total.lines.pct // 0' coverage/coverage-summary.json 2>/dev/null || echo "0")
            echo "📊 Coverage: ${COVERAGE_PCT}%"
          fi
          
          echo "coverage-percentage=$COVERAGE_PCT" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: ⚡ Performance Metrics Collection
        id: performance
        if: inputs.test-type == 'performance' || inputs.test-type == 'e2e'
        run: |
          echo "⚡ Collecting performance metrics..."
          
          # Initialize metrics object
          METRICS='{"execution_time": 0, "memory_usage": 0, "test_speed": "unknown"}'
          
          # Calculate execution time
          START_TIME=${{ steps.timer-start.outputs.start-time }}
          END_TIME=$(date +%s)
          EXECUTION_TIME=$((END_TIME - START_TIME))
          
          # Estimate memory usage (basic calculation)
          MEMORY_USAGE=${{ inputs.memory-limit }}
          
          # Calculate test speed
          TOTAL_TESTS=$((${{ steps.test-results.outputs.tests-passed }} + ${{ steps.test-results.outputs.tests-failed }}))
          if [ $TOTAL_TESTS -gt 0 ] && [ $EXECUTION_TIME -gt 0 ]; then
            TESTS_PER_SECOND=$(echo "scale=2; $TOTAL_TESTS / $EXECUTION_TIME" | bc -l 2>/dev/null || echo "0")
            TEST_SPEED="${TESTS_PER_SECOND} tests/sec"
          else
            TEST_SPEED="unknown"
          fi
          
          # Create metrics JSON
          METRICS=$(echo '{}' | jq \
            --arg exec_time "$EXECUTION_TIME" \
            --arg memory "$MEMORY_USAGE" \
            --arg speed "$TEST_SPEED" \
            --arg total_tests "$TOTAL_TESTS" \
            '{
              execution_time: ($exec_time | tonumber),
              memory_usage_mb: ($memory | tonumber),
              test_speed: $speed,
              total_tests: ($total_tests | tonumber),
              parallel_workers: ${{ inputs.parallel-workers }},
              test_type: "${{ inputs.test-type }}"
            }')
          
          echo "metrics=$METRICS" >> $GITHUB_OUTPUT
          echo "📊 Performance: ${EXECUTION_TIME}s, ${TOTAL_TESTS} tests, ${TEST_SPEED}"

      - name: ⏱️ Calculate Total Execution Time
        id: timing
        run: |
          START_TIME=${{ steps.timer-start.outputs.start-time }}
          END_TIME=$(date +%s)
          TOTAL_TIME=$((END_TIME - START_TIME))
          echo "execution-time=${TOTAL_TIME}s" >> $GITHUB_OUTPUT
          echo "⏱️ Total execution time: ${TOTAL_TIME}s"

      - name: 📤 Upload Test Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ inputs.test-type }}-${{ github.run_number }}
          path: |
            test-results/
            coverage/
            playwright-report/
            logs/
          retention-days: ${{ fromJson(inputs.artifacts-retention) }}
          if-no-files-found: ignore

      - name: 📋 Generate Test Summary
        if: always()
        run: |
          echo "# 🧪 Test Suite Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Test configuration
          echo "## Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "| Setting | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | ${{ inputs.test-type }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | ${{ inputs.test-environment }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Node.js | ${{ inputs.node-version }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Database | ${{ matrix.test-config.database }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Workers | ${{ inputs.parallel-workers }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory | ${{ inputs.memory-limit }}MB |" >> $GITHUB_STEP_SUMMARY
          echo "| Pattern | ${{ inputs.test-pattern || 'All tests' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Test results
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Status | ${{ steps.test-results.outputs.test-result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Tests Passed | ${{ steps.test-results.outputs.tests-passed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Tests Failed | ${{ steps.test-results.outputs.tests-failed }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Execution Time | ${{ steps.timing.outputs.execution-time }} |" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ inputs.coverage-enabled }}" == "true" ]; then
            echo "| Coverage | ${{ steps.coverage.outputs.coverage-percentage }}% |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Performance metrics (if available)
          if [ -n "${{ steps.performance.outputs.metrics }}" ]; then
            echo "## Performance Metrics" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            echo '${{ steps.performance.outputs.metrics }}' | jq '.' >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Status message
          if [ "${{ steps.test-results.outputs.test-result }}" == "success" ]; then
            echo "✅ **All tests passed!** Ready for next steps." >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Test failures detected.** Please review the logs and fix issues." >> $GITHUB_STEP_SUMMARY
          fi