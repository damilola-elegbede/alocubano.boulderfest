---
name: üöß PR Quality Gate

# Comprehensive PR gating workflow that coordinates all test suites and validates results
# - Runs unit tests, integration tests, and quality gates in parallel (Phase 1)
# - Waits for E2E tests to complete (triggered separately by Vercel deployment) (Phase 2)
# - Downloads all test artifacts and validates for false positives (Phase 3)
# - Blocks PR merge if validation fails (can be set as required status check)

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened, ready_for_review]

# Cancel in-progress runs when new commits are pushed
concurrency:
  group: pr-gate-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  actions: read
  checks: write
  deployments: read

env:
  NODE_VERSION: "20.19.5"

jobs:
  # ========================================
  # Phase 1: Parallel Test Execution
  # ========================================

  unit-tests:
    name: üß™ Unit Tests (Node ${{ matrix.node-version }})
    runs-on: ubuntu-latest
    timeout-minutes: 15
    strategy:
      fail-fast: false
      matrix:
        node-version: ['20.x', '22.x']

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üîß Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: üì¶ Install Dependencies
        run: npm ci --prefer-offline --no-audit

      - name: üß™ Run Unit Tests
        id: test-${{ matrix.node-version }}
        run: npm test
        env:
          TEST_ENV: ci

      - name: üì§ Upload Test Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-node-${{ matrix.node-version }}
          path: |
            test-metadata.json
            test-results/**/*
          retention-days: 7
          if-no-files-found: warn

  integration-tests:
    name: üîó Integration Tests (Node ${{ matrix.node-version }})
    runs-on: ubuntu-latest
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        node-version: ['20.x', '22.x']

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üîß Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: üì¶ Install Dependencies
        run: npm ci --prefer-offline --no-audit

      - name: üîó Run Integration Tests
        id: test-${{ matrix.node-version }}
        run: npm run test:integration
        env:
          TEST_ENV: ci

      - name: üì§ Upload Test Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results-node-${{ matrix.node-version }}
          path: |
            test-metadata.json
            test-results/**/*
          retention-days: 7
          if-no-files-found: warn

  quality-gates:
    name: üõ°Ô∏è Quality Gates
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4

      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: üì¶ Install Dependencies
        run: npm ci --prefer-offline --no-audit

      - name: üîç Code Quality Analysis
        run: npm run lint

      - name: üõ°Ô∏è Security Scan
        run: npm audit --audit-level=moderate || true

      - name: üì§ Upload Quality Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-gates-metadata
          path: |
            test-metadata.json
            quality-report.json
          retention-days: 7
          if-no-files-found: warn

  # ========================================
  # Phase 2: Wait for E2E Tests
  # ========================================
  # E2E tests run in a separate workflow triggered by Vercel deployment_status
  # This job waits for the E2E workflow to complete before proceeding to validation

  wait-for-e2e:
    name: ‚è≥ Wait for E2E Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, quality-gates]
    if: success() || failure() # Run even if some tests failed (validation needs to check all results)
    timeout-minutes: 30

    steps:
      - name: üí¨ E2E Wait Info
        run: |
          echo "‚è≥ Waiting for E2E tests to complete..."
          echo "E2E tests are triggered by Vercel deployment and run separately"
          echo "Checking for: üé≠ E2E Tests - Preview Deployments"

      - name: ‚è≥ Wait for E2E Test Completion
        uses: lewagon/wait-on-check-action@v1.3.1
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          check-name: 'üé≠ E2E Tests (chromium)' # Primary E2E job name from e2e-tests-preview.yml
          repo-token: ${{ secrets.GITHUB_TOKEN }}
          wait-interval: 30 # Check every 30 seconds
          allowed-conclusions: success,failure,cancelled,skipped,timed_out

      - name: üìä E2E Status
        run: |
          echo "‚úÖ E2E tests completed (or timed out)"
          echo "Proceeding to validation phase..."

  # ========================================
  # Phase 3: Download Artifacts & Validation
  # ========================================

  validate-results:
    name: üîç Validate All Test Results
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, quality-gates, wait-for-e2e]
    if: always() # Always run to validate whatever tests completed
    timeout-minutes: 10

    steps:
      - name: üì• Checkout Code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - name: üîß Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: üì¶ Install Dependencies
        run: npm ci --prefer-offline --no-audit

      - name: üìÅ Create Validation Directory
        run: mkdir -p .tmp/test-validation

      - name: üì• Download Unit Test Artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          pattern: 'unit-test-results-*'
          path: .tmp/test-validation
          merge-multiple: true

      - name: üì• Download Integration Test Artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          pattern: 'integration-test-results-*'
          path: .tmp/test-validation
          merge-multiple: true

      - name: üì• Download Quality Gates Artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: 'quality-gates-metadata'
          path: .tmp/test-validation

      - name: üì• Download E2E Test Artifacts (Cross-Workflow)
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üîç Searching for E2E workflow run for commit ${{ github.event.pull_request.head.sha }}"

          # Find the most recent E2E workflow run for this commit
          WORKFLOW_RUN_ID=$(gh api "/repos/${{ github.repository }}/actions/runs" \
            --jq ".workflow_runs[] | select(.head_sha==\"${{ github.event.pull_request.head.sha }}\" and .name==\"üé≠ E2E Tests - Preview Deployments\") | .id" \
            | head -1)

          if [ -n "$WORKFLOW_RUN_ID" ]; then
            echo "‚úÖ Found E2E workflow run: $WORKFLOW_RUN_ID"
            echo "üì• Downloading E2E artifacts..."
            gh run download "$WORKFLOW_RUN_ID" --pattern 'e2e-results-*' --dir .tmp/test-validation || echo "‚ö†Ô∏è No E2E artifacts found"
          else
            echo "‚ö†Ô∏è No E2E workflow run found for this commit"
            echo "This is expected if Vercel deployment hasn't completed yet"
          fi

      - name: üìä List Downloaded Artifacts
        run: |
          echo "=== Artifacts Directory Structure ==="
          ls -la .tmp/test-validation/ || echo "No artifacts directory"
          echo ""
          echo "=== Test Metadata Files ==="
          find .tmp/test-validation -name "test-metadata.json" -exec echo "Found: {}" \; || echo "No metadata files"

      - name: üîç Run Validation
        id: validate
        run: |
          echo "üîç Running test result validation..."

          # Run validation script with explicit error handling
          if ! node scripts/validate-test-results.js > .tmp/validation-report.json 2>&1; then
            echo "‚ùå Validation script failed to execute"
            echo "exit_code=1" >> $GITHUB_OUTPUT
            echo "status=ERROR" >> $GITHUB_OUTPUT
            echo "p1_issues=0" >> $GITHUB_OUTPUT
            echo "p2_issues=0" >> $GITHUB_OUTPUT
            echo "block_merge=true" >> $GITHUB_OUTPUT
            exit 0  # Don't fail the step, let outputs control behavior
          fi

          # Verify report was generated and is valid
          if [ ! -f .tmp/validation-report.json ]; then
            echo "‚ùå Validation report not generated"
            echo "exit_code=1" >> $GITHUB_OUTPUT
            echo "status=ERROR" >> $GITHUB_OUTPUT
            echo "p1_issues=0" >> $GITHUB_OUTPUT
            echo "p2_issues=0" >> $GITHUB_OUTPUT
            echo "block_merge=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "üìä Validation report generated"
          cat .tmp/validation-report.json

          # Extract validation results
          EXIT_CODE=$(jq -r '.exit_code // 1' .tmp/validation-report.json)
          STATUS=$(jq -r '.validation_status // "UNKNOWN"' .tmp/validation-report.json)
          P1_COUNT=$(jq -r '.summary.priority1_issues // 0' .tmp/validation-report.json)
          P2_COUNT=$(jq -r '.summary.priority2_issues // 0' .tmp/validation-report.json)

          echo "exit_code=${EXIT_CODE}" >> $GITHUB_OUTPUT
          echo "status=${STATUS}" >> $GITHUB_OUTPUT
          echo "p1_issues=${P1_COUNT}" >> $GITHUB_OUTPUT
          echo "p2_issues=${P2_COUNT}" >> $GITHUB_OUTPUT

          # Determine if we should block merge
          if [ "${P1_COUNT}" -gt 0 ]; then
            echo "‚ùå Priority 1 issues detected - will block merge"
            echo "block_merge=true" >> $GITHUB_OUTPUT
          elif [ "${P2_COUNT}" -gt 0 ]; then
            echo "‚ö†Ô∏è Priority 2 issues detected - will block merge"
            echo "block_merge=true" >> $GITHUB_OUTPUT
          else
            echo "‚úÖ No blocking issues detected"
            echo "block_merge=false" >> $GITHUB_OUTPUT
          fi

      - name: üí¨ Post Validation Comment to PR
        uses: actions/github-script@v7
        if: always()
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            if (!fs.existsSync('.tmp/validation-report.json')) {
              console.log('‚ö†Ô∏è No validation report found');
              return;
            }

            const report = JSON.parse(fs.readFileSync('.tmp/validation-report.json', 'utf8'));

            // Build validation comment
            let comment = '## üîç Test Result Validation Report\n\n';

            const status = report.validation_status || 'UNKNOWN';
            const badge = status === 'FAIL' ? 'üî¥ FAIL' :
                         status === 'WARN' ? 'üü° WARN' :
                         status === 'PASS' ? 'üü¢ PASS' : '‚ö™ UNKNOWN';

            comment += `**Status:** ${badge}\n\n`;

            if (report.summary) {
              comment += '### Summary\n\n';
              comment += `- **Total Issues:** ${report.summary.total_issues || 0}\n`;
              comment += `- **Priority 1 (Critical):** ${report.summary.priority1_issues || 0}\n`;
              comment += `- **Priority 2 (High):** ${report.summary.priority2_issues || 0}\n`;
              comment += `- **Priority 3 (Low):** ${report.summary.priority3_issues || 0}\n\n`;
            }

            if (report.issues && report.issues.length > 0) {
              comment += '### Issues Detected\n\n';
              report.issues.forEach((issue, index) => {
                const priorityBadge = issue.priority === 1 ? 'üî¥ P1' :
                                      issue.priority === 2 ? 'üü° P2' : 'üîµ P3';
                comment += `${index + 1}. ${priorityBadge} **${issue.pattern}**\n`;
                comment += `   - ${issue.message}\n`;
                if (issue.file) comment += `   - File: \`${issue.file}\`\n`;
                comment += '\n';
              });
            }

            comment += '---\n';
            comment += `*Validation completed at ${new Date().toISOString()}*\n`;
            comment += `*Commit: ${context.sha.substring(0, 7)}*`;

            // Find existing validation comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const existingComment = comments.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('üîç Test Result Validation Report')
            );

            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
              console.log('‚úÖ Updated existing validation comment');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
              console.log('‚úÖ Created new validation comment');
            }

      - name: üì§ Upload Validation Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-report
          path: .tmp/validation-report.json
          retention-days: 30

      - name: ‚ùå Block Merge on Critical Issues
        if: steps.validate.outputs.block_merge == 'true'
        run: |
          echo "‚ùå Test validation detected blocking issues"
          echo "Status: ${{ steps.validate.outputs.status }}"
          echo "P1 Issues: ${{ steps.validate.outputs.p1_issues }}"
          echo "P2 Issues: ${{ steps.validate.outputs.p2_issues }}"
          echo ""
          echo "This PR cannot be merged until these issues are resolved."
          exit 1

      - name: ‚úÖ Validation Passed
        if: steps.validate.outputs.block_merge == 'false'
        run: |
          echo "‚úÖ All test results validated successfully"
          echo "Status: ${{ steps.validate.outputs.status }}"
          echo "No blocking issues detected"
          echo "PR is ready for merge"

  # ========================================
  # Final Gate: Summary Status
  # ========================================
  # This is the job that should be set as a required status check

  pr-gate-status:
    name: ‚úÖ PR Quality Gate Status
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, quality-gates, validate-results]
    if: always()

    steps:
      - name: üìä Aggregate Status
        run: |
          echo "=== PR Quality Gate Status Check ==="
          echo ""
          UNIT_STATUS="${{ needs.unit-tests.result }}"
          INTEGRATION_STATUS="${{ needs.integration-tests.result }}"
          QUALITY_STATUS="${{ needs.quality-gates.result }}"
          VALIDATION_STATUS="${{ needs.validate-results.result }}"

          echo "üìä Test Results:"
          echo "  Unit Tests: $UNIT_STATUS"
          echo "  Integration Tests: $INTEGRATION_STATUS"
          echo "  Quality Gates: $QUALITY_STATUS"
          echo "  Validation: $VALIDATION_STATUS"
          echo ""

          # Validation is the most important check
          if [ "$VALIDATION_STATUS" != "success" ]; then
            echo "‚ùå GATE FAILED: Validation detected issues or failed"
            echo "Review the validation report above for details"
            exit 1
          fi

          # Check if core tests passed
          if [ "$UNIT_STATUS" != "success" ] || [ "$INTEGRATION_STATUS" != "success" ]; then
            echo "‚ùå GATE FAILED: Test failures detected"
            echo "Review the failed test logs above"
            exit 1
          fi

          # Quality gates are informational but still important
          if [ "$QUALITY_STATUS" != "success" ]; then
            echo "‚ö†Ô∏è WARNING: Quality gates had issues"
            echo "Consider addressing quality issues before merge"
            # Don't fail - quality issues shouldn't block if tests pass
          fi

          echo ""
          echo "‚úÖ GATE PASSED: All checks successful"
          echo "PR is approved for merge"
