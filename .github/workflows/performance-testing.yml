name: Performance Testing

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened]
  workflow_dispatch:
    inputs:
      deployment_url:
        description: 'Deployment URL to test (optional)'
        required: false
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'critical'
        type: choice
        options:
          - critical
          - full
          - ticket-sales
          - check-in

env:
  NODE_VERSION: '20'
  K6_VERSION: '0.47.0'

jobs:
  performance-test:
    name: Run Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    # Only run on Vercel preview deployments or manual trigger
    if: github.event_name == 'workflow_dispatch' || (github.event_name == 'pull_request' && contains(github.event.pull_request.head.ref, 'performance'))
    
    outputs:
      performance_passed: ${{ steps.performance.outputs.performance_passed }}
      performance_issues: ${{ steps.performance.outputs.performance_issues }}
      deployment_url: ${{ steps.performance.outputs.deployment_url }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: |
          npm ci
          # Install K6 for load testing
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Verify K6 installation
        run: k6 version

      - name: Wait for Vercel deployment
        id: vercel-deployment
        uses: actions/github-script@v7
        with:
          script: |
            // Wait for Vercel deployment to be ready
            const { context } = github;
            
            // Check if manual deployment URL provided
            if (context.payload.inputs && context.payload.inputs.deployment_url) {
              console.log('Using manual deployment URL:', context.payload.inputs.deployment_url);
              core.setOutput('deployment_url', context.payload.inputs.deployment_url);
              return;
            }
            
            // For PR events, wait for Vercel preview deployment
            if (context.eventName === 'pull_request') {
              console.log('Waiting for Vercel preview deployment...');
              
              // In a real scenario, you'd implement logic to:
              // 1. Check Vercel API for deployment status
              // 2. Or parse deployment comments from Vercel bot
              // 3. Or use Vercel GitHub integration
              
              // For now, construct preview URL based on branch
              const branchName = context.payload.pull_request.head.ref;
              const sanitizedBranch = branchName.replace(/[^a-zA-Z0-9]/g, '-').toLowerCase();
              const previewUrl = `https://alocubanoboulderfest-git-${sanitizedBranch}-alocubano.vercel.app`;
              
              console.log('Constructed preview URL:', previewUrl);
              core.setOutput('deployment_url', previewUrl);
            } else {
              // Fallback to production URL
              const productionUrl = 'https://alocubanoboulderfest.vercel.app';
              console.log('Using production URL:', productionUrl);
              core.setOutput('deployment_url', productionUrl);
            }

      - name: Wait for deployment readiness
        env:
          DEPLOYMENT_URL: ${{ steps.vercel-deployment.outputs.deployment_url }}
        run: |
          echo "⏳ Waiting for deployment to be ready: $DEPLOYMENT_URL"
          
          # Wait up to 5 minutes for deployment to respond
          for i in {1..30}; do
            if curl -sf --max-time 10 "$DEPLOYMENT_URL/api/health/check" > /dev/null 2>&1; then
              echo "✅ Deployment is ready!"
              break
            fi
            
            if [ $i -eq 30 ]; then
              echo "❌ Deployment not ready after 5 minutes"
              exit 1
            fi
            
            echo "⏳ Attempt $i/30: Deployment not ready, waiting 10 seconds..."
            sleep 10
          done

      - name: Run performance tests
        id: performance
        env:
          LOAD_TEST_BASE_URL: ${{ steps.vercel-deployment.outputs.deployment_url }}
          TEST_SUITE: ${{ github.event.inputs.test_suite || 'critical' }}
          GITHUB_OUTPUT: ${{ env.GITHUB_OUTPUT }}
        run: |
          echo "🚀 Running performance tests against: $LOAD_TEST_BASE_URL"
          echo "📊 Test suite: $TEST_SUITE"
          
          # Determine tests to run based on input
          case "$TEST_SUITE" in
            "full")
              TESTS="ticket-sales,check-in,sustained"
              ;;
            "ticket-sales")
              TESTS="ticket-sales"
              ;;
            "check-in")
              TESTS="check-in"
              ;;
            *)
              TESTS="ticket-sales,check-in"  # critical
              ;;
          esac
          
          # Run performance CI integration
          node scripts/performance-ci-integration.js || EXIT_CODE=$?
          
          # Ensure outputs are set even if tests fail
          if [ ! -f "$GITHUB_OUTPUT" ] || ! grep -q "performance_passed" "$GITHUB_OUTPUT"; then
            echo "performance_passed=false" >> $GITHUB_OUTPUT
            echo "performance_issues=unknown" >> $GITHUB_OUTPUT
            echo "deployment_url=$LOAD_TEST_BASE_URL" >> $GITHUB_OUTPUT
          fi
          
          exit ${EXIT_CODE:-0}

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            reports/load-test-results/
            reports/performance-baselines/
          retention-days: 30

      - name: Comment performance results
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            // Read performance results
            const passed = '${{ steps.performance.outputs.performance_passed }}' === 'true';
            const issues = '${{ steps.performance.outputs.performance_issues }}' || '0';
            const deploymentUrl = '${{ steps.performance.outputs.deployment_url }}';
            
            // Find latest performance report
            const reportsDir = 'reports/load-test-results';
            let reportContent = 'Performance test results not available.';
            
            try {
              if (fs.existsSync(reportsDir)) {
                const files = fs.readdirSync(reportsDir)
                  .filter(f => f.endsWith('.json') && f.includes('performance-report'))
                  .sort()
                  .reverse();
                
                if (files.length > 0) {
                  const reportPath = path.join(reportsDir, files[0]);
                  const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
                  
                  reportContent = `## 📊 Performance Test Results
            
            **Status:** ${passed ? '✅ PASSED' : '❌ FAILED'}
            **Issues:** ${issues}
            **Deployment:** ${deploymentUrl}
            
            ### Summary
            - **Tests Executed:** ${report.summary?.totalTests || 'N/A'}
            - **Duration:** ${((report.summary?.totalDuration || 0) / 60000).toFixed(1)} minutes
            - **Regressions:** ${report.summary?.totalRegressions || 0}
            
            ${report.summary?.criticalIssues?.length > 0 ? `
            ### ⚠️ Critical Issues
            ${report.summary.criticalIssues.map(issue => 
              `- **${issue.test}:** ${issue.metric} degraded by ${issue.change}`
            ).join('\n')}
            ` : ''}
            
            ### 🎯 Test Results
            ${report.testResults?.map(result => `
            **${result.testConfig?.name}:**
            - Avg Response: ${Math.round(result.results?.summary?.response_times?.avg || 0)}ms
            - P95 Response: ${Math.round(result.results?.summary?.response_times?.p95 || 0)}ms
            - Error Rate: ${((result.results?.summary?.error_rates?.http_failed || 0) * 100).toFixed(2)}%
            `).join('') || 'No detailed results available'}
            
            ---
            *Performance tests run against Vercel serverless deployment*`;
                }
              }
            } catch (error) {
              console.error('Error reading performance report:', error);
            }
            
            // Post or update comment
            const { owner, repo, number } = context.issue;
            
            // Find existing performance comment
            const comments = await github.rest.issues.listComments({
              owner,
              repo,
              issue_number: number,
            });
            
            const existingComment = comments.data.find(comment => 
              comment.user.login === 'github-actions[bot]' && 
              comment.body.includes('Performance Test Results')
            );
            
            if (existingComment) {
              await github.rest.issues.updateComment({
                owner,
                repo,
                comment_id: existingComment.id,
                body: reportContent
              });
            } else {
              await github.rest.issues.createComment({
                owner,
                repo,
                issue_number: number,
                body: reportContent
              });
            }

  performance-gate:
    name: Performance Gate
    runs-on: ubuntu-latest
    needs: performance-test
    if: always() && needs.performance-test.result != 'cancelled'
    
    steps:
      - name: Evaluate performance gate
        run: |
          PERFORMANCE_PASSED="${{ needs.performance-test.outputs.performance_passed }}"
          PERFORMANCE_ISSUES="${{ needs.performance-test.outputs.performance_issues }}"
          
          echo "🚦 Performance Gate Evaluation"
          echo "=============================="
          echo "Passed: $PERFORMANCE_PASSED"
          echo "Issues: $PERFORMANCE_ISSUES"
          
          if [ "$PERFORMANCE_PASSED" = "true" ]; then
            echo "✅ Performance gate PASSED - Deployment approved"
            exit 0
          else
            echo "❌ Performance gate FAILED - Review performance issues before merging"
            echo "Issues found: $PERFORMANCE_ISSUES"
            exit 1
          fi

      - name: Performance gate status
        run: |
          if [ "${{ needs.performance-test.outputs.performance_passed }}" = "true" ]; then
            echo "performance_gate=passed" >> $GITHUB_ENV
          else
            echo "performance_gate=failed" >> $GITHUB_ENV
          fi