# ======================================================================
# A Lo Cubano Boulder Fest - CI/CD Pipeline with Enhanced Test Comments
# ======================================================================
# Production-Ready CI with Quality Gates and PR Comments
# - Comprehensive testing: unit, E2E, performance, security
# - Enhanced test result collection and artifact upload
# - Vercel deployment with preview environments
# - Detailed PR comments via separate workflow
# ======================================================================

name: Main CI Pipeline

on:
  push:
    branches: [main, develop, "feature/**", "release/**", "hotfix/**"]
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened, ready_for_review]
  workflow_dispatch:
    inputs:
      skip_cache:
        description: "Skip build cache"
        required: false
        default: "false"
      debug_mode:
        description: "Enable debug mode"
        required: false
        default: "false"

# Cancel in-progress runs for the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Global environment variables
env:
  NODE_VERSION: "20"
  VITEST_TEST_TIMEOUT: "60000"
  VITEST_HOOK_TIMEOUT: "30000"
  PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: "0"
  TURSO_CONNECTION_TIMEOUT: "30000"
  CI: true

jobs:
  # ======================================================================
  # Stage 1: Code Quality & Static Analysis
  # ======================================================================
  code-quality:
    name: 🎨 Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: 📦 Install dependencies
        run: |
          npm ci --prefer-offline --no-audit --no-fund
          echo "✅ Dependencies installed"

      - name: 🎨 Run ESLint
        id: eslint
        run: |
          echo "🔍 Running ESLint..."
          npm run lint:js 2>&1 | tee eslint-output.log
          ESLINT_EXIT_CODE=${PIPESTATUS[0]}

          if [ $ESLINT_EXIT_CODE -ne 0 ]; then
            echo "❌ ESLint found issues"
            echo "eslint_failed=true" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "✅ ESLint passed"
            echo "eslint_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: 📝 Run Markdown Quality Checks
        id: markdown
        if: always()
        run: |
          echo "📝 Checking markdown quality..."
          if ./scripts/validate-markdown-quality.sh check; then
            echo "✅ Markdown quality passed"
            echo "markdown_failed=false" >> $GITHUB_OUTPUT
          else
            echo "❌ Markdown quality issues found"
            echo "markdown_failed=true" >> $GITHUB_OUTPUT
            echo "💡 Run './scripts/validate-markdown-quality.sh fix' locally to auto-fix"
            exit 1
          fi

      - name: 🏗️ Check HTML validity
        id: html
        if: always()
        run: |
          echo "🏗️ Validating HTML files..."
          npm run lint:html 2>&1 | tee html-output.log
          HTML_EXIT_CODE=${PIPESTATUS[0]}

          if [ $HTML_EXIT_CODE -ne 0 ]; then
            echo "❌ HTML validation failed"
            echo "html_failed=true" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "✅ HTML validation passed"
            echo "html_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: 📤 Upload Quality Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-results-${{ github.run_number }}
          path: |
            eslint-output.log
            html-output.log
          retention-days: 7

      - name: 📊 Quality Summary
        if: always()
        run: |
          echo "========================================"
          echo "📊 Code Quality Summary"
          echo "========================================"
          echo "ESLint:    ${{ steps.eslint.outputs.eslint_failed == 'true' && '❌ Failed' || '✅ Passed' }}"
          echo "Markdown:  ${{ steps.markdown.outputs.markdown_failed == 'true' && '❌ Failed' || '✅ Passed' }}"
          echo "HTML:      ${{ steps.html.outputs.html_failed == 'true' && '❌ Failed' || '✅ Passed' }}"
          echo "========================================"

  # ======================================================================
  # Stage 2: Security Scanning
  # ======================================================================
  security-scan:
    name: 🔒 Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: 🔍 Run npm audit
        id: npm-audit
        continue-on-error: true
        run: |
          echo "🔍 Running npm audit..."
          npm audit --audit-level=high --production 2>&1 | tee audit-output.log
          AUDIT_EXIT_CODE=${PIPESTATUS[0]}

          if [ $AUDIT_EXIT_CODE -ne 0 ]; then
            echo "⚠️ npm audit found vulnerabilities"
            echo "audit_failed=true" >> $GITHUB_OUTPUT

            # Check for critical vulnerabilities
            if grep -q "critical" audit-output.log; then
              echo "🚨 Critical vulnerabilities found!"
              exit 1
            fi
          else
            echo "✅ No vulnerabilities found"
            echo "audit_failed=false" >> $GITHUB_OUTPUT
          fi

      - name: 🔐 Check for secrets
        run: |
          echo "🔐 Checking for exposed secrets..."

          # Check for common secret patterns
          if grep -r --include="*.js" --include="*.json" --exclude-dir=node_modules \
             -E "(api[_-]?key|secret|password|token|private[_-]?key)['\"]?\s*[:=]\s*['\"][^'\"]{10,}" . 2>/dev/null; then
            echo "🚨 Potential secrets found in code!"
            exit 1
          fi

          echo "✅ No exposed secrets detected"

      - name: 📤 Upload Security Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-scan-results-${{ github.run_number }}
          path: |
            audit-output.log
          retention-days: 7

      - name: 📊 Security Summary
        if: always()
        run: |
          echo "========================================"
          echo "🔒 Security Scan Summary"
          echo "========================================"
          echo "npm audit: ${{ steps.npm-audit.outputs.audit_failed == 'true' && '⚠️ Vulnerabilities found' || '✅ Clean' }}"
          echo "Secrets:   ✅ No exposed secrets"
          echo "========================================"

  # ======================================================================
  # Stage 3: Unit Tests with Enhanced Result Collection
  # ======================================================================
  unit-tests:
    name: 🧪 Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [code-quality]

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: 📦 Install dependencies
        run: |
          npm ci --prefer-offline --no-audit --no-fund
          echo "✅ Dependencies installed"

      - name: 🗄️ Setup test database
        run: |
          echo "🗄️ Initializing SQLite database for testing..."

          # Create database directory
          mkdir -p data
          touch data/test.db

          # Initialize database schema
          npm run migrate:status || true

          echo "✅ Test database ready"

      - name: 🧪 Run unit tests with JSON output
        env:
          NODE_ENV: test
          DATABASE_URL: "file:./data/test.db"
          ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD || '$2b$10$K7L1OJ0TfmHrVj2lEwjBOe7MJQkPH5c6WqzT5LJlvGA8MXl7T5jKq' }}
          ADMIN_SECRET: ${{ secrets.ADMIN_SECRET || 'test-secret-minimum-32-characters-long' }}
          TEST_ADMIN_PASSWORD: test-password-123
          STRIPE_SECRET_KEY: ${{ secrets.STRIPE_SECRET_KEY || 'sk_test_dummy' }}
          BREVO_API_KEY: ${{ secrets.BREVO_API_KEY || 'test-api-key' }}
          WALLET_AUTH_SECRET: ${{ secrets.WALLET_AUTH_SECRET || 'test-wallet-secret-32-chars-minimum' }}
        run: |
          echo "🧪 Running unit tests with enhanced reporting..."
          
          # Create test results directory
          mkdir -p test-results
          
          # Run tests with JSON reporter
          npx vitest run --config tests/vitest.config.js --reporter=json --outputFile=test-results/unit-results.json 2>&1 | tee test-output.log
          TEST_EXIT_CODE=${PIPESTATUS[0]}

          # Also create a simple results file for parsing
          if [ ! -f "test-results/unit-results.json" ] && [ -f "test-output.log" ]; then
            PASSED=$(grep -o "[0-9]\+ passed" test-output.log | head -1 | grep -o "[0-9]\+" || echo "0")
            FAILED=$(grep -o "[0-9]\+ failed" test-output.log | head -1 | grep -o "[0-9]\+" || echo "0")
            TOTAL=$((PASSED + FAILED))
            
            cat > test-results/unit-results.json <<EOF
          {
            "numTotalTests": $TOTAL,
            "numPassedTests": $PASSED,
            "numFailedTests": $FAILED,
            "numPendingTests": 0
          }
          EOF
          fi

          # Extract test summary
          if grep -q "Test Files" test-output.log; then
            echo "📊 Test Results:"
            grep -A 5 "Test Files" test-output.log || true
          fi

          if [ $TEST_EXIT_CODE -ne 0 ]; then
            echo "❌ Unit tests failed"
            exit 1
          fi

          echo "✅ All unit tests passed"

      - name: 📊 Generate coverage report
        if: always()
        continue-on-error: true
        run: |
          echo "📊 Generating coverage report..."
          npm run test:coverage || true

      - name: 📤 Upload Enhanced Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-${{ github.run_number }}
          path: |
            test-results/
            test-output.log
            coverage/
          retention-days: 7

  # ======================================================================
  # Stage 4: Build Verification
  # ======================================================================
  build-check:
    name: 🏗️ Build Verification
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [unit-tests]

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: 📦 Install dependencies
        run: |
          npm ci --prefer-offline --no-audit --no-fund
          echo "✅ Dependencies installed"

      - name: 🔍 Verify API structure
        run: |
          echo "🔍 Checking API handler exports..."

          # Check that all API files export default functions
          for file in api/**/*.js; do
            if [ -f "$file" ] && [[ ! "$file" =~ "/lib/" ]]; then
              if ! grep -q "export default" "$file"; then
                echo "❌ Missing default export in $file"
                exit 1
              fi
            fi
          done

          echo "✅ API structure verified"

      - name: 📝 Check migration files
        run: |
          echo "📝 Validating database migrations..."

          # Check migration file format
          for file in migrations/*.sql; do
            if [ -f "$file" ]; then
              # Check for transaction blocks
              if ! grep -q "BEGIN TRANSACTION\|BEGIN;" "$file"; then
                echo "⚠️ Migration $file missing transaction block"
              fi

              # Check for dangerous operations
              if grep -qi "drop table\|truncate\|delete from" "$file"; then
                echo "⚠️ Migration $file contains potentially dangerous operations"
              fi
            fi
          done

          echo "✅ Migration files validated"

      - name: 🏗️ Test build process
        run: |
          echo "🏗️ Verifying build process..."

          # Check package.json scripts
          npm run --silent | grep -E "(build|deploy|test)" || true

          # Verify critical files exist
          CRITICAL_FILES=(
            "package.json"
            "vercel.json"
            "api/lib/database.js"
            "pages/index.html"
          )

          for file in "${CRITICAL_FILES[@]}"; do
            if [ ! -f "$file" ]; then
              echo "❌ Critical file missing: $file"
              exit 1
            fi
          done

          echo "✅ Build verification complete"

  # ======================================================================
  # Stage 5: E2E Tests with Enhanced Artifact Collection
  # ======================================================================
  e2e-tests:
    name: 🎭 E2E - ${{ matrix.suite-name }}
    runs-on: ubuntu-latest
    timeout-minutes: ${{ matrix.timeout-minutes }}
    needs: [build-check]
    if: |
      github.event_name == 'push' ||
      github.event_name == 'pull_request' ||
      github.event_name == 'workflow_dispatch'

    strategy:
      fail-fast: false
      matrix:
        include:
          # Core functionality tests (fastest, most critical)
          - suite: core
            suite-name: "Core Features"
            browser: chromium
            test-pattern: "basic-navigation|registration-flow|cart-functionality"
            retry-count: 2
            timeout-minutes: 15
            memory-limit: 2048

          # Admin functionality tests
          - suite: admin
            suite-name: "Admin Panel"
            browser: chromium
            test-pattern: "admin-auth|admin-dashboard"
            retry-count: 2
            timeout-minutes: 15
            memory-limit: 2048

          # User experience tests
          - suite: user-experience
            suite-name: "User Experience"
            browser: chromium
            test-pattern: "newsletter-simple|gallery-basic|user-engagement"
            retry-count: 2
            timeout-minutes: 20
            memory-limit: 3072

          # Advanced scenarios (payment, tickets)
          - suite: advanced
            suite-name: "Advanced Features"
            browser: chromium
            test-pattern: "payment-flow|ticket-validation"
            retry-count: 3
            timeout-minutes: 20
            memory-limit: 3072

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: 📦 Install dependencies
        run: |
          echo "📦 Installing dependencies for ${{ matrix.suite-name }}..."
          npm ci --prefer-offline --no-audit --no-fund

          # Install Playwright browsers
          echo "🎭 Installing Playwright browsers..."
          npx playwright install --with-deps ${{ matrix.browser }}

          # Install Vercel CLI
          echo "▲ Installing Vercel CLI..."
          npm install -g vercel@latest

          echo "✅ Dependencies ready"

      - name: 🔐 Setup E2E Environment
        id: setup-env
        run: |
          echo "🔐 Configuring E2E environment for ${{ matrix.suite-name }}..."

          # Allocate dynamic port
          DYNAMIC_PORT=$((3000 + RANDOM % 1000))
          echo "DYNAMIC_PORT=${DYNAMIC_PORT}" >> $GITHUB_ENV
          echo "dynamic_port=${DYNAMIC_PORT}" >> $GITHUB_OUTPUT

          # Set base URL for Playwright
          echo "PLAYWRIGHT_BASE_URL=http://localhost:${DYNAMIC_PORT}" >> $GITHUB_ENV

          # Export test mode flag
          echo "E2E_TEST_MODE=true" >> $GITHUB_ENV

          # Create test results directory
          mkdir -p test-results

          echo "✅ Environment configured:"
          echo "  Port: ${DYNAMIC_PORT}"
          echo "  Suite: ${{ matrix.suite }}"
          echo "  Browser: ${{ matrix.browser }}"

      - name: 🗄️ Database Setup Check
        id: db-check
        env:
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL_CI || secrets.TURSO_DATABASE_URL || '' }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN_CI || secrets.TURSO_AUTH_TOKEN || '' }}
        run: |
          echo "🗄️ Checking database configuration..."

          if [ -n "$TURSO_DATABASE_URL" ] && [ -n "$TURSO_AUTH_TOKEN" ]; then
            echo "✅ Turso database configured"
            echo "db_type=turso" >> $GITHUB_OUTPUT

            # Test connection
            echo "Testing Turso connection..."
            npm run health:database || echo "⚠️ Database health check failed"
          else
            echo "⚠️ Turso not configured, using SQLite fallback"
            echo "db_type=sqlite" >> $GITHUB_OUTPUT

            # Setup SQLite
            mkdir -p data
            touch data/e2e-test.db
            echo "DATABASE_URL=file:./data/e2e-test.db" >> $GITHUB_ENV

            # Initialize schema
            npm run migrate:up || echo "⚠️ Migration failed"
            echo "⚠️ SQLite fallback ready for ${{ matrix.suite }}"
          fi

      - name: 🎭 Run E2E Tests with Enhanced Result Collection
        env:
          PLAYWRIGHT_BASE_URL: ${{ env.PLAYWRIGHT_BASE_URL }}
          NODE_OPTIONS: "--max-old-space-size=${{ matrix.memory-limit }}"
          E2E_TEST_MODE: true
          TURSO_DATABASE_URL: ${{ secrets.TURSO_DATABASE_URL_CI || secrets.TURSO_DATABASE_URL || '' }}
          TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN_CI || secrets.TURSO_AUTH_TOKEN || '' }}
          PLAYWRIGHT_BROWSER: ${{ matrix.browser }}
          PORT: ${{ env.DYNAMIC_PORT }}
          DYNAMIC_PORT: ${{ env.DYNAMIC_PORT }}
          TEST_ADMIN_PASSWORD: test-password-123
          ADMIN_PASSWORD: '$2b$10$K7L1OJ0TfmHrVj2lEwjBOe7MJQkPH5c6WqzT5LJlvGA8MXl7T5jKq'
          ADMIN_SECRET: ${{ secrets.ADMIN_SECRET || 'fallback-test-secret-minimum-32-chars' }}
          BREVO_API_KEY: ${{ secrets.BREVO_API_KEY || '' }}
          STRIPE_SECRET_KEY: ${{ secrets.STRIPE_SECRET_KEY || '' }}
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN || '' }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID || '' }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID || '' }}
        run: |
          echo "🎭 Running E2E tests with enhanced result collection: ${{ matrix.suite-name }}..."
          echo "📊 Test suite: ${{ matrix.suite }}"
          echo "🌐 Browser: ${{ matrix.browser }}"
          echo "🎯 Pattern: ${{ matrix.test-pattern || 'All tests' }}"
          echo "📡 Port: ${{ env.DYNAMIC_PORT }}"

          # Construct test command with enhanced reporting
          TEST_CMD="npx playwright test --config=playwright-e2e-vercel-main.config.js --project=${{ matrix.browser }}"

          # Add test pattern filtering
          if [ -n "${{ matrix.test-pattern }}" ]; then
            TEST_CMD="$TEST_CMD --grep=\"(${{ matrix.test-pattern }})\""
          fi

          # Add retries and timeout
          TEST_CMD="$TEST_CMD --retries=${{ matrix.retry-count }} --timeout=120000"

          # Enhanced reporting with JSON output for parsing
          TEST_CMD="$TEST_CMD --reporter=list,json:test-results/e2e-${{ matrix.browser }}-results.json"

          echo "📋 Executing: $TEST_CMD"

          # Execute with timeout protection
          if timeout ${{ matrix.timeout-minutes }}m bash -c "$TEST_CMD"; then
            echo "✅ E2E tests completed successfully: ${{ matrix.suite-name }}"
          else
            EXIT_CODE=$?
            echo "❌ E2E tests failed: ${{ matrix.suite-name }} (exit code: $EXIT_CODE)"
            exit $EXIT_CODE
          fi

      - name: 📤 Upload Enhanced E2E Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.suite }}-${{ matrix.browser }}-${{ github.run_number }}
          path: |
            playwright-report/
            test-results/
            tests/e2e/screenshots/
          retention-days: 7

      - name: 📊 Test Summary
        if: always()
        run: |
          echo "========================================"
          echo "📊 Enhanced E2E Test Summary - ${{ matrix.suite-name }}"
          echo "========================================"
          echo "Suite: ${{ matrix.suite }}"
          echo "Browser: ${{ matrix.browser }}"
          echo "Pattern: ${{ matrix.test-pattern || 'All tests' }}"
          echo "Database: ${{ steps.db-check.outputs.db_type }}"
          echo "Port: ${{ env.DYNAMIC_PORT }}"
          echo "Results: Enhanced artifact collection enabled"
          echo "========================================"

  # ======================================================================
  # Stage 6: Performance Testing with Result Collection
  # ======================================================================
  performance-tests:
    name: ⚡ Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [build-check]
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: 📦 Install dependencies
        run: npm ci

      - name: ⚡ Run Performance Tests with Result Collection
        run: |
          echo "⚡ Running performance validation with enhanced reporting..."
          
          # Create performance results directory
          mkdir -p test-results
          
          TARGET_URL="https://alocubanoboulderfest.vercel.app"
          echo "Target URL: $TARGET_URL"
          
          # Enhanced performance testing with metrics
          echo "Testing API response times..."
          START_TIME=$(date +%s%N)
          if curl -f --max-time 10 "$TARGET_URL/api/health/check" > /dev/null 2>&1; then
            END_TIME=$(date +%s%N)
            RESPONSE_TIME=$(( (END_TIME - START_TIME) / 1000000 )) # Convert to milliseconds
            
            # Create enhanced performance results
            cat > test-results/performance-results.json <<EOF
          {
            "health_check": {
              "response_time_ms": $RESPONSE_TIME,
              "status": "success"
            },
            "target_url": "$TARGET_URL",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "test_duration_ms": $(( RESPONSE_TIME + 100 ))
          }
          EOF
            
            echo "✅ Performance tests completed - Response time: ${RESPONSE_TIME}ms"
          else
            echo "❌ Performance tests failed - Health check timeout"
            cat > test-results/performance-results.json <<EOF
          {
            "health_check": {
              "status": "failure",
              "error": "timeout"
            },
            "target_url": "$TARGET_URL",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF
            exit 1
          fi

      - name: 📤 Upload Performance Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ github.run_number }}
          path: test-results/
          retention-days: 7

  # ======================================================================
  # Stage 7: Deployment (Vercel)
  # ======================================================================
  deploy:
    name: 🚀 Deploy to Vercel
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [code-quality, security-scan, unit-tests, e2e-tests]
    if: |
      github.event_name == 'push' &&
      (github.ref == 'refs/heads/main' ||
       github.ref == 'refs/heads/develop' ||
       startsWith(github.ref, 'refs/heads/release/'))
    environment:
      name: ${{ github.ref == 'refs/heads/main' && 'production' || 'preview' }}
      url: ${{ steps.deploy.outputs.url }}

    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4

      - name: 🔧 Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: ▲ Deploy to Vercel
        id: deploy
        env:
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
          VERCEL_ORG_ID: ${{ secrets.VERCEL_ORG_ID }}
          VERCEL_PROJECT_ID: ${{ secrets.VERCEL_PROJECT_ID }}
        run: |
          echo "🚀 Deploying to Vercel..."

          # Install Vercel CLI
          npm install -g vercel@latest

          # Determine environment
          if [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "📦 Deploying to production..."
            DEPLOYMENT_URL=$(vercel --prod --token=$VERCEL_TOKEN --yes)
          else
            echo "👁️ Deploying preview..."
            DEPLOYMENT_URL=$(vercel --token=$VERCEL_TOKEN --yes)
          fi

          echo "✅ Deployed to: $DEPLOYMENT_URL"
          echo "url=$DEPLOYMENT_URL" >> $GITHUB_OUTPUT

      - name: 🔍 Verify deployment
        run: |
          DEPLOY_URL="${{ steps.deploy.outputs.url }}"
          echo "🔍 Verifying deployment at $DEPLOY_URL..."

          # Wait for deployment to be ready
          sleep 10

          # Check if site is accessible
          if curl -s -o /dev/null -w "%{http_code}" "$DEPLOY_URL" | grep -q "200\|301\|302"; then
            echo "✅ Deployment verified successfully"
          else
            echo "⚠️ Deployment may not be ready yet"
          fi

  # ======================================================================
  # Final Status Report
  # ======================================================================
  ci-status:
    name: 📊 CI Status
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [code-quality, security-scan, unit-tests, build-check, e2e-tests]
    if: always()

    steps:
      - name: 📊 Generate Enhanced CI Report
        run: |
          echo "========================================"
          echo "📊 Enhanced CI Pipeline Summary"
          echo "========================================"
          echo "Workflow: ${{ github.workflow }}"
          echo "Run: #${{ github.run_number }}"
          echo "Triggered by: ${{ github.event_name }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Enhanced Features: ✅ Comprehensive test result collection"
          echo "Enhanced Features: ✅ Detailed artifact upload"
          echo "Enhanced Features: ✅ PR comment integration ready"
          echo "========================================"
          echo "Stage Results:"
          echo "  Code Quality:  ${{ needs.code-quality.result == 'success' && '✅ Passed' || '❌ Failed' }}"
          echo "  Security:      ${{ needs.security-scan.result == 'success' && '✅ Passed' || '❌ Failed' }}"
          echo "  Unit Tests:    ${{ needs.unit-tests.result == 'success' && '✅ Passed' || '❌ Failed' }}"
          echo "  Build Check:   ${{ needs.build-check.result == 'success' && '✅ Passed' || '❌ Failed' }}"
          echo "  E2E Tests:     ${{ needs.e2e-tests.result == 'success' && '✅ Passed' || '❌ Failed' }}"
          echo "========================================"

          # Determine overall status
          if [ "${{ needs.code-quality.result }}" == "success" ] && \
             [ "${{ needs.security-scan.result }}" == "success" ] && \
             [ "${{ needs.unit-tests.result }}" == "success" ] && \
             [ "${{ needs.build-check.result }}" == "success" ] && \
             [ "${{ needs.e2e-tests.result }}" == "success" ]; then
            echo "🎉 Overall Status: SUCCESS"
            echo "✅ Enhanced CI pipeline completed with comprehensive result collection!"
            echo "📝 Test results are available for PR comment integration"
            exit 0
          else
            echo "❌ Overall Status: FAILED"
            echo ""
            echo "Please check the failed stages above for details."
            echo "📝 Enhanced test results and artifacts are preserved for analysis"
            exit 1
          fi

      - name: 🔔 Notify on failure
        if: failure() && github.ref == 'refs/heads/main'
        run: |
          echo "🔔 Enhanced CI Pipeline failed on main branch!"
          echo "This requires immediate attention."
          echo "📊 Enhanced artifacts and test results are available for debugging"
          # Add notification logic here (Slack, email, etc.)