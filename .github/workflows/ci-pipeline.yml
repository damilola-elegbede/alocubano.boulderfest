name: "🚀 CI Pipeline"

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened, ready_for_review]
  deployment_status:
    # Trigger E2E tests when Vercel deployment completes
  workflow_dispatch:
    inputs:
      preview_url:
        description: 'Preview URL for E2E tests (optional)'
        required: false
        type: string

concurrency:
  group: ci-pipeline-${{ github.workflow }}-${{ github.ref }}-${{ github.event_name }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  deployments: read
  statuses: write
  # Optional: issues: write  # For PR comments and issue creation

env:
  NODE_ENV: test
  CI: true
  NODE_VERSION: "20"
  NODE_OPTIONS: "--max-old-space-size=4096"

jobs:
  unit-tests:
    name: "🧪 Unit Tests (Node ${{ matrix.node-version }})"
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Increased from 10 to allow for cold starts
    outputs:
      total_tests: ${{ steps.unit-test.outputs.total_tests }}
      passing_tests: ${{ steps.unit-test.outputs.passing_tests }}
      failing_tests: ${{ steps.unit-test.outputs.failing_tests }}
      duration: ${{ steps.unit-test.outputs.duration }}

    strategy:
      matrix:
        node-version: ['20.x', '22.x']

    env:
      DATABASE_URL: ":memory:"
      PHASE3_PERFORMANCE_TARGET_MS: 2000
      VITEST_TEST_TIMEOUT: 10000       # Increased from 5000 to 10 seconds for unit tests
      VITEST_HOOK_TIMEOUT: 15000       # Increased from 10000 to 15 seconds for hooks
      VITEST_SETUP_TIMEOUT: 15000      # Increased from 10000 to 15 seconds for setup
      VITEST_CLEANUP_TIMEOUT: 10000    # Increased from 5000 to 10 seconds for cleanup
      # Critical API secrets configuration - required for services to initialize (matching vitest.unit.config.js)
      QR_SECRET_KEY: 'test-qr-secret-key-minimum-32-characters-long-for-security-compliance'
      ADMIN_SECRET: 'test-admin-jwt-secret-minimum-32-characters-for-security'
      WALLET_AUTH_SECRET: 'test-wallet-auth-secret-key-for-testing-purposes-32-chars'
      APPLE_PASS_KEY: 'dGVzdC1hcHBsZS1wYXNzLWtleQ=='
      INTERNAL_API_KEY: 'test-internal-api-key-32-chars-min'
      TEST_ADMIN_PASSWORD: 'test-admin-password-123'
      ADMIN_PASSWORD: '$2b$10$test.bcrypt.hash.for.testing.purposes.only'

    steps:
      - name: "📥 Checkout Code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "🔧 Setup Node.js ${{ matrix.node-version }}"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: "📦 Install Dependencies"
        env:
          NGROK_SKIP_DOWNLOAD: true
          # Force npm to rebuild native binaries for current platform
          npm_config_build_from_source: true
          # Ensure proper Python for node-gyp
          PYTHON: python3
        run: |
          echo "📦 Installing dependencies..."

          # Install with npm ci for reproducibility
          npm ci --prefer-offline --no-audit --no-fund

          # Workaround for npm optional dependencies bug #4828
          # Force install Linux-specific binaries that npm ci might skip
          if [ "$RUNNER_OS" = "Linux" ]; then
            echo "🔧 Checking for platform-specific binaries..."

            # Install Rollup Linux binary if missing
            if [ ! -d "node_modules/@rollup/rollup-linux-x64-gnu" ]; then
              echo "⚠️  Rollup Linux binary missing (npm bug #4828)"
              echo "🔧 Installing @rollup/rollup-linux-x64-gnu..."
              npm install @rollup/rollup-linux-x64-gnu@4.50.0 --no-save --no-audit
            fi

            # Install LibSQL Linux binary if missing
            if [ ! -d "node_modules/@libsql/linux-x64-gnu" ]; then
              echo "⚠️  LibSQL Linux binary missing (npm bug #4828)"
              echo "🔧 Installing @libsql/linux-x64-gnu..."
              npm install @libsql/linux-x64-gnu --no-save --no-audit || true
            fi

            # Rebuild native modules
            echo "🔄 Rebuilding native modules..."
            npm rebuild
          fi

          echo "✅ Dependencies installed and verified"

      - name: "🧪 Run Unit Tests"
        id: unit-test
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🚀 Running Unit Test Suite"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📊 Expected: 806+ unit tests"
          echo "🎯 Performance Target: <2 seconds"
          echo "💾 Database: In-memory SQLite"
          echo "⏱️  Timeout: 15 minutes (CI buffer)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          start_time=$(date +%s%3N)
          npm test 2>&1 | tee unit-test-output.log
          test_exit_code=${PIPESTATUS[0]}
          end_time=$(date +%s%3N)
          duration=$((end_time - start_time))

          # Extract test counts - using simple sed approach instead of variable-length lookbehind
          total_tests=$(grep -E "Tests.*passed" unit-test-output.log | sed -E "s/.*Tests[[:space:]]+([0-9]+)[[:space:]]+passed.*/\1/" | head -1 || echo "0")
          passing_tests=$total_tests
          failing_tests=$(grep -E "Tests.*failed" unit-test-output.log | sed -E "s/.*Tests[[:space:]]+([0-9]+)[[:space:]]+failed.*/\1/" | head -1 || echo "0")

          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📊 Unit Test Results"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Tests Passed: $passing_tests"
          echo "❌ Tests Failed: $failing_tests"
          echo "📈 Total Tests: $total_tests"
          echo "⏱️  Duration: ${duration}ms"

          if [ "$duration" -lt "$PHASE3_PERFORMANCE_TARGET_MS" ]; then
            echo "🏆 EXCELLENT: Unit tests completed within 2-second target!"
          else
            echo "⚠️  WARNING: Unit tests exceeded 2-second target"
          fi

          # Set outputs for downstream jobs
          echo "total_tests=$total_tests" >> $GITHUB_OUTPUT
          echo "passing_tests=$passing_tests" >> $GITHUB_OUTPUT
          echo "failing_tests=$failing_tests" >> $GITHUB_OUTPUT
          echo "duration=${duration}ms" >> $GITHUB_OUTPUT

          exit $test_exit_code

      - name: "📤 Upload Unit Test Results"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-node-${{ matrix.node-version }}
          path: unit-test-output.log
          retention-days: 7

  integration-tests:
    name: "🔗 Integration Tests (Node ${{ matrix.node-version }})"
    runs-on: ubuntu-latest
    timeout-minutes: 5  # Increased from 1 to 5 minutes for database initialization and migration
    needs: unit-tests   # Run after unit tests pass
    outputs:
      total_tests: ${{ steps.integration-test.outputs.total_tests }}
      passing_tests: ${{ steps.integration-test.outputs.passing_tests }}
      failing_tests: ${{ steps.integration-test.outputs.failing_tests }}
      duration: ${{ steps.integration-test.outputs.duration }}

    strategy:
      matrix:
        node-version: ['20.x', '22.x']

    env:
      DATABASE_URL: "file:./data/test-integration.db"
      # Force build-time cache access for integration tests (like integration-tests.yml)
      INTEGRATION_TEST_MODE: "true"  # Force shouldUseBuildTimeCache() to return true
      NODE_ENV: "test"  # Match integration-tests.yml environment
      VERCEL: ""  # Explicitly unset to ensure isLocalDevelopment() returns true
      # Test environment variables for integration tests
      STRIPE_SECRET_KEY: ${{ secrets.TEST_STRIPE_SECRET_KEY || 'sk_test_dummy_integration_key' }}
      STRIPE_PUBLISHABLE_KEY: ${{ secrets.TEST_STRIPE_PUBLISHABLE_KEY || 'pk_test_dummy_integration_key' }}
      BREVO_API_KEY: ${{ secrets.TEST_BREVO_API_KEY || 'test_brevo_integration_key' }}
      BREVO_NEWSLETTER_LIST_ID: "1"
      BREVO_WEBHOOK_SECRET: ${{ secrets.TEST_BREVO_WEBHOOK_SECRET || 'test_webhook_secret' }}
      ADMIN_SECRET: ${{ secrets.TEST_ADMIN_SECRET || 'test_admin_secret_minimum_32_characters_for_jwt_signing' }}
      ADMIN_PASSWORD: "$2b$10$dummy.bcrypt.hash.for.integration.testing"
      INTERNAL_API_KEY: ${{ secrets.TEST_INTERNAL_API_KEY }}
      WALLET_AUTH_SECRET: ${{ secrets.TEST_WALLET_AUTH_SECRET || 'test_wallet_auth_secret_minimum_32_chars' }}
      APPLE_PASS_KEY: ${{ secrets.TEST_APPLE_PASS_KEY || 'dGVzdF9hcHBsZV9wYXNzX2tleQ==' }}
      # Integration test timeouts - increased for CI environment
      VITEST_TEST_TIMEOUT: 60000       # Increased from 30000 to 60 seconds for integration tests
      VITEST_HOOK_TIMEOUT: 45000       # Increased from 30000 to 45 seconds for hooks
      VITEST_SETUP_TIMEOUT: 20000      # Increased from 10000 to 20 seconds for setup
      VITEST_CLEANUP_TIMEOUT: 10000    # Increased from 5000 to 10 seconds for cleanup
      VITEST_REQUEST_TIMEOUT: 45000    # Increased from 30000 to 45 seconds for HTTP requests

    steps:
      - name: "📥 Checkout Code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "🔧 Setup Node.js ${{ matrix.node-version }}"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: package-lock.json

      - name: "💾 Restore Dependencies Cache"
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
          key: ${{ runner.os }}-node-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: "📦 Install Dependencies"
        env:
          NGROK_SKIP_DOWNLOAD: true
          # Force npm to rebuild native binaries for current platform
          npm_config_build_from_source: true
          # Ensure proper Python for node-gyp
          PYTHON: python3
        run: |
          echo "📦 Installing dependencies..."

          # Install with npm ci for reproducibility
          npm ci --prefer-offline --no-audit --no-fund

          # Workaround for npm optional dependencies bug #4828
          # Force install Linux-specific binaries that npm ci might skip
          if [ "$RUNNER_OS" = "Linux" ]; then
            echo "🔧 Checking for platform-specific binaries..."

            # Install Rollup Linux binary if missing
            if [ ! -d "node_modules/@rollup/rollup-linux-x64-gnu" ]; then
              echo "⚠️  Rollup Linux binary missing (npm bug #4828)"
              echo "🔧 Installing @rollup/rollup-linux-x64-gnu..."
              npm install @rollup/rollup-linux-x64-gnu@4.50.0 --no-save --no-audit
            fi

            # Install LibSQL Linux binary if missing
            if [ ! -d "node_modules/@libsql/linux-x64-gnu" ]; then
              echo "⚠️  LibSQL Linux binary missing (npm bug #4828)"
              echo "🔧 Installing @libsql/linux-x64-gnu..."
              npm install @libsql/linux-x64-gnu --no-save --no-audit || true
            fi

            # Rebuild native modules
            echo "🔄 Rebuilding native modules..."
            npm rebuild
          fi

          echo "✅ Dependencies installed and verified"

      - name: "🗃️ Setup SQLite Database"
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🗃️ Setting up Integration Test Database"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          # Create data directory if it doesn't exist
          mkdir -p data

          # Remove any existing integration test database
          rm -f data/test-integration.db

          # Run database migrations first
          echo "🔄 Running database migrations..."
          NODE_ENV=test node scripts/migrate.js

          # Verify database creation
          if [ -f "data/test-integration.db" ]; then
            echo "✅ Integration test database created successfully"
            echo "📊 Database size: $(du -h data/test-integration.db | cut -f1)"
          else
            echo "❌ Failed to create integration test database"
            exit 1
          fi

      - name: "🔗 Run Integration Tests"
        id: integration-test
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🔗 Running Integration Test Suite"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🎯 Timeout: 5 minutes total (stage limit)"
          echo "🗃️ Database: SQLite with real file storage"
          echo "🌐 APIs: Limited external service integration"
          echo "🧪 Expected: ~30-50 integration tests"
          echo "⚙️  Environment: Test credentials configured"
          echo "⏱️  Individual Test Timeout: 60 seconds"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          # Record start time
          start_time=$(date +%s%3N)

          # Run integration tests with increased timeout (280 seconds, 20 seconds buffer)
          timeout 280s npm run test:integration 2>&1 | tee integration-test-output.log
          test_exit_code=${PIPESTATUS[0]}

          # Record end time
          end_time=$(date +%s%3N)
          duration=$((end_time - start_time))

          # Extract test counts from output - using simple sed approach instead of variable-length lookbehind
          total_tests=$(grep -E "Tests.*passed" integration-test-output.log | sed -E "s/.*Tests[[:space:]]+([0-9]+)[[:space:]]+passed.*/\1/" | head -1 || echo "0")
          passing_tests=$total_tests
          failing_tests=$(grep -E "Tests.*failed" integration-test-output.log | sed -E "s/.*Tests[[:space:]]+([0-9]+)[[:space:]]+failed.*/\1/" | head -1 || echo "0")

          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📊 Integration Test Results"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Tests Passed: $passing_tests"
          echo "❌ Tests Failed: $failing_tests"
          echo "📈 Total Tests: $total_tests"
          echo "⏱️  Duration: ${duration}ms"
          echo "🗃️ Database: File-based SQLite"

          # Timeout evaluation
          if [ "$duration" -lt 300000 ]; then
            echo "🏆 EXCELLENT: Integration tests completed within 5-minute limit!"
          else
            echo "⚠️  WARNING: Integration tests exceeded 5-minute timeout limit"
          fi

          # Set outputs for summary
          echo "total_tests=$total_tests" >> $GITHUB_OUTPUT
          echo "passing_tests=$passing_tests" >> $GITHUB_OUTPUT
          echo "failing_tests=$failing_tests" >> $GITHUB_OUTPUT
          echo "duration=${duration}ms" >> $GITHUB_OUTPUT

          exit $test_exit_code

      - name: "🧹 Database Cleanup"
        if: always()
        run: |
          echo "🧹 Cleaning up integration test database..."
          rm -f data/test-integration.db
          echo "✅ Integration test database cleanup completed"

      - name: "📤 Upload Integration Test Results"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results-node-${{ matrix.node-version }}
          path: integration-test-output.log
          retention-days: 7

  # ============================================================================
  # PREVIEW URL EXTRACTION - For E2E Testing with Vercel Deployments
  # ============================================================================
  extract-preview-url:
    name: "🔗 Extract Preview URL"
    runs-on: ubuntu-latest
    if: github.event_name == 'deployment_status' && github.event.deployment_status.state == 'success'
    outputs:
      preview_url: ${{ steps.extract_url.outputs.preview_url }}
      commit_sha: ${{ steps.extract_url.outputs.commit_sha }}
      pr_number: ${{ steps.extract_url.outputs.pr_number }}

    steps:
      - name: "🔍 Extract Deployment Info"
        id: extract_url
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🔗 Extracting Preview URL from Deployment"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          PREVIEW_URL="${{ github.event.deployment_status.target_url }}"
          COMMIT_SHA="${{ github.event.deployment.sha }}"
          ENVIRONMENT="${{ github.event.deployment.environment }}"

          echo "📌 Deployment Environment: $ENVIRONMENT"
          echo "🔗 Preview URL: $PREVIEW_URL"
          echo "📝 Commit SHA: $COMMIT_SHA"

          # Extract PR number from deployment ref
          DEPLOYMENT_REF="${{ github.event.deployment.ref }}"
          if [[ "$DEPLOYMENT_REF" =~ ^refs/pull/([0-9]+)/ ]]; then
            PR_NUMBER="${BASH_REMATCH[1]}"
            echo "🔢 PR Number: $PR_NUMBER"
          else
            echo "⚠️ Could not extract PR number from ref: $DEPLOYMENT_REF"
            PR_NUMBER=""
          fi

          # Set outputs
          echo "preview_url=$PREVIEW_URL" >> $GITHUB_OUTPUT
          echo "commit_sha=$COMMIT_SHA" >> $GITHUB_OUTPUT
          echo "pr_number=$PR_NUMBER" >> $GITHUB_OUTPUT

          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Preview URL extracted successfully"

  # ============================================================================
  # E2E TESTS - With Dynamic Test Matrix and Browser Support
  # ============================================================================
  build-test-matrix:
    name: "📊 Build E2E Test Matrix"
    runs-on: ubuntu-latest
    needs: extract-preview-url
    if: github.event_name == 'deployment_status' || github.event.inputs.preview_url != ''
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
      has_tests: ${{ steps.set-matrix.outputs.has_tests }}

    steps:
      - name: "📥 Checkout Code"
        uses: actions/checkout@v4
        with:
          ref: ${{ needs.extract-preview-url.outputs.commit_sha || github.sha }}

      - name: "🔧 Build Test Matrix"
        id: set-matrix
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📊 Building E2E Test Execution Matrix"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          # Discover test files
          TEST_FILES=$(find tests/e2e/flows -name "*.test.js" -type f | sort)

          # Build browser-test combinations
          MATRIX_JSON='{"include":['
          FIRST=true

          for TEST_FILE in $TEST_FILES; do
            TEST_NAME=$(basename "$TEST_FILE" .test.js)

            # Add combinations for each browser
            for BROWSER in chromium firefox webkit; do
              if [ "$FIRST" = true ]; then
                FIRST=false
              else
                MATRIX_JSON+=','
              fi

              MATRIX_JSON+="{\"browser\":\"$BROWSER\",\"test_suite\":\"$TEST_NAME\",\"test_file\":\"$TEST_FILE\"}"
            done
          done

          # Add mobile browser tests for specific suites
          MOBILE_TESTS=("mobile-registration-experience" "basic-navigation" "gallery-basic")
          for TEST in "${MOBILE_TESTS[@]}"; do
            if echo "$TEST_FILES" | grep -q "$TEST.test.js"; then
              TEST_FILE=$(echo "$TEST_FILES" | grep "$TEST.test.js")

              # Mobile Chrome
              MATRIX_JSON+=",{\"browser\":\"mobile-chrome\",\"test_suite\":\"$TEST\",\"test_file\":\"$TEST_FILE\"}"

              # Mobile Safari
              MATRIX_JSON+=",{\"browser\":\"mobile-safari\",\"test_suite\":\"$TEST\",\"test_file\":\"$TEST_FILE\"}"
            fi
          done

          MATRIX_JSON+=']}'

          # Count tests
          TEST_COUNT=$(echo "$TEST_FILES" | wc -l)

          echo "📋 Found $TEST_COUNT test files"
          echo "🌐 Browsers: chromium, firefox, webkit, mobile-chrome, mobile-safari"
          echo "📊 Matrix combinations: $(echo "$MATRIX_JSON" | jq '.include | length')"

          # Set outputs
          echo "matrix=$MATRIX_JSON" >> $GITHUB_OUTPUT
          echo "has_tests=$( [ "$TEST_COUNT" -gt 0 ] && echo "true" || echo "false" )" >> $GITHUB_OUTPUT

          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Test matrix built successfully"

  e2e-tests:
    name: "🎭 E2E: ${{ matrix.test_suite }} (${{ matrix.browser }})"
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [extract-preview-url, build-test-matrix]
    if: (github.event_name == 'deployment_status' || github.event.inputs.preview_url != '') && needs.build-test-matrix.outputs.has_tests == 'true'

    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.build-test-matrix.outputs.matrix) }}

    env:
      PREVIEW_URL: ${{ needs.extract-preview-url.outputs.preview_url || github.event.inputs.preview_url }}
      PLAYWRIGHT_BROWSERS_PATH: ${{ github.workspace }}/.playwright-browsers
      # E2E Test Environment Variables
      TEST_ADMIN_PASSWORD: ${{ secrets.TEST_ADMIN_PASSWORD }}
      ADMIN_PASSWORD: ${{ secrets.ADMIN_PASSWORD }}
      ADMIN_SECRET: ${{ secrets.ADMIN_SECRET }}
      TURSO_DATABASE_URL: ${{ vars.TURSO_DATABASE_URL }}
      TURSO_AUTH_TOKEN: ${{ secrets.TURSO_AUTH_TOKEN }}
      GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
      GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
      GOOGLE_DRIVE_GALLERY_FOLDER_ID: ${{ secrets.GOOGLE_DRIVE_GALLERY_FOLDER_ID }}
      # Stripe Test Credentials
      STRIPE_SECRET_KEY: ${{ secrets.STRIPE_SECRET_KEY }}
      STRIPE_PUBLISHABLE_KEY: ${{ secrets.STRIPE_PUBLISHABLE_KEY }}
      # Timeout Configuration
      E2E_STARTUP_TIMEOUT: ${{ vars.E2E_STARTUP_TIMEOUT || '60000' }}
      E2E_TEST_TIMEOUT: ${{ vars.E2E_TEST_TIMEOUT || '120000' }}
      E2E_ACTION_TIMEOUT: ${{ vars.E2E_ACTION_TIMEOUT || '30000' }}
      E2E_NAVIGATION_TIMEOUT: ${{ vars.E2E_NAVIGATION_TIMEOUT || '30000' }}
      E2E_WEBSERVER_TIMEOUT: ${{ vars.E2E_WEBSERVER_TIMEOUT || '180000' }}
      E2E_EXPECT_TIMEOUT: ${{ vars.E2E_EXPECT_TIMEOUT || '10000' }}
      E2E_HEALTH_CHECK_INTERVAL: ${{ vars.E2E_HEALTH_CHECK_INTERVAL || '2000' }}

    steps:
      - name: "📥 Checkout Code"
        uses: actions/checkout@v4
        with:
          ref: ${{ needs.extract-preview-url.outputs.commit_sha || github.sha }}

      - name: "🔧 Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: "📦 Install Dependencies"
        run: |
          npm ci --prefer-offline --no-audit --no-fund
        env:
          NGROK_SKIP_DOWNLOAD: true

      - name: "🧩 Cache Playwright Browsers"
        uses: actions/cache@v4
        with:
          path: .playwright-browsers
          key: ${{ runner.os }}-playwright-${{ matrix.browser }}-${{ hashFiles('package-lock.json') }}-v3
          restore-keys: |
            ${{ runner.os }}-playwright-${{ matrix.browser }}-

      - name: "🎭 Install Playwright Browser"
        run: |
          # Map mobile browsers to their base browser
          INSTALL_BROWSER="${{ matrix.browser }}"

          case "${{ matrix.browser }}" in
            mobile-chrome)
              INSTALL_BROWSER="chromium"
              ;;
            mobile-safari)
              INSTALL_BROWSER="webkit"
              ;;
          esac

          echo "🔄 Installing Playwright browser: $INSTALL_BROWSER"
          npx playwright install --with-deps "$INSTALL_BROWSER"
          echo "✅ Browser installed: $INSTALL_BROWSER"

      - name: "🎯 Run E2E Test: ${{ matrix.test_suite }}"
        id: run-test
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🎭 Running E2E Test Suite"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📝 Test: ${{ matrix.test_suite }}"
          echo "🌐 Browser: ${{ matrix.browser }}"
          echo "🔗 Preview URL: $PREVIEW_URL"
          echo "📁 Test File: ${{ matrix.test_file }}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          # Map browser to Playwright project
          BROWSER_PROJECT="${{ matrix.browser }}"
          case "${{ matrix.browser }}" in
            mobile-chrome)
              BROWSER_PROJECT="Mobile Chrome"
              ;;
            mobile-safari)
              BROWSER_PROJECT="Mobile Safari"
              ;;
          esac

          # Run the test
          npx playwright test "${{ matrix.test_file }}" \
            --project="$BROWSER_PROJECT" \
            --reporter=list \
            --retries=2 \
            || TEST_EXIT_CODE=$?

          # Handle results
          if [ "${TEST_EXIT_CODE:-0}" -eq 0 ]; then
            echo "✅ Test passed: ${{ matrix.test_suite }} on ${{ matrix.browser }}"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "❌ Test failed: ${{ matrix.test_suite }} on ${{ matrix.browser }}"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: "📸 Upload Test Artifacts"
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-artifacts-${{ matrix.test_suite }}-${{ matrix.browser }}
          path: |
            playwright-report/
            test-results/
            screenshots/
          retention-days: 7

  # ============================================================================
  # E2E TEST RESULTS COLLECTION
  # ============================================================================
  e2e-results:
    name: "📊 E2E Test Results"
    runs-on: ubuntu-latest
    needs: [extract-preview-url, e2e-tests]
    if: always() && (github.event_name == 'deployment_status' || github.event.inputs.preview_url != '')

    steps:
      - name: "📊 Post E2E Results to PR"
        if: needs.extract-preview-url.outputs.pr_number != ''
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = '${{ needs.extract-preview-url.outputs.pr_number }}';
            const previewUrl = '${{ needs.extract-preview-url.outputs.preview_url }}';

            // Analyze job results
            const jobs = await github.rest.actions.listJobsForWorkflowRun({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId
            });

            const e2eJobs = jobs.data.jobs.filter(job => job.name.startsWith('🎭 E2E:'));
            const passedJobs = e2eJobs.filter(job => job.conclusion === 'success');
            const failedJobs = e2eJobs.filter(job => job.conclusion === 'failure');

            const overallStatus = failedJobs.length === 0 ? '✅' : '❌';
            const statusText = failedJobs.length === 0 ? 'PASSED' : 'FAILED';

            // Build results table
            let resultsTable = `| Test Suite | Browser | Status | Duration |\n|------------|---------|--------|----------|\n`;

            for (const job of e2eJobs) {
              const match = job.name.match(/E2E: ([\w-]+) \(([\w-]+)\)/);
              if (match) {
                const [, testSuite, browser] = match;
                const status = job.conclusion === 'success' ? '✅' : '❌';
                const duration = job.completed_at && job.started_at ?
                  `${Math.round((new Date(job.completed_at) - new Date(job.started_at)) / 1000)}s` :
                  'N/A';

                resultsTable += `| ${testSuite} | ${browser} | ${status} | ${duration} |\n`;
              }
            }

            const comment = `## ${overallStatus} E2E Tests ${statusText}

            ### 🎭 Test Execution Summary
            - **Preview URL**: ${previewUrl}
            - **Total Tests**: ${e2eJobs.length}
            - **Passed**: ${passedJobs.length} ✅
            - **Failed**: ${failedJobs.length} ❌

            ### 📊 Detailed Results
            ${resultsTable}

            ### 🌐 Browser Coverage
            - **Desktop**: Chromium, Firefox, WebKit
            - **Mobile**: Chrome Mobile, Safari Mobile

            ${failedJobs.length > 0 ? `
            ### ⚠️ Failed Tests
            Please review the failed tests and their artifacts in the workflow run.
            ` : `
            ### 🎉 All E2E tests passed successfully!
            The application is working correctly in the preview environment.
            `}

            [View Full Workflow Run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
            `;

            // Post or update comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: parseInt(prNumber),
              body: comment
            });

  summary:
    name: "📊 CI Pipeline Summary"
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always() && github.event_name == 'pull_request'

    steps:
      - name: "📊 Generate CI Pipeline Report"
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const unitStatus = '${{ needs.unit-tests.result }}';
            const integrationStatus = '${{ needs.integration-tests.result }}';

            // Get output values from jobs
            const unitOutputs = ${{ toJSON(needs.unit-tests.outputs) }};
            const integrationOutputs = ${{ toJSON(needs.integration-tests.outputs) }};

            const overallStatus = (unitStatus === 'success' && integrationStatus === 'success') ? '✅' : '❌';
            const statusText = (unitStatus === 'success' && integrationStatus === 'success') ? 'PASSED' : 'FAILED';

            const comment = `## ${overallStatus} CI Pipeline ${statusText}

            ### 🚀 Pipeline Results
            | Stage | Status | Tests | Duration | Details |
            |-------|--------|-------|----------|---------|
            | **Unit Tests** | ${unitStatus === 'success' ? '✅ Passed' : '❌ Failed'} | ${unitOutputs?.total_tests || 'N/A'} | ${unitOutputs?.duration || 'N/A'} | In-memory SQLite, <2s target, Node 20.x & 22.x |
            | **Integration Tests** | ${integrationStatus === 'success' ? '✅ Passed' : '❌ Failed'} | ${integrationOutputs?.total_tests || 'N/A'} | ${integrationOutputs?.duration || 'N/A'} | File SQLite, <5min limit, Node 20.x & 22.x |
            | **E2E Tests** | ⏳ Pending | 12 test suites × 5 browsers | Runs on deployment | Triggered by Vercel deployment completion |

            ### 🎯 Unified CI/CD Architecture
            - **Pull Request Tests**: Unit + Integration tests run immediately on PR
            - **E2E Tests**: Automatically triggered when Vercel deployment succeeds
            - **Single Workflow**: All testing consolidated into unified CI Pipeline
            - **Smart Triggers**: Different test stages activate based on event type
            - **Browser Matrix**: Chromium, Firefox, WebKit, Mobile Chrome, Mobile Safari

            ### ⚙️ Pipeline Features
            - **Multi-Node Support**: Tests run on Node.js 20.x and 22.x
            - **Sequential Execution**: Each stage runs only after previous stages pass
            - **Quality Gates**: Unit and Integration tests must pass for PR approval
            - **Deployment-Triggered E2E**: E2E tests run automatically on preview deployments
            - **Database Isolation**: In-memory (unit) → File (integration) → Production (E2E)
            - **Dependency Caching**: NPM dependencies cached for faster execution
            - **Automated Cleanup**: Database and artifact cleanup after tests

            ${overallStatus === '✅' ?
              '### 🎉 Unit and Integration tests passed!' :
              '### ⚠️ Pipeline failures detected - review workflow output'}

            ### 📋 Next Steps
            ${unitStatus !== 'success' ? '- ❌ Fix failing unit tests\n' : ''}
            ${unitStatus === 'success' && integrationStatus !== 'success' ? '- ❌ Fix integration test failures\n' : ''}
            ${overallStatus === '✅' ? '- ✅ Ready for deployment - E2E tests will run automatically when Vercel deployment completes\n' : ''}

            ### 🔄 Workflow Consolidation
            - **Unified Pipeline**: All tests now in single CI Pipeline workflow
            - **Event-Based Execution**: PR tests → Deploy → E2E tests
            - **No Duplicate Runs**: Smart conditional logic prevents redundant execution
            - **Streamlined Maintenance**: Single workflow file to maintain
            `;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('CI Pipeline')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: "🔒 Deployment Quality Gate"
        if: github.event_name == 'pull_request'
        run: |
          UNIT_STATUS="${{ needs.unit-tests.result }}"
          INTEGRATION_STATUS="${{ needs.integration-tests.result }}"

          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🔒 Deployment Quality Gate Validation"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🧪 Unit Tests: $UNIT_STATUS"
          echo "🔗 Integration Tests: $INTEGRATION_STATUS"
          echo "🎭 E2E Tests: Will run on deployment"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          if [ "$UNIT_STATUS" = "success" ] && [ "$INTEGRATION_STATUS" = "success" ]; then
            echo "✅ CI PIPELINE QUALITY GATES PASSED - Ready for deployment"
            echo "🎉 This PR meets CI pipeline requirements:"
            echo "   - Unit tests: Fast feedback loop validated (Node 20.x & 22.x)"
            echo "   - Integration tests: API contracts verified (Node 20.x & 22.x)"
            echo "   - E2E tests: Will run automatically when Vercel deployment completes"
            echo ""
            echo "📋 Unified Pipeline Benefits:"
            echo "   - All tests in single CI Pipeline workflow"
            echo "   - E2E tests trigger automatically on deployment_status"
            echo "   - No duplicate test execution or conflicts"
            echo "   - Streamlined workflow maintenance"
            exit 0
          else
            echo "❌ CI PIPELINE QUALITY GATE FAILURE - Deployment blocked"
            echo "🚫 This PR does not meet CI pipeline requirements:"
            [ "$UNIT_STATUS" != "success" ] && echo "   - Unit tests failed or skipped"
            [ "$INTEGRATION_STATUS" != "success" ] && echo "   - Integration tests failed or skipped"
            echo "📋 Action required: Fix failing tests before deployment"
            echo ""
            echo "ℹ️  E2E tests will only run after unit and integration tests pass"
            exit 1
          fi