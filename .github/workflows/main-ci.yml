# ======================================================================
# A Lo Cubano Boulder Fest - CI/CD Pipeline
# ======================================================================
# Smart dependency chains with fallback resilience:
# - No cascading failures from single-point failures
# - Conditional execution based on prerequisite success
# - Maximum test coverage even with partial failures
# - Clear reporting on execution vs skipped steps
# - FIXED: Standardized environment variables (Issue #6)
# ======================================================================

name: "CI/CD Pipeline"

on:
  push:
    branches: [main, develop, "feature/**", "release/**", "hotfix/**"]
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened, ready_for_review]
  workflow_dispatch:

# Cancel in-progress runs for the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# FIXED: Standardized global environment variables (Issue #6)
env:
  # Universal Configuration
  NODE_VERSION: "20"
  CI: true
  NODE_ENV: test
  
  # Performance Optimization
  NODE_OPTIONS: "--max-old-space-size=4096"
  NPM_CONFIG_CACHE: ${{ github.workspace }}/.npm-cache
  
  # Database Configuration (environment-specific)
  DATABASE_URL: "file:./data/ci-test.db"
  
  # CI Environment Type
  CI_ENVIRONMENT: "ci"

jobs:
  # ======================================================================
  # Stage 0: Environment Validation (NEW)
  # ======================================================================
  validate-environment:
    name: ğŸ” Environment Validation
    runs-on: ubuntu-latest
    timeout-minutes: 2
    outputs:
      validation_passed: ${{ steps.validate.outcome }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: ğŸ“¦ Install dependencies (validation only)
        run: npm ci --prefer-offline --no-audit --ignore-scripts

      - name: âœ… Validate Environment Configuration
        id: validate
        run: |
          echo "ğŸ” Validating CI environment configuration..."
          node scripts/ci/validate-environment.js --env=ci
          
          echo "âœ… Environment validation passed"
        env:
          # Test configuration variables
          BASE_URL: "http://localhost:3000"
          DATABASE_URL: ${{ env.DATABASE_URL }}
          CI_ENVIRONMENT: ${{ env.CI_ENVIRONMENT }}

  # ======================================================================
  # Stage 1: Unit Tests (Always Run - Foundation)
  # ======================================================================
  unit-tests:
    name: ğŸ§ª Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: validate-environment
    outputs:
      status: ${{ steps.test.outcome }}
      coverage: ${{ steps.coverage.outputs.coverage }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: ğŸ“¦ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: ğŸ§ª Run unit tests
        id: test
        run: npm test
        env:
          NODE_ENV: ${{ env.NODE_ENV }}
          CI: ${{ env.CI }}
          DATABASE_URL: ${{ env.DATABASE_URL }}
          # Vitest timeout configuration
          VITEST_TEST_TIMEOUT: 60000
          VITEST_HOOK_TIMEOUT: 30000

      - name: ğŸ“Š Extract coverage percentage
        id: coverage
        if: always()
        run: |
          if [ -f coverage/coverage-summary.json ]; then
            COVERAGE=$(node -e "console.log(JSON.parse(require('fs').readFileSync('coverage/coverage-summary.json')).total.lines.pct || 0)")
            echo "coverage=${COVERAGE}" >> $GITHUB_OUTPUT
          else
            echo "coverage=0" >> $GITHUB_OUTPUT
          fi

      - name: ğŸ“Š Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results
          path: |
            coverage/
            test-results/

  # ======================================================================
  # Stage 2: Build Verification (Always Runs, Warnings Only on Fail)
  # ======================================================================
  build:
    name: ğŸ—ï¸ Build Verification
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: validate-environment
    outputs:
      status: ${{ steps.build.outcome }}
      artifacts: ${{ steps.verify.outputs.artifacts }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: ğŸ“¦ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: ğŸ”¨ Build project
        id: build
        continue-on-error: true
        run: |
          echo "ğŸ”¨ Starting build process..."
          npm run build
          echo "âœ… Build completed successfully"

      - name: âœ… Verify build artifacts
        id: verify
        if: always()
        run: |
          echo "Verifying build output..."
          if [ "${{ steps.build.outcome }}" = "success" ]; then
            echo "âœ… Build artifacts verified"
            echo "artifacts=available" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸ Build failed - continuing with degraded mode"
            echo "artifacts=unavailable" >> $GITHUB_OUTPUT
          fi
          ls -la

  # ======================================================================
  # Stage 3: Security Scan (Independent - Always Runs)
  # ======================================================================
  security-scan:
    name: ğŸ”’ Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: validate-environment
    outputs:
      status: ${{ steps.security.outcome }}
      vulnerabilities: ${{ steps.audit.outputs.vulnerabilities }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: ğŸ“¦ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: ğŸ”’ Run security audit
        id: security
        continue-on-error: true
        run: |
          echo "ğŸ”’ Running security audit..."
          npm audit --audit-level=high

      - name: ğŸ“Š Extract vulnerability count
        id: audit
        if: always()
        run: |
          VULN_COUNT=$(npm audit --json --audit-level=high 2>/dev/null | jq -r '.metadata.vulnerabilities.total // 0' || echo "0")
          echo "vulnerabilities=${VULN_COUNT}" >> $GITHUB_OUTPUT
          echo "Found ${VULN_COUNT} high-severity vulnerabilities"

  # ======================================================================
  # Stage 4: Deploy to Vercel (Only if Build Succeeds)
  # ======================================================================
  deploy-preview:
    name: ğŸš€ Deploy to Vercel
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [build]
    if: github.event_name == 'pull_request' && needs.build.outputs.status == 'success'
    outputs:
      preview-url: ${{ steps.deploy.outputs.preview-url }}
      status: ${{ steps.deploy.outcome }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸš€ Wait for Vercel Preview
        id: deploy
        run: |
          echo "ğŸš€ Waiting for Vercel bot to deploy preview..."
          # Wait for Vercel deployment (the bot handles this)
          sleep 30
          
          # Extract preview URL from PR comments
          PREVIEW_URL=$(gh pr view ${{ github.event.number }} --json comments -q '.comments[] | select(.author.login == "vercel[bot]") | .body' | grep -oP 'https://[a-z0-9-]+\.vercel\.app' | head -1)
          
          if [ -z "$PREVIEW_URL" ]; then
            echo "âš ï¸ No preview URL found yet, using fallback"
            PREVIEW_URL="https://alocubano-boulderfest.vercel.app"
          fi
          
          echo "âœ… Preview URL: $PREVIEW_URL"
          echo "preview-url=$PREVIEW_URL" >> $GITHUB_OUTPUT
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # ======================================================================
  # Stage 5: E2E Tests (Only if Deploy Succeeds) - Unified Browser Matrix
  # ======================================================================
  e2e-tests:
    name: ğŸ­ E2E Tests (${{ matrix.browser-name }})
    runs-on: ubuntu-latest
    timeout-minutes: ${{ matrix.timeout-minutes }}
    needs: [deploy-preview]
    if: github.event_name == 'pull_request' && needs.deploy-preview.outputs.status == 'success'
    strategy:
      fail-fast: false
      max-parallel: 2
      matrix:
        include:
          - browser: "chromium"
            browser-name: "Chrome"
            category: "core"
            timeout-minutes: 12
            retry-count: 2
            memory-limit: "3GB"
            priority: 1
          - browser: "firefox"
            browser-name: "Firefox"
            category: "core"
            timeout-minutes: 15
            retry-count: 3
            memory-limit: "4GB"
            priority: 2
    
    # Unified concurrency control to prevent conflicts
    concurrency:
      group: e2e-standard-${{ github.ref }}-${{ matrix.browser }}-${{ github.workflow }}
      cancel-in-progress: true

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: ğŸ“¦ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: ğŸ­ Install Playwright (${{ matrix.browser-name }})
        run: |
          echo "Installing Playwright for ${{ matrix.browser-name }} (Memory: ${{ matrix.memory-limit }})"
          npx playwright install --with-deps ${{ matrix.browser }}
        env:
          # Memory optimization for browser installation
          NODE_OPTIONS: "--max-old-space-size=${{ matrix.memory-limit == '3GB' && '3072' || '3072' }}"

      - name: ğŸŒ Run E2E tests against preview (${{ matrix.browser-name }})
        run: |
          echo "ğŸ­ Running E2E tests with ${{ matrix.browser-name }} against: ${{ needs.deploy-preview.outputs.preview-url }}"
          echo "Browser: ${{ matrix.browser }} (Priority: ${{ matrix.priority }})"
          echo "Timeout: ${{ matrix.timeout-minutes }} minutes, Retries: ${{ matrix.retry-count }}"
          
          npx playwright test --project=${{ matrix.browser }} --retries=${{ matrix.retry-count }}
        env:
          # FIXED: Standardized environment variables (Issue #6)
          BASE_URL: ${{ needs.deploy-preview.outputs.preview-url }}
          PREVIEW_URL: ${{ needs.deploy-preview.outputs.preview-url }}
          DATABASE_URL: ${{ env.DATABASE_URL }}
          CI_ENVIRONMENT: "e2e"
          
          # E2E timeout configuration
          E2E_TEST_TIMEOUT: 60000
          E2E_ACTION_TIMEOUT: 30000
          E2E_NAVIGATION_TIMEOUT: 60000
          E2E_EXPECT_TIMEOUT: 10000
          E2E_HEALTH_CHECK_INTERVAL: 5000

      - name: ğŸ“Š Upload E2E results (${{ matrix.browser-name }})
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.browser }}-${{ github.run_number }}
          path: |
            playwright-report/
            test-results/
          retention-days: 7

  # ======================================================================
  # Stage 6: Performance Tests (Runs if Deploy Available, Non-Blocking) - FIXED Environment Variables
  # ======================================================================
  performance-tests:
    name: âš¡ Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [deploy-preview]
    if: github.event_name == 'pull_request' && (needs.deploy-preview.outputs.status == 'success' || needs.deploy-preview.outputs.status == 'skipped')
    continue-on-error: true
    outputs:
      status: ${{ steps.performance.outcome }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: ğŸ“¦ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: âš¡ Run performance tests
        id: performance
        run: |
          BASE_URL="${{ needs.deploy-preview.outputs.preview-url || 'https://alocubano-boulderfest.vercel.app' }}"
          echo "âš¡ Running performance tests against: $BASE_URL"
          npm run test:performance
        env:
          # FIXED: Standardized environment variables (Issue #6)
          BASE_URL: ${{ needs.deploy-preview.outputs.preview-url || 'https://alocubano-boulderfest.vercel.app' }}
          CI_ENVIRONMENT: "performance"
          
          # Performance test timeout configuration
          E2E_TEST_TIMEOUT: 180000  # 3 minutes for performance tests
          NODE_OPTIONS: "--max-old-space-size=6144"  # 6GB for load testing

  # ======================================================================
  # Stage 7: Lint and Quality (Independent - Always Runs)
  # ======================================================================
  lint-and-quality:
    name: ğŸ” Lint & Quality
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: validate-environment
    outputs:
      status: ${{ steps.lint.outcome }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ğŸ”§ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: ğŸ“¦ Install dependencies
        run: npm ci --prefer-offline --no-audit

      - name: ğŸ” Run linters
        id: lint
        continue-on-error: true
        run: |
          echo "ğŸ” Running ESLint..."
          npm run lint:js || echo "âš ï¸ ESLint warnings found"
          
          echo "ğŸ” Running HTMLHint..."
          npm run lint:html || echo "âš ï¸ HTML issues found"

  # ======================================================================
  # Stage 8: CI Status Report (Always Runs - Smart Summary)
  # ======================================================================
  ci-status:
    name: ğŸ“Š CI Status Report
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [validate-environment, unit-tests, build, security-scan, deploy-preview, e2e-tests, performance-tests, lint-and-quality]
    if: always()

    steps:
      - name: ğŸ“‹ Generate CI Status Report
        run: |
          echo "# ğŸ“Š CI Pipeline Status Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Environment validation status (NEW)
          echo "## ğŸ” Environment Validation" >> $GITHUB_STEP_SUMMARY
          if [ "${{ needs.validate-environment.outputs.validation_passed }}" = "success" ]; then
            echo "âœ… **Environment Configuration**: VALIDATED" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Environment Configuration**: FAILED" >> $GITHUB_STEP_SUMMARY
            ENVIRONMENT_FAILURE=true
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Core Requirements (Critical)
          echo "## ğŸ¯ Core Requirements" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.unit-tests.outputs.status }}" = "success" ]; then
            echo "âœ… **Unit Tests**: PASSED (${{ needs.unit-tests.outputs.coverage }}% coverage)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Unit Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
            CORE_FAILURE=true
          fi
          
          if [ "${{ needs.build.outputs.status }}" = "success" ]; then
            echo "âœ… **Build**: PASSED (artifacts available)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Build**: FAILED (continuing in degraded mode)" >> $GITHUB_STEP_SUMMARY
            BUILD_FAILED=true
          fi
          
          # Quality Gates
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ğŸ” Quality Gates" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.security-scan.outputs.status }}" = "success" ]; then
            echo "âœ… **Security**: PASSED (${{ needs.security-scan.outputs.vulnerabilities }} vulnerabilities)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Security**: Issues found (${{ needs.security-scan.outputs.vulnerabilities }} vulnerabilities)" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.lint-and-quality.outputs.status }}" = "success" ]; then
            echo "âœ… **Linting**: PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Linting**: Issues found" >> $GITHUB_STEP_SUMMARY
          fi
          
          # E2E Testing (PR only)
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## ğŸ­ Integration Testing" >> $GITHUB_STEP_SUMMARY
            
            if [ "${{ needs.deploy-preview.result }}" = "success" ]; then
              echo "âœ… **Deploy**: PASSED ([Preview URL](${{ needs.deploy-preview.outputs.preview-url }}))" >> $GITHUB_STEP_SUMMARY
              
              if [ "${{ needs.e2e-tests.result }}" = "success" ]; then
                echo "âœ… **E2E Tests**: PASSED" >> $GITHUB_STEP_SUMMARY
              else
                echo "âŒ **E2E Tests**: FAILED" >> $GITHUB_STEP_SUMMARY
              fi
              
              if [ "${{ needs.performance-tests.outputs.status }}" = "success" ]; then
                echo "âœ… **Performance**: PASSED" >> $GITHUB_STEP_SUMMARY
              else
                echo "âš ï¸ **Performance**: Issues detected (non-blocking)" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "âš ï¸ **Deploy**: FAILED (Build required for deployment)" >> $GITHUB_STEP_SUMMARY
              echo "â­ï¸ **E2E Tests**: SKIPPED (No deployment available)" >> $GITHUB_STEP_SUMMARY
              echo "â­ï¸ **Performance**: SKIPPED (No deployment available)" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
          # Environment Configuration Summary (NEW)
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ğŸ”§ Environment Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Node.js**: ${{ env.NODE_VERSION }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Environment**: ${{ env.CI_ENVIRONMENT }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Database**: ${{ env.DATABASE_URL }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Memory Allocation**: ${{ env.NODE_OPTIONS }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸ†• **Issue #6 Fixed**: Standardized environment variables across all workflows" >> $GITHUB_STEP_SUMMARY
          
          # Final Status
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ğŸ Final Status" >> $GITHUB_STEP_SUMMARY
          
          if [ "$ENVIRONMENT_FAILURE" = "true" ]; then
            echo "âŒ **CI FAILED**: Environment configuration validation failed" >> $GITHUB_STEP_SUMMARY
            exit 1
          elif [ "$CORE_FAILURE" = "true" ]; then
            echo "âŒ **CI FAILED**: Core requirements not met" >> $GITHUB_STEP_SUMMARY
            exit 1
          elif [ "$BUILD_FAILED" = "true" ]; then
            echo "âš ï¸ **CI PARTIAL**: Core tests passed, build issues detected" >> $GITHUB_STEP_SUMMARY
            echo "_Deployment and E2E testing skipped due to build failure_" >> $GITHUB_STEP_SUMMARY
            exit 0
          else
            echo "âœ… **CI PASSED**: All requirements met" >> $GITHUB_STEP_SUMMARY
          fi

      - name: ğŸ¯ Check Critical Requirements
        run: |
          # Fail CI if environment validation or unit tests fail
          if [ "${{ needs.validate-environment.outputs.validation_passed }}" != "success" ]; then
            echo "âŒ Critical failure: Environment validation must pass"
            exit 1
          fi
          
          if [ "${{ needs.unit-tests.outputs.status }}" != "success" ]; then
            echo "âŒ Critical failure: Unit tests must pass"
            exit 1
          fi
          
          echo "âœ… Critical requirements met"