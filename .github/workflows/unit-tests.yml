name: "🧪 Unit Tests"

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened]
  workflow_dispatch:

concurrency:
  group: unit-tests-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write

env:
  NODE_ENV: test
  CI: true
  DATABASE_URL: ":memory:"
  PHASE3_PERFORMANCE_TARGET_MS: 2000

jobs:
  unit-tests:
    name: "🧪 Unit Test Suite"
    runs-on: ubuntu-latest
    timeout-minutes: 10

    strategy:
      matrix:
        node-version: ['20.x', '22.x']

    steps:
      - name: "📥 Checkout Code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "🔧 Setup Node.js ${{ matrix.node-version }}"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: "📦 Install Dependencies"
        run: npm ci --prefer-offline --no-audit

      - name: "🧪 Run Unit Tests (806+ tests)"
        id: test
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "🚀 Running Unit Test Suite"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📊 Expected: 806+ unit tests"
          echo "🎯 Performance Target: <2 seconds"
          echo "📁 Categories: Security (248), Business Logic (300), Frontend (258)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

          # Run unit tests with precise timing
          start_time=$(date +%s%3N)

          # Run tests and capture exit code
          npm test 2>&1 | tee test-output.log
          test_exit_code=${PIPESTATUS[0]}

          end_time=$(date +%s%3N)
          duration=$((end_time - start_time))

          # Extract test counts from output
          total_tests=$(grep -oP '\d+(?= passing)' test-output.log | head -1 || echo "0")
          passing_tests=$(grep -oP '\d+(?= passing)' test-output.log | head -1 || echo "0")
          failing_tests=$(grep -oP '\d+(?= failing)' test-output.log | head -1 || echo "0")

          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "📊 Test Results Summary"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Tests Passed: $passing_tests"
          echo "❌ Tests Failed: $failing_tests"
          echo "📈 Total Tests: $total_tests"
          echo "⏱️  Execution Time: ${duration}ms"

          # Performance evaluation
          if [ "$duration" -lt "$PHASE3_PERFORMANCE_TARGET_MS" ]; then
            echo "🏆 EXCELLENT: Execution under 2-second target!"
          else
            echo "⚠️  WARNING: Execution time (${duration}ms) exceeds target"
          fi

          # Set outputs for PR comment
          echo "total_tests=$total_tests" >> $GITHUB_OUTPUT
          echo "passing_tests=$passing_tests" >> $GITHUB_OUTPUT
          echo "failing_tests=$failing_tests" >> $GITHUB_OUTPUT
          echo "duration=${duration}ms" >> $GITHUB_OUTPUT
          echo "node_version=${{ matrix.node-version }}" >> $GITHUB_OUTPUT

          exit $test_exit_code
        env:
          NODE_ENV: ${{ env.NODE_ENV }}
          CI: ${{ env.CI }}
          DATABASE_URL: ${{ env.DATABASE_URL }}

      - name: "📊 Generate Test Report"
        if: always()
        run: |
          cat > test-report.json << EOF
          {
            "node_version": "${{ matrix.node-version }}",
            "total_tests": "${{ steps.test.outputs.total_tests }}",
            "passing_tests": "${{ steps.test.outputs.passing_tests }}",
            "failing_tests": "${{ steps.test.outputs.failing_tests }}",
            "duration": "${{ steps.test.outputs.duration }}",
            "status": "${{ steps.test.outcome }}"
          }
          EOF

      - name: "📤 Upload Test Results"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-node-${{ matrix.node-version }}
          path: |
            test-output.log
            test-report.json
          retention-days: 7

      - name: "💬 Comment Test Results on PR"
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const testReport = JSON.parse(fs.readFileSync('test-report.json', 'utf8'));

            const status = testReport.status === 'success' ? '✅' : '❌';
            const statusText = testReport.status === 'success' ? 'PASSED' : 'FAILED';

            const comment = `## ${status} Unit Tests ${statusText} - Node ${{ matrix.node-version }}

            ### 📊 Test Summary
            | Metric | Value |
            |--------|-------|
            | **Total Tests** | ${testReport.total_tests} |
            | **Passed** | ✅ ${testReport.passing_tests} |
            | **Failed** | ❌ ${testReport.failing_tests} |
            | **Duration** | ⏱️ ${testReport.duration} |
            | **Node Version** | 🔧 ${{ matrix.node-version }} |

            ${testReport.status === 'success' ?
              '### 🏆 All tests passed successfully!' :
              '### ⚠️ Some tests failed. Please review the test output.'}

            <details>
            <summary>View detailed test output</summary>

            \`\`\`
            Check the artifacts for full test output
            \`\`\`
            </details>`;

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes(`Unit Tests`) &&
              comment.body.includes(`Node ${{ matrix.node-version }}`)
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }