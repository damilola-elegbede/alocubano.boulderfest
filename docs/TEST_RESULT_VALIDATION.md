# Test Result Validation

## Quick Start

```bash
# Run validation after tests complete
npm run validate:test-results

# View validation report in CI
# Check "Validate Test Results" step in GitHub Actions
```

## Overview

### What is Test Result Validation?

Test Result Validation is an automated quality gate that detects **false positives** in CI/CD test runs.
It analyzes test metadata to identify scenarios where tests appear to pass but actually contain critical issues.

### Why We Need It

Traditional CI/CD pipelines only check exit codes:

- **Exit code 0** = Tests passed, merge allowed
- **Exit code non-zero** = Tests failed, merge blocked

**The problem**: Exit codes can lie. Common false positive scenarios:

1. Test framework crashes but returns exit code 0
2. All tests skip due to misconfiguration, but exit code is 0
3. Tests fail but framework error handling masks the failure
4. Test count mismatches indicate incomplete execution
5. Performance regressions go unnoticed

**The solution**: Validate test results using pattern matching on test metadata, not just exit codes.

### When It Runs

Test Result Validation runs automatically:

1. **After all tests complete** in CI/CD pipeline
2. **Before merge decision** is made
3. **Can be triggered manually** via `npm run validate:test-results`

## Architecture

### Integration with Existing Workflows

```text
┌─────────────────────────────────────────────────────────────┐
│                     CI/CD Pipeline Flow                     │
└─────────────────────────────────────────────────────────────┘

1. Run Tests
   ├── Unit Tests (npm test)
   ├── Integration Tests (npm run test:integration)
   ├── E2E Tests (npm run test:e2e)
   └── Quality Tests (npm run lint)
         │
         ▼
2. Generate Metadata
   ├── .tmp/test-validation/unit/test-metadata.json
   ├── .tmp/test-validation/integration/test-metadata.json
   ├── .tmp/test-validation/e2e/test-metadata.json
   └── .tmp/test-validation/quality/test-metadata.json
         │
         ▼
3. Validate Results (npm run validate:test-results)
   ├── Read all metadata files
   ├── Run 10 validation patterns
   ├── Generate validation report
   └── Exit with appropriate code (0 = pass, 1 = fail)
         │
         ▼
4. Merge Decision
   ├── All validations pass → Merge allowed ✅
   └── Any P1/P2 issues → Merge blocked ❌
```

### Workflow Execution Flow

```text
┌────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Test Suite   │───▶│ Metadata Writer  │───▶│  Metadata File  │
│  (Vitest/PW)   │    │  (Reporter)      │    │     (JSON)      │
└────────────────┘    └──────────────────┘    └─────────────────┘
                                                        │
                                                        ▼
                           ┌────────────────────────────────────────┐
                           │   Test Result Validator                │
                           │   (validate-test-results.js)           │
                           └────────────────────────────────────────┘
                                    │         │         │
                     ┌──────────────┴─────────┴────────┴──────────────┐
                     ▼              ▼                   ▼              ▼
            ┌────────────┐  ┌────────────┐    ┌────────────┐  ┌────────────┐
            │  Pattern 1 │  │  Pattern 2 │    │  Pattern 9 │  │ Pattern 10 │
            │ Exit Code  │  │ Test Count │ ...│   Flaky    │  │   Retry    │
            └────────────┘  └────────────┘    └────────────┘  └────────────┘
                     │              │                   │              │
                     └──────────────┴───────────────────┴──────────────┘
                                            │
                                            ▼
                           ┌────────────────────────────────────┐
                           │      Validation Report             │
                           │  (JSON with P1/P2/P3 issues)      │
                           └────────────────────────────────────┘
                                            │
                                            ▼
                           ┌────────────────────────────────────┐
                           │       Exit Code Calculation        │
                           │  P1 or P2 issues → exit(1)        │
                           │  P3 only or none → exit(0)        │
                           └────────────────────────────────────┘
```

### Components

**Metadata Generation**:

- **Unit Tests**: Generated by Vitest custom reporter
- **Integration Tests**: Generated by Vitest custom reporter
- **E2E Tests**: Generated by Playwright custom reporter
- **Quality Tests**: Generated by lint script wrapper

**Validation**: `scripts/validate-test-results.js`

- Scans `.tmp/test-validation/` for metadata files
- Runs 10 validation patterns
- Generates structured report

**Reporting**: CI/CD integration

- GitHub Actions annotations for P1/P2 issues
- JSON report for debugging
- Exit code for merge decision

## Validation Patterns

### Priority 1 (Critical) - Blocks Merge

#### 1. Exit Code Discrepancy

**Description**: Test framework reports success (exit code 0) but tests actually failed, or vice versa.

**Example**:

```json
{
  "exitCode": 0,
  "passed": 100,
  "failed": 5,
  "total": 105
}
```

**Detection**: `exitCode === 0 && failed > 0`

**Recommendation**: Check test framework configuration. Test failures may not be propagating to process exit code.

#### 2. Test Count Anomaly

**Description**: No tests executed, or test counts don't add up correctly.

**Example**:

```json
{
  "total": 0,
  "passed": 0,
  "failed": 0,
  "exitCode": 0
}
```

**Detection**: `total === 0 && exitCode === 0` OR `(passed + failed + skipped) !== total`

**Recommendation**: Verify test files exist and patterns match test files. Check test runner configuration.

#### 3. All Tests Skipped

**Description**: All tests were skipped due to conditions or misconfiguration.

**Example**:

```json
{
  "total": 50,
  "passed": 0,
  "failed": 0,
  "skipped": 50
}
```

**Detection**: `total > 0 && passed === 0 && failed === 0 && skipped === total`

**Recommendation**: Check test conditions and skip logic. Verify environment setup is correct.

#### 4. JSON Parsing Failure

**Description**: Metadata file exists but couldn't be parsed as valid JSON.

**Example**: Corrupted or incomplete JSON file.

**Detection**: `JSON.parse()` throws error when reading metadata file.

**Recommendation**: Check test metadata generation script. Verify JSON is valid and complete.

### Priority 2 (Warnings) - Review Before Merge

#### 5. High Skip Rate

**Description**: More than 20% of tests are being skipped.

**Example**:

```json
{
  "total": 100,
  "passed": 60,
  "failed": 0,
  "skipped": 40
}
```

**Detection**: `(skipped / total) > 0.20`

**Recommendation**: Review skipped tests. Consider removing obsolete tests or fixing skip conditions.

#### 6. Version Inconsistency

**Description**: Different Node versions or browsers report different test results.

**Example**:

```json
[
  { "testType": "e2e", "nodeVersion": "20.x", "failed": 0 },
  { "testType": "e2e", "nodeVersion": "22.x", "failed": 5 }
]
```

**Detection**: Same test type has different failure counts across environments.

**Recommendation**: Check for environment-specific issues. Verify test assumptions about Node/browser versions.

#### 7. Browser-Specific Failures

**Description**: E2E tests fail only in specific browsers.

**Example**:

```json
[
  { "testType": "e2e", "browser": "chromium", "failed": 0 },
  { "testType": "e2e", "browser": "firefox", "failed": 3 }
]
```

**Detection**: E2E tests in one browser have more failures than others.

**Recommendation**: Check for browser compatibility issues. Verify selectors and timing work across browsers.

#### 8. Retry Success Mask

**Description**: High retry rate where all tests eventually pass, potentially masking instability.

**Example**:

```json
{
  "total": 100,
  "passed": 100,
  "failed": 0,
  "retries": 25
}
```

**Detection**: `(retries / total) > 0.1 && failed === 0`

**Recommendation**: High retry rate may mask underlying instability. Fix root causes instead of relying on retries.

#### 9. Exit Code Mismatch (Non-Critical)

**Description**: Non-zero exit code but no test failures reported (and tests passed).

**Example**:

```json
{
  "exitCode": 1,
  "passed": 50,
  "failed": 0,
  "total": 50
}
```

**Detection**: `exitCode !== 0 && failed === 0 && passed > 0`

**Recommendation**: Investigate non-test error causing process exit. Check for unhandled exceptions or infrastructure issues.

### Priority 3 (Informational) - Consider Addressing

#### 10. Performance Regression

**Description**: Test suite duration significantly exceeds baseline (>50% slower than median).

**Example**:

```json
{
  "testType": "unit",
  "nodeVersion": "22.x",
  "duration": 45.2,
  "median": 28.5
}
```

**Detection**: `duration > (median * 1.5)` within same test type and environment.

**Recommendation**: Investigate slow tests. Check for infrastructure issues or test inefficiencies.

#### 11. Flaky Tests (Informational)

**Description**: Tests that required retry to pass.

**Example**:

```json
{
  "total": 100,
  "passed": 100,
  "failed": 0,
  "retries": 3
}
```

**Detection**: `retries > 0 && passed > 0`

**Recommendation**: Investigate flaky tests. Add proper waits, fix race conditions, or improve test isolation.

## Metadata Format

### JSON Schema

```json
{
  "workflow": "string (required) - CI workflow name (unit, integration, e2e, quality)",
  "testType": "string (required) - Test type identifier",
  "nodeVersion": "string (optional) - Node.js version (e.g., '22.x')",
  "browser": "string (optional) - Browser name for E2E tests",
  "total": "number (required) - Total number of tests",
  "passed": "number (required) - Number of passed tests",
  "failed": "number (required) - Number of failed tests",
  "skipped": "number (required) - Number of skipped tests",
  "retries": "number (optional) - Number of test retries",
  "duration": "number (optional) - Test suite duration in seconds",
  "exitCode": "number (required) - Process exit code",
  "timestamp": "string (required) - ISO 8601 timestamp"
}
```

### Field Descriptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `workflow` | string | Yes | CI workflow name (unit, integration, e2e, quality) |
| `testType` | string | Yes | Test type identifier (unit, integration, e2e, lint) |
| `nodeVersion` | string | No | Node.js version running tests (e.g., "22.x") |
| `browser` | string | No | Browser name for E2E tests (chromium, firefox, webkit) |
| `total` | number | Yes | Total number of tests in suite |
| `passed` | number | Yes | Number of tests that passed |
| `failed` | number | Yes | Number of tests that failed |
| `skipped` | number | Yes | Number of tests skipped |
| `retries` | number | No | Number of test retries that occurred |
| `duration` | number | No | Total suite duration in seconds |
| `exitCode` | number | Yes | Process exit code (0 = success, non-zero = failure) |
| `timestamp` | string | Yes | ISO 8601 timestamp when tests completed |

### Example Metadata

#### Unit Tests

```json
{
  "workflow": "ci",
  "testType": "unit",
  "nodeVersion": "22.x",
  "total": 1126,
  "passed": 1126,
  "failed": 0,
  "skipped": 0,
  "retries": 0,
  "duration": 5.13,
  "exitCode": 0,
  "timestamp": "2025-10-31T12:34:56.789Z"
}
```

#### Integration Tests

```json
{
  "workflow": "ci",
  "testType": "integration",
  "nodeVersion": "22.x",
  "total": 85,
  "passed": 83,
  "failed": 2,
  "skipped": 0,
  "retries": 1,
  "duration": 12.45,
  "exitCode": 1,
  "timestamp": "2025-10-31T12:35:30.123Z"
}
```

#### E2E Tests

```json
{
  "workflow": "ci",
  "testType": "e2e",
  "nodeVersion": "22.x",
  "browser": "chromium",
  "total": 36,
  "passed": 36,
  "failed": 0,
  "skipped": 0,
  "retries": 2,
  "duration": 180.5,
  "exitCode": 0,
  "timestamp": "2025-10-31T12:40:15.456Z"
}
```

#### Quality Tests (Linting)

```json
{
  "workflow": "ci",
  "testType": "quality",
  "nodeVersion": "22.x",
  "total": 4,
  "passed": 4,
  "failed": 0,
  "skipped": 0,
  "duration": 2.1,
  "exitCode": 0,
  "timestamp": "2025-10-31T12:41:00.789Z"
}
```

## Running Validation

### Manual Execution

```bash
# Run validation on existing test metadata
npm run validate:test-results

# Output: JSON report to stdout
# Exit code: 0 = pass, 1 = fail (P1 or P2 issues detected)
```

### Viewing Results in CI

**GitHub Actions**:

1. Navigate to Actions tab → Select workflow run
2. Expand "Validate Test Results" step
3. Review JSON report and recommendations

**Report Structure**:

```json
{
  "validation_status": "PASS|WARN|FAIL",
  "exit_code": 0,
  "summary": {
    "priority1_issues": 0,
    "priority2_issues": 0,
    "priority3_issues": 0,
    "total_issues": 0,
    "total_tests_validated": 1251,
    "metadata_files_processed": 4
  },
  "issues": [],
  "metadata_summary": [],
  "recommendations": []
}
```

### Interpreting Reports

**Validation Status**:

- **PASS**: No P1/P2 issues, merge allowed ✅
- **WARN**: P2 issues detected, review recommended ⚠️
- **FAIL**: P1 issues detected, merge blocked ❌

**Issue Format**:

```json
{
  "severity": "P1|P2|P3",
  "pattern": "exit_code_discrepancy",
  "workflow": "ci",
  "testType": "unit",
  "nodeVersion": "22.x",
  "browser": null,
  "description": "Exit code 0 but 5 test(s) failed",
  "recommendation": "Check test framework configuration...",
  "details": {
    "exitCode": 0,
    "passed": 100,
    "failed": 5,
    "total": 105
  }
}
```

## Troubleshooting

### Common Issues and Fixes

#### Issue: "No test metadata files found"

**Cause**: Metadata files not generated or in wrong location.

**Fix**:

```bash
# Check metadata directory exists
ls -la .tmp/test-validation/

# Run tests to regenerate metadata
npm test
npm run test:integration
npm run test:e2e

# Then run validation
npm run validate:test-results
```

#### Issue: "JSON parsing failure"

**Cause**: Corrupted or incomplete metadata file.

**Fix**:

```bash
# Check metadata file contents
cat .tmp/test-validation/unit/test-metadata.json

# Regenerate metadata
rm -rf .tmp/test-validation/
npm test

# Validate again
npm run validate:test-results
```

#### Issue: "High skip rate warning"

**Cause**: Too many tests are being skipped.

**Fix**:

```bash
# Review skipped tests
npm test -- --reporter=verbose

# Remove obsolete tests or fix skip conditions
# Update test to run unconditionally or remove it

# Re-run validation
npm run validate:test-results
```

#### Issue: "Exit code discrepancy"

**Cause**: Test framework not propagating failures to exit code.

**Fix**:

```javascript
// Check Vitest configuration
export default defineConfig({
  test: {
    // Ensure failures propagate
    passWithNoTests: false,
    bail: 1 // Optional: fail fast
  }
});
```

### False Alarms and Suppression

**Scenario**: P2 warning is expected (e.g., known performance regression being investigated).

**Solution**: Document in PR description and proceed with caution.

**Note**: No suppression mechanism exists intentionally - all issues should be investigated.

### Emergency Bypass Procedures

**CRITICAL**: Only use in extreme circumstances (e.g., production outage fix).

```bash
# NOT RECOMMENDED: Skip validation entirely
# This removes the safety net - use with extreme caution

# Option 1: Fix the underlying issue (RECOMMENDED)
# Address the P1/P2 issues detected

# Option 2: Temporary workaround (USE SPARINGLY)
# Merge with admin override after manual review
# Document reason in PR and create follow-up ticket
```

## Configuration

### Thresholds

Located in `scripts/validate-test-results.js`:

```javascript
const SKIP_RATE_THRESHOLD = 0.20;          // 20% skip rate triggers warning
const PERF_REGRESSION_THRESHOLD = 1.5;     // 50% slower triggers info
const MIN_TOTAL_TESTS = 1;                 // Minimum tests per suite
const RETRY_RATE_WARNING = 0.1;            // 10% retry rate triggers warning
```

**Adjusting Thresholds**:

```javascript
// Example: Increase skip rate tolerance to 30%
const SKIP_RATE_THRESHOLD = 0.30;

// Example: Tighten performance regression to 25%
const PERF_REGRESSION_THRESHOLD = 1.25;
```

### Customizing Patterns

**Adding New Validation Logic**:

```javascript
/**
 * Pattern 11: Custom Validation
 * Description of what this pattern detects
 */
function checkCustomPattern(metadata, results) {
  for (const data of metadata) {
    if (data._parseError) continue;

    // Your validation logic here
    if (/* condition */) {
      results.priority2.push({
        severity: PRIORITY.WARNING,
        pattern: 'custom_pattern',
        workflow: data.workflow,
        testType: data.testType,
        description: 'Custom issue detected',
        recommendation: 'How to fix this issue',
        details: { /* relevant data */ }
      });
    }
  }
}

// Add to validation function
checkCustomPattern(allMetadata, results);
```

### Adding New Validation Rules

**Example**: Detect when unit tests take too long

```javascript
/**
 * Pattern: Unit Test Duration Threshold
 * Unit tests should complete in under 10 seconds
 */
function checkUnitTestDuration(metadata, results) {
  const UNIT_TEST_MAX_DURATION = 10; // seconds

  for (const data of metadata) {
    if (data._parseError) continue;
    if (data.testType !== 'unit') continue;

    if (data.duration > UNIT_TEST_MAX_DURATION) {
      results.priority2.push({
        severity: PRIORITY.WARNING,
        pattern: 'unit_test_duration',
        workflow: data.workflow,
        testType: data.testType,
        description: `Unit tests took ${data.duration}s (max: ${UNIT_TEST_MAX_DURATION}s)`,
        recommendation: 'Optimize slow unit tests or move them to integration suite',
        details: {
          duration: data.duration,
          threshold: UNIT_TEST_MAX_DURATION,
          total: data.total
        }
      });
    }
  }
}
```

## Development

### How to Add New Patterns

1. **Define the pattern function**:

```javascript
function checkNewPattern(metadata, results) {
  for (const data of metadata) {
    if (data._parseError) continue;

    // Validation logic
    if (/* detection condition */) {
      results.priority1.push({
        severity: PRIORITY.CRITICAL,
        pattern: 'new_pattern_name',
        workflow: data.workflow,
        testType: data.testType,
        description: 'Clear description of issue',
        recommendation: 'Actionable fix guidance',
        details: { /* supporting data */ }
      });
    }
  }
}
```

2. **Add to validation execution**:

```javascript
// In validateTestResults() function
checkExitCodeDiscrepancy(allMetadata, results);
checkTestCountAnomaly(allMetadata, results);
// ... existing patterns ...
checkNewPattern(allMetadata, results); // Add here
```

3. **Test the pattern**:

```bash
# Create test metadata with issue condition
echo '{"total":0,"passed":0,"failed":0,"exitCode":0}' > .tmp/test-validation/test/test-metadata.json

# Run validation
npm run validate:test-results

# Verify pattern detects the issue
```

### Testing Validation Locally

**Setup Test Metadata**:

```bash
# Create test metadata directory
mkdir -p .tmp/test-validation/test

# Create sample metadata with issues
cat > .tmp/test-validation/test/test-metadata.json << 'EOF'
{
  "workflow": "test",
  "testType": "unit",
  "total": 100,
  "passed": 95,
  "failed": 5,
  "skipped": 0,
  "exitCode": 0,
  "timestamp": "2025-10-31T12:00:00Z"
}
EOF

# Run validation
npm run validate:test-results

# Expected: P1 issue detected (exit code discrepancy)
```

**Test Multiple Patterns**:

```bash
# All tests skipped
echo '{"workflow":"test","testType":"unit","total":50,"passed":0,"failed":0,"skipped":50,"exitCode":0}' \
  > .tmp/test-validation/test/test-metadata.json

# High skip rate
echo '{"workflow":"test","testType":"unit","total":100,"passed":60,"failed":0,"skipped":40,"exitCode":0}' \
  > .tmp/test-validation/test2/test-metadata.json

npm run validate:test-results
```

### Updating Metadata Schema

**Adding New Field**:

1. Update metadata generation (test reporters)
2. Update validation patterns to use new field
3. Update documentation in this file
4. Test with real test runs

**Example**: Add `commit_sha` field

```javascript
// In test reporter
metadata.commit_sha = process.env.GITHUB_SHA || 'unknown';

// In validation pattern
console.log(`Issue in commit: ${data.commit_sha}`);
```

## See Also

- [INSTALLATION.md](../INSTALLATION.md) - Setup and testing commands
- [CI/CD Configuration](../.github/workflows/ci.yml) - GitHub Actions workflow
- [Test Strategy](../CLAUDE.md#testing-strategy) - Overall testing approach
- [Vitest Configuration](../tests/vitest.config.js) - Unit test configuration
- [Playwright Configuration](../tests/config/playwright-e2e-optimized.config.js) - E2E test configuration
